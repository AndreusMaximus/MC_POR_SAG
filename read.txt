--------------------------------------------------------------------------------
Profile data file 'callgrind.out.17531' (creator: callgrind-3.16.1)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 11769227845
Trigger: Program termination
Profiled target:  ./build/nptest -r -m 4 ../real-time-task-generators-main/proper testsets/c4_slow.csv --por=priority (PID 17531, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
Ir                      
--------------------------------------------------------------------------------
38,237,050,287 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
Ir                       file:function
--------------------------------------------------------------------------------
10,952,342,376 (28.64%)  include/jobs.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
 9,889,609,045 (25.86%)  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
 4,636,064,961 (12.12%)  /usr/include/c++/10/bits/predefined_ops.h:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
 2,341,779,270 ( 6.12%)  /usr/include/c++/10/bits/stl_algobase.h:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
 1,322,503,480 ( 3.46%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
   812,303,369 ( 2.12%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] [/usr/local/lib/libtbbmalloc.so.2.9]
   631,296,580 ( 1.65%)  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   619,318,880 ( 1.62%)  /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   491,237,764 ( 1.28%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free [/usr/local/lib/libtbbmalloc.so.2.9]
   488,761,028 ( 1.28%)  /usr/include/c++/10/bits/stl_tree.h:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
   479,341,218 ( 1.25%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
   428,941,596 ( 1.12%)  ./nptl/pthread_getspecific.c:pthread_getspecific [/usr/lib/x86_64-linux-gnu/libpthread-2.31.so]
   333,237,553 ( 0.87%)  /usr/include/c++/10/bits/stl_vector.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)
   327,199,119 ( 0.86%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) [/usr/local/lib/libtbbmalloc.so.2.9]
   255,480,674 ( 0.67%)  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0]
   252,284,760 ( 0.66%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator delete(void*) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
   251,500,043 ( 0.66%)  /usr/include/c++/10/ext/new_allocator.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)
   230,084,196 ( 0.60%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
   227,057,535 ( 0.59%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::getBackRef(rml::internal::BackRefIdx)
   227,057,151 ( 0.59%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) [/usr/local/lib/libtbbmalloc.so.2.9]
   201,827,920 ( 0.53%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc [/usr/local/lib/libtbbmalloc.so.2.9]
   201,827,808 ( 0.53%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
   166,618,803 ( 0.44%)  /usr/include/c++/10/bits/stl_algo.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   137,234,055 ( 0.36%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::freeSmallObject(void*) [clone .lto_priv.0]
   127,324,682 ( 0.33%)  /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, long long>, true>*, unsigned long) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   126,142,435 ( 0.33%)  /usr/include/c++/10/bits/atomic_base.h:__TBB_malloc_safer_free
   122,643,750 ( 0.32%)  /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex()
   100,913,948 ( 0.26%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.h:__TBB_malloc_safer_free
   100,600,016 ( 0.26%)  /usr/include/c++/10/bits/stl_uninitialized.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)
    99,093,083 ( 0.26%)  /usr/include/c++/10/bits/hashtable.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    88,043,548 ( 0.23%)  /usr/include/c++/10/bits/stl_iterator.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    77,087,740 ( 0.20%)  include/global/reduction_set.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
    76,026,116 ( 0.20%)  /usr/include/c++/10/bits/hashtable_policy.h:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    71,341,592 ( 0.19%)  ???:std::_Rb_tree_increment(std::_Rb_tree_node_base const*) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
    70,541,495 ( 0.18%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) [/usr/local/lib/libtbbmalloc.so.2.9]
    62,873,693 ( 0.16%)  /usr/include/c++/10/ext/new_allocator.h:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex()
    53,482,357 ( 0.14%)  /usr/include/c++/10/bits/stl_iterator.h:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
    53,445,023 ( 0.14%)  /usr/include/c++/10/bits/stl_algobase.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    50,456,970 ( 0.13%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:rml::internal::internalMalloc(unsigned long)
    50,456,970 ( 0.13%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::internalMalloc(unsigned long)
    50,456,934 ( 0.13%)  ???:operator delete(void*, unsigned long) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
    50,451,640 ( 0.13%)  ./nptl/pthread_self.c:pthread_self [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
    50,451,150 ( 0.13%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:__TBB_malloc_safer_free
    50,451,150 ( 0.13%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:rml::internal::freeSmallObject(void*) [clone .lto_priv.0]
    47,721,144 ( 0.12%)  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    41,086,298 ( 0.11%)  /usr/include/c++/10/bits/vector.tcc:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    38,997,273 ( 0.10%)  include/jobs.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    36,791,411 ( 0.10%)  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_erms [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
    35,707,005 ( 0.09%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] [/usr/local/lib/libtbbmalloc.so.2.9]
    32,309,029 ( 0.08%)  /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    31,101,561 ( 0.08%)  include/jobs.hpp:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&)
    28,293,759 ( 0.07%)  /usr/include/c++/10/bits/predefined_ops.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0]
    28,293,750 ( 0.07%)  /usr/include/c++/10/bits/stl_algo.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    25,228,476 ( 0.07%)  /usr/include/c++/10/bits/atomic_base.h:operator delete(void*)
    25,225,810 ( 0.07%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long)
    22,942,187 ( 0.06%)  /usr/include/c++/10/bits/unordered_map.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
    22,291,800 ( 0.06%)  /usr/include/c++/10/bits/stl_vector.h:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
    22,083,496 ( 0.06%)  /usr/include/c++/10/bits/hashtable_policy.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0]

--------------------------------------------------------------------------------
-- Auto-annotated source: include/global/reduction_set.hpp
--------------------------------------------------------------------------------
Ir                     

-- line 145 ----------------------------------------
            .           		};
            .           
            .           		template <class Time>
            .           		class lst_Job
            .           		{
            .           		public:
            .           			const Job<Time> *J;
            .           			Time LST;
       41,328 ( 0.00%)  			lst_Job(const Job<Time> *base_J, Time base_LST) : J{base_J}, LST{base_LST} {}
            .           		};
            .           
            .           		template <class Time>
            7 ( 0.00%)  		class Reduction_set
            .           		{
            .           		public:
            .           			typedef std::vector<const Job<Time> *> Job_set;
            .           			typedef std::vector<std::size_t> Job_precedence_set;
            .           			typedef std::unordered_map<JobID, Time> Job_map;
            .           			typedef typename Job<Time>::Priority Priority;
            .           			typedef std::unordered_map<JobID, LST_container<Time>> LST_values;
            .           
-- line 165 ----------------------------------------
-- line 192 ----------------------------------------
            .           			std::map<std::size_t, const Job<Time> *> job_by_index;
            .           			Time total_count = 0;
            .           			LST_values LST_map;
            .           
            .           			bool deadline_miss;
            .           
            .           		public:
            .           			// CPU availability gaat dus wat anders worden a.d.h.v de meerdere cores
           12 ( 0.00%)  			Reduction_set(std::vector<Interval<Time>> cpu_availability, const Job_set &jobs, std::vector<std::size_t> &indices, const Job_precedence_set job_precedence_sets)
            .           				: cpu_availability{cpu_availability},
            .           				  jobs{jobs},
            .           				  indices{indices},
            .           				  job_precedence_sets{job_precedence_sets},
            .           				  jobs_by_latest_arrival{jobs},
            .           				  jobs_by_earliest_arrival{jobs},
            .           				  // jobs_by_wcet{jobs},
            .           				  jobs_by_rmax_cmin{jobs},
            .           				  jobs_by_rmin_cmin{jobs},
            .           				  key{0},
            .           				  num_interfering_jobs_added{0},
            .           				  index_by_job(),
            .           				  job_by_index(),
           31 ( 0.00%)  				  LST_map()
          825 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::vector(std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&) (5x)
          195 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (2x)
            .           			{
            .           				//std::cout << "INITIALIZE NEW REDUCITON SET" << std::endl;
            1 ( 0.00%)  				deadline_miss = false;
            .           				std::sort(jobs_by_latest_arrival.begin(), jobs_by_latest_arrival.end(),
            .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
           28 ( 0.00%)  						  { if(i->latest_arrival() < j->latest_arrival()){
            .           							return true;
           21 ( 0.00%)  						  }else if(i->latest_arrival() > j->latest_arrival()){
            .           							return false;
            .           						  }else{
            .           							return i->get_priority() < j->get_priority();
            .           						  }; });
            .           
            .           				std::sort(jobs_by_earliest_arrival.begin(), jobs_by_earliest_arrival.end(),
            .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
            .           						  { return i->earliest_arrival() < j->earliest_arrival(); });
-- line 230 ----------------------------------------
-- line 231 ----------------------------------------
            .           
            .           				// std::sort(jobs_by_wcet.begin(), jobs_by_wcet.end(),
            .           				//		  [](const Job<Time> *i, const Job<Time> *j) -> bool
            .           				//		  { return i->maximal_cost() < j->maximal_cost(); });
            .           
            .           				// these sorts are required for the reverse fill algorithm
            .           				std::sort(jobs_by_rmin_cmin.begin(), jobs_by_rmin_cmin.end(),
            .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
           44 ( 0.00%)  						  { return i->earliest_arrival() + i->minimal_cost() < j->earliest_arrival() + j->minimal_cost(); });
            .           
            .           				std::sort(jobs_by_rmax_cmin.begin(), jobs_by_rmax_cmin.end(),
            .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
           46 ( 0.00%)  						  { return i->earliest_arrival() + i->maximal_cost() < j->earliest_arrival() + j->maximal_cost(); });
            .           
            .           				// No clue yet why this exists but i dont want to remove it yet.
           27 ( 0.00%)  				for (int i = 0; i < jobs.size(); i++)
            .           				{
            8 ( 0.00%)  					auto j = jobs[i];
           24 ( 0.00%)  					std::size_t idx = indices[i];
            .           
            .           					index_by_job.emplace(j->get_id(), idx);
           24 ( 0.00%)  					job_by_index.emplace(std::make_pair(idx, jobs[i]));
            .           				}
            .           
            .           				// latest_busy_time = compute_latest_busy_time();
            4 ( 0.00%)  				latest_idle_time = compute_latest_idle_time();
          358 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() (1x)
            .           				// compute_latest_start_times();
            2 ( 0.00%)  				compute_latest_start_time_complex();
       26,138 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() (1x)
            1 ( 0.00%)  				max_priority = compute_max_priority();
            .           				initialize_key();
            .           				// std::cout << "Candidate reduction set contains " << jobs.size() << " jobs" << std::endl;
            .           
            .           				// for (const Job<Time>* j : jobs) {
            .           				//	std::cout<<j->get_id()<<" ";
            .           				// }
            .           				// std::cout<<std::endl;
            8 ( 0.00%)  			}
            .           
            .           			// For test purposes
            .           			Reduction_set(std::vector<Interval<Time>> cpu_availability, const Job_set &jobs, std::vector<std::size_t> indices)
            9 ( 0.00%)  				: Reduction_set(cpu_availability, jobs, indices, {})
       32,703 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >) (1x)
            .           			{
            .           			}
            .           
            .           			int get_number_of_jobs()
            .           			{
            .           				return jobs.size();
            .           			}
            .           
            .           			void initialize_key()
            .           			{
           34 ( 0.00%)  				for (const Job<Time> *j : jobs)
            .           				{
            8 ( 0.00%)  					key = key ^ j->get_key();
            .           				}
            .           			}
            .           
            .           			hash_value_t get_key() const
            .           			{
            .           				return key;
            .           			}
            .           
            .           			bool has_potential_deadline_misses() const
            .           			{
        5,002 ( 0.00%)  				if (deadline_miss)
            .           				{
            .           					return true;
            .           				}
            .           				return false;
            .           			}
            .           
       19,992 ( 0.00%)  			void add_job(const Job<Time> *jx, std::size_t index)
            .           			{
        2,499 ( 0.00%)  				deadline_miss = false;
        2,499 ( 0.00%)  				num_interfering_jobs_added++;
            9 ( 0.00%)  				jobs.push_back(jx);
            .           
            .           				index_by_job.emplace(jx->get_id(), index);
        7,497 ( 0.00%)  				job_by_index.emplace(std::make_pair(index, jobs.back()));
            9 ( 0.00%)  				indices.push_back(index);
            .           
        2,499 ( 0.00%)  				insert_sorted(jobs_by_latest_arrival, jx,
            .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
       48,842 ( 0.00%)  							  { if(i->latest_arrival() < j->latest_arrival()){
            .           							return true;
       15,069 ( 0.00%)  						  }else if(i->latest_arrival() > j->latest_arrival()){
            .           							return false;
            .           						  }else{
            .           							return i->get_priority() < j->get_priority();
            .           						  }; });
        2,499 ( 0.00%)  				insert_sorted(jobs_by_earliest_arrival, jx,
            .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
            .           							  { return i->earliest_arrival() < j->earliest_arrival(); });
            .           				// insert_sorted(jobs_by_wcet, jx,
            .           				//			  [](const Job<Time> *i, const Job<Time> *j) -> bool
            .           				//			  { return i->maximal_cost() < j->maximal_cost(); });
        2,499 ( 0.00%)  				insert_sorted(jobs_by_rmax_cmin, jx,
            .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
       97,712 ( 0.00%)  							  { return i->latest_arrival() + i->minimal_cost() < j->latest_arrival() + j->minimal_cost(); });
        2,499 ( 0.00%)  				insert_sorted(jobs_by_rmin_cmin, jx,
            .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
       97,712 ( 0.00%)  							  { return i->earliest_arrival() + i->minimal_cost() < j->earliest_arrival() + j->minimal_cost(); });
            .           
            .           				// latest_busy_time = compute_latest_busy_time();
        7,497 ( 0.00%)  				key = key ^ jx->get_key();
            .           				// latest_idle_time = compute_latest_idle_time();
            .           				//  compute_latest_start_times();
            .           				// compute_latest_start_time_complex();
            .           
        4,998 ( 0.00%)  				if (!jx->priority_at_least(max_priority))
            .           				{
          272 ( 0.00%)  					max_priority = jx->get_priority();
            .           				}
       14,994 ( 0.00%)  			}
            .           
            .           			void update_set()
            .           			{
        7,497 ( 0.00%)  				latest_idle_time = compute_latest_idle_time();
   54,068,286 ( 0.14%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() (2,499x)
        4,998 ( 0.00%)  				compute_latest_start_time_complex();
19,410,182,935 (50.76%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() (2,499x)
            .           			}
            .           
            .           			Job_set get_jobs() const
            .           			{
           16 ( 0.00%)  				return jobs;
       20,304 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::vector(std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&) (1x)
            .           			}
            .           
            .           			Time get_latest_busy_time() const
            .           			{
            .           				return latest_busy_time;
            .           			}
            .           
            .           			Time get_latest_idle_time() const
            .           			{
            .           				return latest_idle_time;
            .           			}
            .           
            .           			Time get_latest_LST()
            .           			{
   67,069,654 ( 0.18%)  				return latest_LST;
            .           			}
            .           
            .           			Job_map get_latest_start_times() const
            .           			{
            .           				return latest_start_times;
            .           			}
            .           
            .           			Time get_latest_start_time(const Job<Time> &job) const
            .           			{
            .           				auto iterator = latest_start_times.find(job.get_id());
        2,507 ( 0.00%)  				return iterator == latest_start_times.end() ? -1 : iterator->second;
            .           			}
            .           
            .           			Time earliest_finish_time(const Job<Time> &job) const
            .           			{
       12,535 ( 0.00%)  				return std::max(cpu_availability[0].min(), job.earliest_arrival()) + job.least_cost();
            .           			}
            .           
            .           			Time latest_finish_time(const Job<Time> &job) const
            .           			{
        2,507 ( 0.00%)  				return get_latest_start_time(job) + job.maximal_cost();
            .           			}
            .           
            .           			Priority compute_max_priority() const
            .           			{
            2 ( 0.00%)  				Priority max_prio{};
            .           
           35 ( 0.00%)  				for (const Job<Time> *j : jobs)
            .           				{
            .           					if (!j->priority_exceeds(max_prio))
            .           					{
            .           						max_prio = j->get_priority();
            .           					}
            .           				}
            .           
            .           				return max_prio;
-- line 405 ----------------------------------------
-- line 418 ----------------------------------------
            .           					Time LST_j = compute_latest_start_time(j);
            .           					latest_start_times.emplace(j->get_id(), LST_j);
            .           				}
            .           
            .           				return;
            .           			}
            .           
            .           			// Hierin gaan we voor iedere job de LST berekenen en opslaan dan hoeven we dat niet voor iedere job te doen opnieuw
       25,000 ( 0.00%)  			void compute_latest_start_time_complex()
            .           			{
            .           				// std::cout << "starting complex calculation :0" << std::endl;
            .           				latest_start_times = {};
            .           				std::vector<lst_Job<Time>> LST_list;
            .           				std::vector<const Job<Time> *> no_LST_list;
        7,500 ( 0.00%)  				typename std::vector<const Job<Time> *>::iterator lb_LPIW = jobs_by_earliest_arrival.begin();
   19,636,224 ( 0.05%)  				for (const Job<Time> *j : jobs_by_latest_arrival)
            .           				{
    3,143,750 ( 0.01%)  					Time HPIW = 0;
   34,581,250 ( 0.09%)  					std::vector<Time> BIW = complexBIW(LST_list, no_LST_list, lb_LPIW, j, HPIW);
4,232,992,119 (11.07%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) (3,143,750x)
   15,718,750 ( 0.04%)  					Time LST_j = complexHPIW(HPIW, lb_LPIW, BIW, j);
    6,287,500 ( 0.02%)  					latest_start_times.emplace(j->get_id(), LST_j);
    6,287,500 ( 0.02%)  					if (j->exceeds_deadline(LST_j + j->maximal_cost()))
            .           					{
            .           						deadline_miss = true;
            .           						return;
            .           					}
            .           				}
            .           
            .           				return;
       20,000 ( 0.00%)  			}
            .           
            .           			Time complexHPIW(Time HPIW, typename std::vector<const Job<Time> *>::iterator it_LPIW, std::vector<Time> BIW, const Job<Time> *j_i)
            .           			{
            .           				// ToDo friday:
            .           				// add the EQ
            .           				std::vector<Time> Ceq;
   34,581,250 ( 0.09%)  				for (int i = 1; i < cpu_availability.size(); i++)
            .           				{
   56,587,500 ( 0.15%)  					Ceq.emplace_back(BIW[i] - BIW[i - 1]);
2,540,291,644 ( 6.64%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (9,431,250x)
            .           				}
   56,587,500 ( 0.15%)  				for (int i = 1; i < cpu_availability.size() - 1; i++)
            .           				{
   25,150,000 ( 0.07%)  					Ceq[i] = Ceq[i] * (1 + i) + Ceq[i - 1];
            .           				}
            .           				Time ref = BIW[0];
            .           				Time HPIW_i = HPIW;
            .           
            .           				// initial LST using the HPIW from previous jobs
   18,862,500 ( 0.05%)  				Time LST_i = j_i->latest_arrival() + ref + HPIW;
   18,256,850 ( 0.05%)  				for (int i = 0; i < cpu_availability.size() - 1; i++)
            .           				{
   19,277,274 ( 0.05%)  					if (HPIW >= Ceq[cpu_availability.size() - 2 - i])
            .           					{
   36,067,044 ( 0.09%)  						LST_i = j_i->latest_arrival() + ref + BIW[cpu_availability.size() - 1 - i] + floor((HPIW - Ceq[cpu_availability.size() - 2 - i]) / (cpu_availability.size() - i));
            .           						break;
            .           					}
            .           				}
            .           
            .           				// we restart this loop where we ended when calculating the BIW
            .           				// this means that the earliest arrival of all the upcoming jobs are >= than the current job.
            .           				//	meaning that they shoudl never have an lst
            .           				typename std::vector<const Job<Time> *>::iterator it_HPIW = it_LPIW;
3,178,664,335 ( 8.31%)  				while (it_HPIW != jobs_by_earliest_arrival.end())
            .           				{
1,053,814,133 ( 2.76%)  					const Job<Time> *j_j = *it_HPIW;
            .           					// only take high/same prio jobs into account
5,269,070,665 (13.78%)  					if (j_j != j_i && j_j->get_priority() <= j_i->get_priority())
            .           					{
            .           						Time current_j_j_lst = j_j->latest_arrival();
    4,158,408 ( 0.01%)  						if (j_j->earliest_arrival() <= LST_i)
            .           						{
      990,716 ( 0.00%)  							if (current_j_j_lst >= j_i->latest_arrival())
            .           							{
      990,716 ( 0.00%)  								HPIW += j_j->maximal_cost();
      990,716 ( 0.00%)  								LST_i = j_i->latest_arrival() + ref + HPIW;
            .           
    3,005,971 ( 0.01%)  								for (int i = 0; i < cpu_availability.size() - 1; i++)
            .           								{
    2,510,613 ( 0.01%)  									if (HPIW >= Ceq[cpu_availability.size() - 2 - i])
            .           									{
    5,448,938 ( 0.01%)  										LST_i = j_i->latest_arrival() + ref + BIW[cpu_availability.size() - 1 - i] + floor((HPIW - Ceq[cpu_availability.size() - 2 - i]) / (cpu_availability.size() - i));
            .           										break;
            .           									}
            .           								}
            .           							}
            .           						}
            .           						else
            .           						{
    9,503,076 ( 0.02%)  							latest_LST = std::max(latest_LST, LST_i);
    1,583,846 ( 0.00%)  							return LST_i;
            .           						}
            .           					}
            .           					it_HPIW++;
            .           				}
    7,799,520 ( 0.02%)  				latest_LST = std::max(latest_LST, LST_i);
            .           				return LST_i;
            .           			}
            .           
   47,156,250 ( 0.12%)  			std::vector<Time> complexBIW(std::vector<lst_Job<Time>> &LST_list, std::vector<const Job<Time> *> &no_LST_list, typename std::vector<const Job<Time> *>::iterator &it_LPIW, const Job<Time> *j_i, Time &pre_calc_high)
            .           			{
            .           				// we will loop over this indefinately
            .           				std::vector<Time> Cmax;
            .           				// number of cores that we have
    3,143,750 ( 0.01%)  				int m = cpu_availability.size();
   47,156,250 ( 0.12%)  				for (int i = 0; i < m; i++)
            .           				{
            .           					// init this list with only zeroes
   50,300,000 ( 0.13%)  					Cmax.emplace_back((Time)0);
2,612,599,089 ( 6.83%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (12,575,000x)
            .           				}
            .           				// first, this list with jobs that we have an LST for
            .           				auto lst_it = LST_list.begin();
    9,480,085 ( 0.02%)  				while (lst_it != LST_list.end())
            .           				{
            .           					lst_Job<Time> &lst_job = *lst_it;
            .           					// we have to go through the entire list, as we cannot sort i properly since that is reliant on the current j_i->latest_arrival value.
            .           					// but we can remove jobs that we do not need anymore
            .           					// namely jobs that do not cross the current j_i->latest_arrival value anymore.
      140,855 ( 0.00%)  					if (lst_job.LST + lst_job.J->maximal_cost() <= j_i->latest_arrival())
            .           					{
            .           						// update the iterator and continue
            .           						lst_it = LST_list.erase(lst_it);
            .           						continue;
            .           					}
            .           					// okay so now we know that the job still traverses the r_i^max
       22,521 ( 0.00%)  					if (lst_job.J->get_priority() > j_i->get_priority())
            .           					{
            .           						// if its a lower prio job, then we add it as follows to the Cmax
        2,796 ( 0.00%)  						Time C_aug = lst_job.LST + lst_job.J->maximal_cost() - (j_i->latest_arrival() - 1);
        2,796 ( 0.00%)  						if (C_aug > Cmax[0])
            .           						{
            .           							linear_inserter(Cmax, C_aug);
            .           						}
            .           					}
            .           					else
            .           					{
            .           						// if its a high prio job, then we we can do two things
       12,218 ( 0.00%)  						if (lst_job.LST >= j_i->latest_arrival())
            .           						{
            .           							// if its larger or equal to the upcoming latest arrival, then add it to the HPIW already
            .           							// we're allowed to do this as we know that the earliest release is before rimax and the LST is after, so it can interfere
       12,218 ( 0.00%)  							pre_calc_high += lst_job.J->maximal_cost();
            .           						}
            .           						else
            .           						{
            .           							// otherwise we can do the same kind of calculation as performed for the Low prio interference :)
            .           							Time C_aug = lst_job.LST + lst_job.J->maximal_cost() - (j_i->latest_arrival() - 1);
            .           							if (C_aug > Cmax[0])
            .           							{
            .           								linear_inserter(Cmax, C_aug);
-- line 566 ----------------------------------------
-- line 568 ----------------------------------------
            .           						}
            .           					}
            .           					lst_it++;
            .           				}
            .           
            .           				// okay now we just have to check all jobs that had a higher rmax than the previous job, and thus did not have an LST yet
            .           				// std::cout << "we have " << LST_list.size() << " jobs without a LST" << std::endl;
            .           				auto no_lst_it = no_LST_list.begin();
   34,690,200 ( 0.09%)  				while (no_lst_it != no_LST_list.end())
            .           				{
   14,201,350 ( 0.04%)  					const Job<Time> *no_lst_job = *no_lst_it;
            .           
   28,402,700 ( 0.07%)  					if (no_lst_job == j_i)
            .           					{
            .           						no_lst_it++;
            .           						continue;
            .           					}
            .           					// first check if it might have an LST now
   33,180,300 ( 0.09%)  					if (no_lst_job->latest_arrival() >= j_i->latest_arrival())
            .           					{
            .           						// now we can assume that it doesnt have an LST yet (high prio ones will have but does not matter at this point)
   22,813,113 ( 0.06%)  						if (no_lst_job->get_priority() <= j_i->get_priority())
            .           						{
            .           							// so if its a higher prio job, then we can add it to HPIW as well
            .           
    6,394,920 ( 0.02%)  							pre_calc_high += no_lst_job->maximal_cost();
            .           						}
            .           						else
            .           						{
            .           							// otherwise its a lower prio job, and thus can block maximally so lets check if its large enough to block
            .           							Time C = no_lst_job->maximal_cost();
    8,813,822 ( 0.02%)  							if (C > Cmax[0])
            .           							{
   13,136,832 ( 0.03%)  								linear_inserter(Cmax, C);
  176,781,937 ( 0.46%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (4,378,944x)
            .           							}
            .           						}
            .           					}
            .           					else
            .           					{
            .           						// arriving here means that the job has a rmax lower than the current rmax, and thus MUST have an LST
   10,367,187 ( 0.03%)  						Time lst = latest_start_times[no_lst_job->get_id()];
   10,367,187 ( 0.03%)  						if (lst + no_lst_job->maximal_cost() <= j_i->latest_arrival())
            .           						{
            .           							// if it will never interfere, then just remove it from the list
            .           
            .           							no_lst_it = no_LST_list.erase(no_lst_it);
            .           							continue;
            .           						}
    1,005,432 ( 0.00%)  						if (no_lst_job->get_priority() <= j_i->get_priority())
            .           						{
       85,174 ( 0.00%)  							if (lst >= j_i->latest_arrival())
            .           							{
            .           								// if it has a higher priority then that still means that it can interfere with the current job and therefor we add it to the HPIW and the other list
       41,328 ( 0.00%)  								pre_calc_high += no_lst_job->maximal_cost();
            .           								// and higher prio jobs also have an LST already so lets move it to the other list
            .           								LST_list.emplace_back(lst_Job<Time>(no_lst_job, lst));
            .           								// and remove it from the current
            .           
            .           								no_lst_it = no_LST_list.erase(no_lst_it);
    3,141,249 ( 0.01%)  								continue;
            .           							}
            .           							else
            .           							{
            .           								// otherwise we can do the same kind of calculation as performed for the Low prio interference :)
       43,846 ( 0.00%)  								Time C_aug = lst + no_lst_job->maximal_cost() - (j_i->latest_arrival() - 1);
      336,403 ( 0.00%)  								if (C_aug > Cmax[0])
            .           								{
            .           									linear_inserter(Cmax, C_aug);
            .           								}
            .           							}
            .           						}
            .           						else
            .           						{
            .           							// so if its a lower prio job then we can
      585,114 ( 0.00%)  							if (lst >= j_i->latest_arrival())
            .           							{
            .           								Time C = no_lst_job->maximal_cost();
      110,038 ( 0.00%)  								if (C > Cmax[0])
            .           								{
      165,057 ( 0.00%)  									linear_inserter(Cmax, C);
    2,299,664 ( 0.01%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (55,019x)
            .           								}
            .           							}
            .           							else
            .           							{
            .           								// otherwise we can still do the augmented one
      475,076 ( 0.00%)  								Time C_aug = lst + no_lst_job->maximal_cost() - (j_i->latest_arrival() - 1);
      475,076 ( 0.00%)  								if (C_aug > Cmax[0])
            .           								{
      518,922 ( 0.00%)  									linear_inserter(Cmax, C_aug);
   11,409,039 ( 0.03%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (259,461x)
            .           								}
            .           							}
            .           						}
            .           					}
            .           					no_lst_it++;
            .           				}
            .           
   12,580,694 ( 0.03%)  				while (it_LPIW != jobs_by_earliest_arrival.end())
            .           				{
   12,558,612 ( 0.03%)  					const Job<Time> *LPIW_job = *it_LPIW;
   12,558,612 ( 0.03%)  					if (LPIW_job == j_i)
            .           					{
            .           						// if we encounter ourselves, then just place it in the no LST list for future
            .           						// std::cout<<"self encounter"<<std::endl;
            .           						no_LST_list.emplace_back(LPIW_job);
            .           						it_LPIW++;
            .           						continue;
            .           					}
   12,553,612 ( 0.03%)  					if (LPIW_job->earliest_arrival() >= j_i->latest_arrival())
            .           					{
            .           						// return where we've ended in this exploration so we can continue further at a later stage
            .           						break;
            .           					}
            .           					// all jobs that we encounter here are not present in the lists above since we will start from where we ended last time, so we'll never encounter the same job twice
            .           
    9,423,750 ( 0.02%)  					if (LPIW_job->get_priority() <= j_i->get_priority())
            .           					{
    1,418,408 ( 0.00%)  						if (LPIW_job->latest_arrival() >= j_i->latest_arrival())
            .           						{
    2,127,612 ( 0.01%)  							pre_calc_high += LPIW_job->maximal_cost();
            .           							// since its latest arrival is >= rimax, it wont have an LST so add it to the no LST list
            .           							// std::cout<<"\t pushed hi job" << LPIW_job->get_id() << " to no LST list" << std::endl;
            .           							no_LST_list.emplace_back(LPIW_job);
            .           						}
            .           						else
            .           						{
            .           							// since its latest arrival is < rimax it will have an LST, so it might be BIW
            .           							Time high_LST = latest_start_times[LPIW_job->get_id()];
-- line 694 ----------------------------------------
-- line 714 ----------------------------------------
            .           							// since we have an LST we add it to the LST list
            .           							// std::cout<<"\t pushed hi job " << LPIW_job->get_id() << " to LST list" << std::endl;
            .           							LST_list.emplace_back(lst_Job<Time>(LPIW_job, high_LST));
            .           						}
            .           					}
            .           					else
            .           					{
            .           						// if its a lower prio job then
    4,864,092 ( 0.01%)  						if (LPIW_job->latest_arrival() >= j_i->latest_arrival())
            .           						{
            .           							Time C = LPIW_job->maximal_cost();
    4,864,092 ( 0.01%)  							if (C > Cmax[0])
            .           							{
    4,638,918 ( 0.01%)  								linear_inserter(Cmax, C);
   86,028,683 ( 0.22%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (2,319,459x)
            .           							}
            .           							// std::cout<<"\t pushed lo job" << LPIW_job->get_id() << " to no LST list" << std::endl;
            .           							no_LST_list.emplace_back(LPIW_job);
            .           						}
            .           						else
            .           						{
            .           							// again this means that the latest arrival < rmax so it must ahve an LST
            .           							Time low_LST = latest_start_times[LPIW_job->get_id()];
-- line 735 ----------------------------------------
-- line 764 ----------------------------------------
            .           					}
            .           
            .           					it_LPIW++;
            .           				}
            .           				//  okay now we have the maximum interference that can be caused by jobs that start before j_i
            .           				//  we just need to take the system availabilites into consideration now.
            .           				//  Cmax  is sorted low -> high
            .           				//  A^max is sorted low -> high
   62,218,365 ( 0.16%)  				for (int i = 0; i < m; i++)
            .           				{
            .           					// highest value from Cmax
   28,293,750 ( 0.07%)  					Time Cmax_i = Cmax[m - 1 - i];
            .           					// compared to the lowest value from A^max
            .           					Time A_max_i = cpu_availability[i].max();
            .           					// compared to the current job offset
            .           					// r_i^max
   88,025,000 ( 0.23%)  					Cmax[m - 1 - i] = std::max(j_i->latest_arrival() - 1 + Cmax_i, std::max(A_max_i, j_i->latest_arrival())) - j_i->latest_arrival();
            .           				}
            .           				// make sure its a ascending list
            .           				std::sort(Cmax.begin(), Cmax.end());
            .           				return Cmax;
   28,293,750 ( 0.07%)  			}
            .           
            .           			void linear_inserter(std::vector<Time> &list, Time object)
            .           			{
            .           				Time swap;
    7,014,281 ( 0.02%)  				list[0] = object;
    7,014,281 ( 0.02%)  				int m = cpu_availability.size() - 1;
   86,554,489 ( 0.23%)  				for (int i = 0; i < m; i++)
            .           				{
   79,296,548 ( 0.21%)  					if (list[i] > list[i + 1])
            .           					{
            .           						swap = list[i + 1];
            .           						list[i + 1] = list[i];
   68,644,112 ( 0.18%)  						list[i] = swap;
            .           					}
            .           					else
            .           					{
            .           						return;
            .           					}
            .           				}
    7,012,883 ( 0.02%)  			}
            .           
            .           			Time compute_latest_start_time(const Job<Time> *j_i)
            .           			{
            .           
            .           				//  Blocking interfering workload for job j_i
            .           
            .           				Time Ceq[cpu_availability.size() - 1];
            .           				Time BIW[cpu_availability.size()];
-- line 813 ----------------------------------------
-- line 1074 ----------------------------------------
            .           					{
            .           						// std::cout<<"initial deadline miss for " << j->get_id() << std::endl;
            .           						deadline_miss = true;
            .           						return;
            .           					}
            .           				}
            .           			}
            .           
       20,000 ( 0.00%)  			Time compute_latest_idle_time()
            .           			{
            .           				// remove
            .           				Time latest_idle_time{-1};
            .           				// this is a counter to check how much jobs will always overlap with the release interval of the current job, if its > m then there will never be an interval at that point
            .           				Time overflow = 0;
            .           				// Save the m-1 largest jobs
       10,000 ( 0.00%)  				Time Cmax[cpu_availability.size() - 1];
            .           				// Sum to add the remaining workload
            .           				Time Crest = 0;
            .           				// for convenience sake, remember the earliest time a core might become free
        5,000 ( 0.00%)  				Time Amin = cpu_availability.front().min();
            .           
            .           				// std::cout << "Finding idle intervals..." << std::endl;
            .           				// std::cout << jobs_by_latest_arrival.size() << std::endl;
        5,000 ( 0.00%)  				for (auto it_x = jobs_by_latest_arrival.rbegin(); it_x != jobs_by_latest_arrival.rend(); it_x++)
            .           				{
        2,500 ( 0.00%)  					const Job<Time> *j_x = *it_x;
            .           					// std::cout << "Looking for idle interval for job " << j_x->get_id() << " possibly ending at " << j_x->latest_arrival() << std::endl;
            .           					//  reset the overflow and Crest values for each job under investigation
        2,500 ( 0.00%)  					overflow = 0;
        5,000 ( 0.00%)  					Crest = 0;
        2,500 ( 0.00%)  					int A_i = 0;
            .           					// Compute the interference from possible previous states
       32,500 ( 0.00%)  					for (Interval<Time> A : cpu_availability)
            .           					{
            .           						// since we dont use the first value as it is always 0
            .           						if (A_i != 0)
            .           						{
            .           							// Save the interfering workloads as the initial "highest workloads"
       15,000 ( 0.00%)  							Cmax[A_i - 1] = A.min() - Amin;
            .           							// std::cout << "\t" << Cmax[A_i - 1] << std::endl;
            .           						}
        2,500 ( 0.00%)  						A_i++;
            .           					}
            .           
            .           					// Now we again need to iterate over this list
            .           					// i think i can set it_y to it_x, to limit the search
    6,287,500 ( 0.02%)  					for (auto it_y = jobs_by_latest_arrival.rbegin(); it_y != jobs_by_latest_arrival.rend(); it_y++)
            .           					{
    3,143,750 ( 0.01%)  						const Job<Time> *j_y = *it_y;
    6,287,500 ( 0.02%)  						if (j_y->latest_arrival() < j_x->latest_arrival())
            .           						{
            .           							// this statement checks the overflow condition
    6,282,500 ( 0.02%)  							if (j_y->earliest_arrival() + j_y->minimal_cost() >= j_x->latest_arrival())
            .           							{
   12,565,000 ( 0.03%)  								overflow++;
            .           							}
            .           							// if the current minimal cost is larger than the smallest saved value then add that value to Crest and insert the new value linearly
    9,423,750 ( 0.02%)  							if (j_y->minimal_cost() > Cmax[0])
            .           							{
       17,442 ( 0.00%)  								Crest += Cmax[0];
       17,442 ( 0.00%)  								Cmax[0] = j_y->minimal_cost();
            .           								Time swap;
      129,792 ( 0.00%)  								for (int i = 0; i < cpu_availability.size() - 2; i++)
            .           								{
      117,628 ( 0.00%)  									if (Cmax[i] > Cmax[i + 1])
            .           									{
            .           										swap = Cmax[i + 1];
            .           										Cmax[i + 1] = Cmax[i];
       80,032 ( 0.00%)  										Cmax[i] = swap;
            .           									}
            .           									else
            .           									{
            .           										break;
            .           									}
            .           								}
            .           							}
            .           							else
            .           							{
            .           								// if its smaller than all values in Cmax, just add it to Crest
    3,123,808 ( 0.01%)  								Crest += j_y->minimal_cost();
            .           							}
            .           						}
            .           					}
            .           					/*
            .           					There is an interval if the workload of all jobs except the m-1 largest jobs spread over all cores
            .           					is smaller than the latest release time of the current job under investigation
            .           					*/
      120,000 ( 0.00%)  					if (Amin + ceil((double)Crest / (double)cpu_availability.size()) < j_x->latest_arrival() && overflow < cpu_availability.size())
            .           					{
            .           
            .           						// std::cout << "\t Latest interval might end at:" << j_x->latest_arrival() << std::endl;
            .           						return j_x->latest_arrival();
            .           					}
            .           				}
            .           
            .           				// std::cout << "\t No interval possible in this set" << std::endl;
            .           				//  no jobs can be released at -1 so we can safely return 0 if there is no interval anywhere.
            .           				return 0;
       22,500 ( 0.00%)  			}
            .           
            .           			unsigned long get_num_interfering_jobs_added() const
            .           			{
            .           				return num_interfering_jobs_added;
            .           			}
            .           
            .           			bool can_interfere(const Job<Time> &job) const
            .           			{
            .           				// find_if geeft een pointer naar de eerste value die overeenkomt naar de eerset value in die lijst
            .           				auto pos = std::find_if(jobs.begin(), jobs.end(),
            .           										[&job](const Job<Time> *j)
            .           										{ return j->get_id() == job.get_id(); });
            .           
            .           				// als die pointer naar de laatste plek wijst, dan betekent dat dat deze dus nog niet in de lijst zit en daarom dus.. buiten de lijst zit
    5,563,508 ( 0.01%)  				if (pos != jobs.end())
            .           				{
            .           					// std::cout << "i already use job " << job.get_id() << std::endl;
            .           					return false;
            .           				}
            .           
            .           				// rx_min < delta_M
            .           				// latest idle time is het einde van de laatste idle interval in de set, dus die moet steeds herberekend
            .           				// worden voor we nieuwe interfering jobs gaan zoekn
    4,437,074 ( 0.01%)  				if (job.earliest_arrival() <= latest_idle_time)
            .           				{
            .           					// std::cout << "Interference due to idle period " << job.get_id() << std::endl;
            .           					return true;
            .           				}
            .           
            .           				// min_wcet wordt hier niet gebruikt dus waarom doen we dit?
            .           				// Time min_wcet = min_lower_priority_wcet(job);
            .           
-- line 1204 ----------------------------------------
-- line 1223 ----------------------------------------
            .           				}
            .           
            .           				return false;
            .           			}
            .           
            .           			void certainly_available_i(Time *Chigh, Time Clow, Time s, std::vector<Time> *CA_values)
            .           			{
            .           				Time tClow = Clow;
            2 ( 0.00%)  				if (cpu_availability.size() == 1)
            .           				{
       10,650 ( 0.00%)  					CA_values->emplace_back(s + Clow + Chigh[0]);
            .           					return;
            .           				}
            .           
            2 ( 0.00%)  				if (Chigh[0] <= 0)
            .           				{
           15 ( 0.00%)  					for (int i = 0; i < cpu_availability.size(); i++)
            .           					{
           12 ( 0.00%)  						if (Chigh[i] > 0)
            .           						{
           17 ( 0.00%)  							CA_values->emplace_back(s + Chigh[i]);
           23 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (1x)
            .           						}
            .           						else
            .           						{
            .           							CA_values->emplace_back(s);
            .           						}
            .           					}
            .           				}
            .           				else
-- line 1251 ----------------------------------------
-- line 1281 ----------------------------------------
            .           							// we have enough to properly equalize at this point
            .           							CA_values->emplace_back(s + floor((double)eq / (double)cpu_availability.size()) + Chigh[cpu_availability.size() - 1]);
            .           						}
            .           						Clow += Chigh[i - 1];
            .           					}
            .           				}
            .           			}
            .           
           11 ( 0.00%)  			std::vector<Time> compute_certainly_available()
            .           			{
            .           				// std::cout<<"\ncomputing CA"<<std::endl;
            .           				std::vector<Time> CA_values;
           11 ( 0.00%)  				Time Chigh[cpu_availability.size()];
            1 ( 0.00%)  				Time Clow = 0;
            .           
            2 ( 0.00%)  				Time last_event = -1;
            2 ( 0.00%)  				Time s = std::max(cpu_availability[0].max(), jobs_by_latest_arrival[0]->latest_arrival());
            .           
            .           				Time event = s;
            .           
            1 ( 0.00%)  				Time prev = 0;
           30 ( 0.00%)  				for (int i = 1; i < cpu_availability.size(); i++)
            .           				{
           12 ( 0.00%)  					prev += cpu_availability[i].max() - cpu_availability[0].max();
            .           				}
           29 ( 0.00%)  				prev = cpu_availability[0].max() + ceil((double)prev / (double)cpu_availability.size());
            .           
            .           				// for some reason i need to have this print otherwise i have a seg fault?!?
            .           				// std::cout<<"\t seg fault prevention?!?"<<std::endl;
            .           				//  initialize the chigh values for the first iterative step
           17 ( 0.00%)  				for (int Chigh_i = 0; Chigh_i < cpu_availability.size(); Chigh_i++)
            .           				{
            .           					// std::cout<<cpu_availability[Chigh_i].max()<<" - ";
           15 ( 0.00%)  					Chigh[Chigh_i] = cpu_availability[Chigh_i].max() - s;
            .           					// std::cout<<Chigh[Chigh_i]<<std::endl;
            .           				}
            .           				// std::cout<<"\tChihgh Filled"<<std::endl;
       10,031 ( 0.00%)  				for (const Job<Time> *j_x : jobs_by_latest_arrival)
            .           				{
            .           
       10,028 ( 0.00%)  					if (j_x->latest_arrival() <= event && j_x->latest_arrival() > last_event)
            .           					{
            .           						// std::cout << j_x->get_id() << std::endl;
        1,465 ( 0.00%)  						if (j_x->maximal_cost() > Chigh[0])
            .           						{
            .           							if (Chigh[0] > 0)
        2,196 ( 0.00%)  								Clow += Chigh[0];
          732 ( 0.00%)  							Chigh[0] = j_x->maximal_cost();
            .           							Time swap;
            .           							// its in this point i think,
        8,691 ( 0.00%)  							for (int i = 0; i < cpu_availability.size() - 1; i++)
            .           							{
        8,496 ( 0.00%)  								if (Chigh[i] > Chigh[i + 1])
            .           								{
            .           									swap = Chigh[i];
        7,684 ( 0.00%)  									Chigh[i] = Chigh[i + 1];
            .           									Chigh[i + 1] = swap;
            .           								}
            .           								else
            .           								{
            .           									break;
            .           								}
            .           							}
            .           						}
            .           						else
            .           						{
          732 ( 0.00%)  							Clow += j_x->maximal_cost();
            .           						}
            .           
        2,928 ( 0.00%)  						if (Chigh[0] > 0)
            .           						{
          598 ( 0.00%)  							event = s + ceil((double)(Clow + Chigh[0]) / (double)cpu_availability.size());
            .           						}
            .           						else
            .           						{
            .           							event = s;
            .           						}
            .           					}
            .           					else
            .           					{
            .           						// std::cout << "\t new iteration" << std::endl;
            .           						last_event = event;
        3,550 ( 0.00%)  						if (Chigh[0] > 0)
            .           						{
            .           							prev = Clow;
          400 ( 0.00%)  							for (int i = 0; i < cpu_availability.size(); i++)
            .           							{
          406 ( 0.00%)  								prev += (Chigh[i] >= 0) ? Chigh[i] : 0;
            .           							}
          552 ( 0.00%)  							prev = s + ceil((double)prev / (double(cpu_availability.size())));
            .           						}
            .           						// std::cout << "prev" << prev << std::endl;
            .           						s = j_x->latest_arrival();
            .           
       30,175 ( 0.00%)  						for (int i = 0; i < cpu_availability.size(); i++)
            .           						{
            .           
       21,300 ( 0.00%)  							if (Chigh[i] > 0)
            .           							{
       12,304 ( 0.00%)  								Chigh[i] = (0 > ((event + Chigh[i]) - s)) ? prev - s : ((event + Chigh[i]) - s);
            .           							}
            .           							else
            .           							{
        9,349 ( 0.00%)  								Chigh[i] = prev - s;
            .           							}
            .           							// std::cout << Chigh[i] << " ";
            .           						}
            .           						// std::cout << std::endl;
            .           						//  reset Clow
            .           						Clow = 0;
        5,325 ( 0.00%)  						if (j_x->maximal_cost() > Chigh[0])
            .           						{
            .           							Clow += Chigh[0];
        1,775 ( 0.00%)  							Chigh[0] = j_x->maximal_cost();
            .           							Time swap = 0;
       17,870 ( 0.00%)  							for (int i = 0; i < cpu_availability.size() - 1; i++)
            .           							{
       21,300 ( 0.00%)  								if (Chigh[i] > Chigh[i + 1])
            .           								{
            .           									swap = Chigh[i];
       19,088 ( 0.00%)  									Chigh[i] = Chigh[i + 1];
            .           									Chigh[i + 1] = swap;
            .           								}
            .           								else
            .           								{
            .           									break;
            .           								}
            .           							}
            .           						}
-- line 1409 ----------------------------------------
-- line 1411 ----------------------------------------
            .           						{
            .           							Clow += j_x->maximal_cost();
            .           						}
            .           						event = s;
            .           					}
            .           				}
            .           				certainly_available_i(Chigh, Clow, s, &CA_values);
            .           				return CA_values;
            9 ( 0.00%)  			}
            .           
            .           			void possibly_available_pre_filter(int *LFP, int i)
            .           			{
            .           				// create an empty temporary set
            .           				Job_set J_temp{};
            3 ( 0.00%)  				int m = cpu_availability.size();
            .           
            .           				// Initialize the LFP array
            6 ( 0.00%)  				for (int j = 0; j < m; j++)
            .           				{
           18 ( 0.00%)  					LFP[j] = 0; // Set a default value here, or you can choose any appropriate initial value
           33 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (3x)
            .           				}
            .           				// std::cout << "\t PA_" << i << " for " << m << " cores i have " << jobs_by_rmax_cmin.size() << " jobs to use" << std::endl;
           70 ( 0.00%)  				for (auto it_x = jobs_by_rmax_cmin.rbegin(); it_x != jobs_by_rmax_cmin.rend(); it_x++)
            .           				{
           70 ( 0.00%)  					const Job<Time> *j_x = *it_x;
            .           
           85 ( 0.00%)  					if (J_temp.size() < m - i)
            .           					{
            .           						// remove all elementes from J_temp that exceed the latest finish time of the new job j_x as long as J_temp is not full
           64 ( 0.00%)  						Time newLFP = j_x->latest_arrival() + j_x->minimal_cost();
            .           						// std::cout << "\tnew LFP to add is " << newLFP << std::endl;
            .           						J_temp.erase(std::remove_if(J_temp.begin(),
            .           													J_temp.end(),
            .           													[&newLFP](const Job<Time> *j_y)
            .           													{ return j_y->latest_arrival() > newLFP; }),
            .           									 J_temp.end());
            .           						// afterwards we just add j_x
            .           						J_temp.emplace_back(j_x);
-- line 1448 ----------------------------------------
-- line 1450 ----------------------------------------
            .           					else
            .           					{
            .           						break;
            .           					}
            .           				}
            .           				// Now that J_temp is filled, we can calculate the m-i latest finish points using only m-i cores
            .           				int LFP_i = 0;
            .           				// std::cout << "\tComputed " << J_temp.size() << " values for the LFP are" << std::endl;
           51 ( 0.00%)  				for (const Job<Time> *j_t : J_temp)
            .           				{
           18 ( 0.00%)  					LFP[LFP_i] = j_t->latest_arrival() + j_t->minimal_cost();
            .           					LFP_i++;
            .           				}
            .           			}
            .           
            .           			Time possibly_available_i(int *LFP, int i)
            .           			{
            .           				int m = cpu_availability.size();
            .           				// std::cout << m << "-" << i << "-" << 2 << "=" << m - i - 2 << std::endl;
           24 ( 0.00%)  				int Cmax[m - 1 - i];
            6 ( 0.00%)  				int Crest = 0;
            .           				// just a really small sort, but can be improved upon. so TODO
           12 ( 0.00%)  				std::sort(LFP, LFP + (m - i));
            .           				// This is the latest end point.
            3 ( 0.00%)  				int LEP = LFP[m - i - 1];
            6 ( 0.00%)  				if (m - i - 1 > 0)
            .           				{
            .           					// create the initial Cmax values;
           23 ( 0.00%)  					for (int Cmax_i = 0; Cmax_i < m - i - 1; Cmax_i++)
            .           					{
           13 ( 0.00%)  						Cmax[Cmax_i] = LEP - LFP[Cmax_i];
            .           					}
            .           				}
            .           				// std::cout<<"init CMax"<<std::endl;
            .           
       15,048 ( 0.00%)  				for (auto it_s = jobs_by_rmin_cmin.rbegin(); it_s != jobs_by_rmin_cmin.rend(); it_s++)
            .           				{
        7,521 ( 0.00%)  					const Job<Time> *j_s = *it_s;
       15,048 ( 0.00%)  					if (j_s->latest_arrival() < LFP[m - i - 1])
            .           					{
            .           						// we should also check if A^max is less than that value i think.
      224,853 ( 0.00%)  						if (j_s->earliest_arrival() + j_s->minimal_cost() > LEP - ceil((double)Crest / (double)(m - i)))
            .           						{
            .           							// here we have to use the i'th core
            .           							// std::cout << "\tWe have to use the i^th core at time " << j_s->earliest_arrival() + j_s->minimal_cost() << std::endl;
            .           							return j_s->earliest_arrival() + j_s->minimal_cost();
            .           						}
            .           						// we cannot use Cmax if we have only one core to fill
       14,990 ( 0.00%)  						if (m - i - 1 > 0)
            .           						{
       14,985 ( 0.00%)  							if (j_s->minimal_cost() < Cmax[m - i - 2])
            .           							{
            .           								Crest += j_s->minimal_cost();
            .           							}
            .           							else
            .           							{
            .           								// insert it into the Cmax array
            .           								// we know for certain that Cmax[m-i-2] has to be added to Crest
          544 ( 0.00%)  								Crest += Cmax[m - i - 2];
          544 ( 0.00%)  								Cmax[m - i - 2] = j_s->minimal_cost();
            .           								Time swap;
        2,720 ( 0.00%)  								for (int Cmax_i = m - i - 2; Cmax_i > 0; Cmax_i--)
            .           								{
        1,355 ( 0.00%)  									if (Cmax[Cmax_i - 1] < Cmax[i])
            .           									{
            .           										swap = Cmax[Cmax_i - 1];
            2 ( 0.00%)  										Cmax[Cmax_i - 1] = Cmax[Cmax_i];
            1 ( 0.00%)  										Cmax[Cmax_i] = swap;
            .           									}
            .           									else
            .           									{
            .           										break;
            .           									}
            .           								}
            .           							}
            .           						}
            .           						else
            .           						{
        6,951 ( 0.00%)  							Crest += j_s->minimal_cost();
            .           						}
            .           					}
            .           				}
           18 ( 0.00%)  				return cpu_availability[i - 1].min();
            .           			}
            .           
            .           			Time possibly_available_m()
            .           			{
            .           				Time Ctot = 0;
            .           				Time maxEnd = 0;
            1 ( 0.00%)  				Time least_start_time = jobs[0]->earliest_arrival();
            .           
       10,030 ( 0.00%)  				for (const Job<Time> *j : jobs)
            .           				{
        7,518 ( 0.00%)  					maxEnd = (maxEnd < j->earliest_arrival() + j->minimal_cost()) ? j->earliest_arrival() + j->minimal_cost() : maxEnd;
        7,518 ( 0.00%)  					Ctot += j->minimal_cost();
            .           					least_start_time = std::min(least_start_time, j->earliest_arrival());
            .           				}
            .           				// this returns 0 + that value so the further we go the worse this bound gets
            .           				// We have two options,  or we do it by setting the value to Amin + bound
            .           				//						or we determine the earliest time that A job can start in this set and then take the maximum between that and the Amin
           33 ( 0.00%)  				return std::max(cpu_availability[0].min(), least_start_time) + (ceil((double)Ctot / (double)cpu_availability.size()) > maxEnd) ? ceil((double)Ctot / (double)cpu_availability.size()) : maxEnd;
            .           			}
            .           
           10 ( 0.00%)  			std::vector<Time> compute_possibly_available()
            .           			{
            3 ( 0.00%)  				int LFP[cpu_availability.size()];
            3 ( 0.00%)  				std::vector<Time> PA_values{};
           22 ( 0.00%)  				for (int i = 1; i < cpu_availability.size(); i++)
            .           				{
            .           					possibly_available_pre_filter(LFP, i);
            3 ( 0.00%)  					PA_values.push_back(possibly_available_i(LFP, i));
            .           				}
            1 ( 0.00%)  				PA_values.push_back(possibly_available_m());
            .           
            .           				return PA_values;
            9 ( 0.00%)  			}
            .           
            .           			void compute_new_system_states()
            .           			{
            .           				std::cout << "\tCores are Possibly available at times: ";
            .           				for (Time PA : compute_possibly_available())
            .           				{
            .           					std::cout << PA << " ";
            .           				}
-- line 1573 ----------------------------------------
-- line 1605 ----------------------------------------
            .           				std::cout << "Where the new system states would be:" << std::endl;
            .           				compute_new_system_states();
            .           			}
            .           		};
            .           
            .           		template <typename T, typename Compare>
            .           		typename std::vector<T>::iterator insert_sorted(std::vector<T> &vec, const T &item, Compare comp)
            .           		{
       37,485 ( 0.00%)  			return vec.insert(std::upper_bound(vec.begin(), vec.end(), item, comp), item);
    4,559,398 ( 0.01%)  => /usr/include/c++/10/bits/vector.tcc:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::insert(__gnu_cxx::__normal_iterator<NP::Job<long long> const* const*, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, NP::Job<long long> const* const&) (9,996x)
            .           		}
            .           
            .           		template <class Time>
            6 ( 0.00%)  		class Reduction_set_statistics
            .           		{
            .           
            .           		public:
            .           			bool reduction_success;
            .           
            .           			unsigned long num_jobs, num_interfering_jobs_added;
            .           
            .           			std::vector<Time> priorities;
            .           
            9 ( 0.00%)  			Reduction_set_statistics(bool reduction_success, Reduction_set<Time> &reduction_set)
            .           				: reduction_success{reduction_success},
            .           				  num_jobs{reduction_set.get_jobs().size()},
            .           				  num_interfering_jobs_added{reduction_set.get_num_interfering_jobs_added()},
            4 ( 0.00%)  				  priorities{}
            .           			{
        7,523 ( 0.00%)  				for (const Job<Time> *j : reduction_set.get_jobs())
            .           				{
            .           					priorities.push_back(j->get_priority());
            .           				}
            8 ( 0.00%)  			}
            .           		};
            .           
            .           	}
            .           
            .           };
            .           
            .           #endif
--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/vector.tcc
--------------------------------------------------------------------------------
Ir                   

-- line 101 ----------------------------------------
          .           #if __cplusplus >= 201103L
          .             template<typename _Tp, typename _Alloc>
          .               template<typename... _Args>
          .           #if __cplusplus > 201402L
          .                 typename vector<_Tp, _Alloc>::reference
          .           #else
          .                 void
          .           #endif
176,052,556 ( 0.46%)        vector<_Tp, _Alloc>::
          .                 emplace_back(_Args&&... __args)
          .                 {
 79,215,502 ( 0.21%)  	if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
          .           	  {
          .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
          .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
          .           				     std::forward<_Args>(__args)...);
 16,067,189 ( 0.04%)  	    ++this->_M_impl._M_finish;
          .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
          .           	  }
          .           	else
     76,538 ( 0.00%)  	  _M_realloc_insert(end(), std::forward<_Args>(__args)...);
        875 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<Interval<long long>, std::allocator<Interval<long long> > >::_M_realloc_insert<long long&, long long&>(__gnu_cxx::__normal_iterator<Interval<long long>*, std::vector<Interval<long long>, std::allocator<Interval<long long> > > >, long long&, long long&) (3x)
          .           #if __cplusplus > 201402L
          .           	return back();
          .           #endif
176,052,538 ( 0.46%)        }
          .           #endif
          .           
          .             template<typename _Tp, typename _Alloc>
          .               typename vector<_Tp, _Alloc>::iterator
     69,972 ( 0.00%)      vector<_Tp, _Alloc>::
          .           #if __cplusplus >= 201103L
          .               insert(const_iterator __position, const value_type& __x)
          .           #else
          .               insert(iterator __position, const value_type& __x)
          .           #endif
          .               {
          .                 const size_type __n = __position - begin();
     29,988 ( 0.00%)        if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
     19,920 ( 0.00%)  	if (__position == end())
          .           	  {
          .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
          .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
          .           				     __x);
     12,836 ( 0.00%)  	    ++this->_M_impl._M_finish;
          .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
          .           	  }
          .           	else
          .           	  {
          .           #if __cplusplus >= 201103L
          .           	    const auto __pos = begin() + (__position - cbegin());
          .           	    // __x could be an existing element of this vector, so make a
          .           	    // copy of it before _M_insert_aux moves elements around.
-- line 152 ----------------------------------------
-- line 153 ----------------------------------------
          .           	    _Temporary_value __x_copy(this, __x);
          .           	    _M_insert_aux(__pos, std::move(__x_copy._M_val()));
          .           #else
          .           	    _M_insert_aux(__position, __x);
          .           #endif
          .           	  }
          .                 else
          .           #if __cplusplus >= 201103L
         72 ( 0.00%)  	_M_realloc_insert(begin() + (__position - cbegin()), __x);
    113,330 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::_M_realloc_insert<NP::Job<long long> const* const&>(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, NP::Job<long long> const* const&) (36x)
          .           #else
          .           	_M_realloc_insert(__position, __x);
          .           #endif
          .           
     14,232 ( 0.00%)        return iterator(this->_M_impl._M_start + __n);
     62,852 ( 0.00%)      }
          .           
          .             template<typename _Tp, typename _Alloc>
          .               typename vector<_Tp, _Alloc>::iterator
          .               vector<_Tp, _Alloc>::
          .               _M_erase(iterator __position)
          .               {
  6,323,826 ( 0.02%)        if (__position + 1 != end())
          .           	_GLIBCXX_MOVE3(__position + 1, end(), __position);
 12,607,935 ( 0.03%)        --this->_M_impl._M_finish;
          .                 _Alloc_traits::destroy(this->_M_impl, this->_M_impl._M_finish);
          .                 _GLIBCXX_ASAN_ANNOTATE_SHRINK(1);
          .                 return __position;
          .               }
          .           
          .             template<typename _Tp, typename _Alloc>
          .               typename vector<_Tp, _Alloc>::iterator
          .               vector<_Tp, _Alloc>::
          .               _M_erase(iterator __first, iterator __last)
          .               {
          8 ( 0.00%)        if (__first != __last)
          .           	{
          .           	  if (__last != end())
          .           	    _GLIBCXX_MOVE3(__last, end(), __first);
          .           	  _M_erase_at_end(__first.base() + (end() - __last));
          .           	}
          .                 return __first;
          .               }
          .           
-- line 195 ----------------------------------------
-- line 399 ----------------------------------------
          .               void
          .               vector<_Tp, _Alloc>::
          .               _M_insert_aux(iterator __position, const _Tp& __x)
          .           #endif
          .               {
          .                 _GLIBCXX_ASAN_ANNOTATE_GROW(1);
          .                 _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
          .           			       _GLIBCXX_MOVE(*(this->_M_impl._M_finish - 1)));
      7,084 ( 0.00%)        ++this->_M_impl._M_finish;
          .                 _GLIBCXX_ASAN_ANNOTATE_GREW(1);
          .           #if __cplusplus < 201103L
          .                 _Tp __x_copy = __x;
          .           #endif
      7,084 ( 0.00%)        _GLIBCXX_MOVE_BACKWARD3(__position.base(),
          .           			      this->_M_impl._M_finish - 2,
          .           			      this->_M_impl._M_finish - 1);
          .           #if __cplusplus < 201103L
          .                 *__position = __x_copy;
          .           #else
      7,084 ( 0.00%)        *__position = std::forward<_Arg>(__arg);
          .           #endif
          .               }
          .           
          .           #if __cplusplus >= 201103L
          .             template<typename _Tp, typename _Alloc>
          .               template<typename... _Args>
          .                 void
 19,364,713 ( 0.05%)        vector<_Tp, _Alloc>::
          .                 _M_realloc_insert(iterator __position, _Args&&... __args)
          .           #else
          .             template<typename _Tp, typename _Alloc>
          .               void
          .               vector<_Tp, _Alloc>::
          .               _M_realloc_insert(iterator __position, const _Tp& __x)
          .           #endif
          .               {
-- line 434 ----------------------------------------
-- line 441 ----------------------------------------
          .                 pointer __new_finish(__new_start);
          .                 __try
          .           	{
          .           	  // The order of the three operations is dictated by the C++11
          .           	  // case, where the moves could alter a new element belonging
          .           	  // to the existing vector.  This is an issue only for callers
          .           	  // taking the element by lvalue ref (see last bullet of C++11
          .           	  // [res.on.arguments]).
         30 ( 0.00%)  	  _Alloc_traits::construct(this->_M_impl,
          .           				   __new_start + __elems_before,
          .           #if __cplusplus >= 201103L
          .           				   std::forward<_Args>(__args)...);
          .           #else
          .           				   __x);
          .           #endif
          .           	  __new_finish = pointer();
          .           
          .           #if __cplusplus >= 201103L
          .           	  if _GLIBCXX17_CONSTEXPR (_S_use_relocate())
          .           	    {
          .           	      __new_finish = _S_relocate(__old_start, __position.base(),
          .           					 __new_start, _M_get_Tp_allocator());
          .           
 18,917,192 ( 0.05%)  	      ++__new_finish;
          .           
          .           	      __new_finish = _S_relocate(__position.base(), __old_finish,
          .           					 __new_finish, _M_get_Tp_allocator());
          .           	    }
          .           	  else
          .           #endif
          .           	    {
          .           	      __new_finish
          .           		= std::__uninitialized_move_if_noexcept_a
          .           		(__old_start, __position.base(),
          .           		 __new_start, _M_get_Tp_allocator());
          .           
      2,658 ( 0.00%)  	      ++__new_finish;
          .           
          .           	      __new_finish
          .           		= std::__uninitialized_move_if_noexcept_a
          .           		(__position.base(), __old_finish,
          .           		 __new_finish, _M_get_Tp_allocator());
          .           	    }
          .           	}
          .                 __catch(...)
-- line 485 ----------------------------------------
-- line 493 ----------------------------------------
          .           	  __throw_exception_again;
          .           	}
          .           #if __cplusplus >= 201103L
          .                 if _GLIBCXX17_CONSTEXPR (!_S_use_relocate())
          .           #endif
          .           	std::_Destroy(__old_start, __old_finish, _M_get_Tp_allocator());
          .                 _GLIBCXX_ASAN_ANNOTATE_REINIT;
          .                 _M_deallocate(__old_start,
 25,286,921 ( 0.07%)  		    this->_M_impl._M_end_of_storage - __old_start);
 75,682,492 ( 0.20%)        this->_M_impl._M_start = __new_start;
          .                 this->_M_impl._M_finish = __new_finish;
 56,782,442 ( 0.15%)        this->_M_impl._M_end_of_storage = __new_start + __len;
    464,696 ( 0.00%)      }
          .           
          .             template<typename _Tp, typename _Alloc>
          .               void
          .               vector<_Tp, _Alloc>::
          .               _M_fill_insert(iterator __position, size_type __n, const value_type& __x)
          .               {
          .                 if (__n != 0)
          .           	{
-- line 513 ----------------------------------------
-- line 816 ----------------------------------------
          .                 this->_M_deallocate();
          .                 this->_M_impl._M_start = __start;
          .                 this->_M_impl._M_finish = __finish;
          .                 this->_M_impl._M_end_of_storage = __q + _S_nword(__n);
          .               }
          .           
          .             template<typename _Alloc>
          .               void
     25,310 ( 0.00%)      vector<bool, _Alloc>::
          .               _M_fill_insert(iterator __position, size_type __n, bool __x)
          .               {
      5,062 ( 0.00%)        if (__n == 0)
          .           	return;
      7,593 ( 0.00%)        if (capacity() - size() >= __n)
          .           	{
          .           	  std::copy_backward(__position, end(),
          .           			     this->_M_impl._M_finish + difference_type(__n));
          .           	  std::fill(__position, __position + difference_type(__n), __x);
          .           	  this->_M_impl._M_finish += difference_type(__n);
          .           	}
          .                 else
          .           	{
-- line 837 ----------------------------------------
-- line 838 ----------------------------------------
          .           	  const size_type __len = 
          .           	    _M_check_len(__n, "vector<bool>::_M_fill_insert");
          .           	  _Bit_pointer __q = this->_M_allocate(__len);
          .           	  iterator __start(std::__addressof(*__q), 0);
          .           	  iterator __i = _M_copy_aligned(begin(), __position, __start);
          .           	  std::fill(__i, __i + difference_type(__n), __x);
          .           	  iterator __finish = std::copy(__position, end(),
          .           					__i + difference_type(__n));
         33 ( 0.00%)  	  this->_M_deallocate();
      1,238 ( 0.00%)  => /usr/include/c++/10/bits/stl_bvector.h:std::_Bvector_base<std::allocator<bool> >::_M_deallocate() (11x)
         33 ( 0.00%)  	  this->_M_impl._M_end_of_storage = __q + _S_nword(__len);
         22 ( 0.00%)  	  this->_M_impl._M_start = __start;
         44 ( 0.00%)  	  this->_M_impl._M_finish = __finish;
          .           	}
     20,248 ( 0.00%)      }
          .           
          .             template<typename _Alloc>
          .               template<typename _ForwardIterator>
          .                 void
          .                 vector<bool, _Alloc>::
          .                 _M_insert_range(iterator __position, _ForwardIterator __first, 
          .           		      _ForwardIterator __last, std::forward_iterator_tag)
          .                 {
-- line 859 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/unordered_map.h
--------------------------------------------------------------------------------
Ir                  

-- line 94 ----------------------------------------
         .              *
         .              *  Base is _Hashtable, dispatched at compile time via template
         .              *  alias __umap_hashtable.
         .              */
         .             template<typename _Key, typename _Tp,
         .           	   typename _Hash = hash<_Key>,
         .           	   typename _Pred = equal_to<_Key>,
         .           	   typename _Alloc = allocator<std::pair<const _Key, _Tp>>>
        14 ( 0.00%)      class unordered_map
   330,136 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable() (1x)
        39 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, NP::Global::LST_container<long long> >, std::allocator<std::pair<NP::JobID const, NP::Global::LST_container<long long> > >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable() (1x)
         .               {
         .                 typedef __umap_hashtable<_Key, _Tp, _Hash, _Pred, _Alloc>  _Hashtable;
         .                 _Hashtable _M_h;
         .           
         .               public:
         .                 // typedefs:
         .                 //@{
         .                 /// Public typedefs.
-- line 110 ----------------------------------------
-- line 283 ----------------------------------------
         .                  *
         .                  *  Note that the assignment completely changes the %unordered_map and
         .                  *  that the resulting %unordered_map's size is the same as the number
         .                  *  of elements assigned.
         .                  */
         .                 unordered_map&
         .                 operator=(initializer_list<value_type> __l)
         .                 {
     5,000 ( 0.00%)  	_M_h = __l;
         .           	return *this;
         .                 }
         .           
         .                 ///  Returns the allocator object used by the %unordered_map.
         .                 allocator_type
         .                 get_allocator() const noexcept
         .                 { return _M_h.get_allocator(); }
         .           
-- line 299 ----------------------------------------
-- line 380 ----------------------------------------
         .                  *  inserted if its first element (the key) is not already present in the
         .                  *  %unordered_map.
         .                  *
         .                  *  Insertion requires amortized constant time.
         .                  */
         .                 template<typename... _Args>
         .           	std::pair<iterator, bool>
         .           	emplace(_Args&&... __args)
     2,500 ( 0.00%)  	{ return _M_h.emplace(std::forward<_Args>(__args)...); }
         .           
         .                 /**
         .                  *  @brief Attempts to build and insert a std::pair into the
         .                  *  %unordered_map.
         .                  *
         .                  *  @param  __pos  An iterator that serves as a hint as to where the pair
         .                  *                should be inserted.
         .                  *  @param  __args  Arguments used to generate a new pair instance (see
-- line 396 ----------------------------------------
-- line 912 ----------------------------------------
         .                  *
         .                  *  This function takes a key and tries to locate the element with which
         .                  *  the key matches.  If successful the function returns an iterator
         .                  *  pointing to the sought after element.  If unsuccessful it returns the
         .                  *  past-the-end ( @c end() ) iterator.
         .                  */
         .                 iterator
         .                 find(const key_type& __x)
    15,046 ( 0.00%)        { return _M_h.find(__x); }
    76,776 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/jobs.hpp:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::find(NP::JobID const&) (2,507x)
         .           
         .                 const_iterator
         .                 find(const key_type& __x) const
     7,521 ( 0.00%)        { return _M_h.find(__x); }
    76,776 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/jobs.hpp:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, Interval<long long> >, std::allocator<std::pair<NP::JobID const, Interval<long long> > >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::find(NP::JobID const&) const (2,507x)
         .                 //@}
         .           
         .                 /**
         .                  *  @brief  Finds the number of elements.
         .                  *  @param  __x  Key to count.
         .                  *  @return  Number of elements with specified key.
         .                  *
         .                  *  This function only makes sense for %unordered_multimap; for
-- line 932 ----------------------------------------
-- line 980 ----------------------------------------
         .                  *  Lookup requires constant time.
         .                  */
         .                 mapped_type&
         .                 operator[](const key_type& __k)
         .                 { return _M_h[__k]; }
         .           
         .                 mapped_type&
         .                 operator[](key_type&& __k)
22,942,187 ( 0.06%)        { return _M_h[std::move(__k)]; }
127,862,116 ( 0.33%)  => /usr/include/c++/10/bits/hashtable_policy.h:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) (3,455,729x)
         .                 //@}
         .           
         .                 //@{
         .                 /**
         .                  *  @brief  Access to %unordered_map data.
         .                  *  @param  __k  The key for which data should be retrieved.
         .                  *  @return  A reference to the data whose key is equal to @a __k, if
         .                  *           such a data is present in the %unordered_map.
-- line 996 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h
--------------------------------------------------------------------------------
Ir                   

-- line 31 ----------------------------------------
          .           #endif
          .           
          .           /*
          .            * Functions to align an integer down or up to the given power of two,
          .            * and test for such an alignment, and for power of two.
          .            */
          .           template<typename T>
          .           static inline T alignDown(T arg, uintptr_t alignment) {
100,905,218 ( 0.26%)      return T( (uintptr_t)arg                & ~(alignment-1));
          .           }
          .           template<typename T>
          .           static inline T alignUp  (T arg, uintptr_t alignment) {
     20,922 ( 0.00%)      return T(((uintptr_t)arg+(alignment-1)) & ~(alignment-1));
          .               // /*is this better?*/ return (((uintptr_t)arg-1) | (alignment-1)) + 1;
          .           }
          .           template<typename T> // works for not power-of-2 alignments
          .           static inline T alignUpGeneric(T arg, uintptr_t alignment) {
         63 ( 0.00%)      if (size_t rem = arg % alignment) {
          8 ( 0.00%)          arg += alignment - rem;
          .               }
          .               return arg;
          .           }
          .           
          .           /*
          .            * Compile time Log2 calculation
          .            */
          .           template <size_t NUM>
-- line 57 ----------------------------------------
-- line 97 ----------------------------------------
          .               #pragma warning(pop)
          .           #endif
          .           
          .           #if __SUNPRO_CC
          .               #pragma error_messages (on, refmemnoconstr)
          .           #endif
          .           
          .           template <int BUF_LINE_SIZE, int N>
         10 ( 0.00%)  void parseFile(const char* file, const parseFileItem (&items)[N]) {
          .               // Tries to find all items in each line
          1 ( 0.00%)      int found[N] = { 0 };
          .               // If all items found, stop forward file reading
          .               int numFound = 0;
          .               // Line storage
          .               char buf[BUF_LINE_SIZE];
          .           
         27 ( 0.00%)      if (FILE *f = fopen(file, "r")) {
      2,889 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
        435 ( 0.00%)          while (numFound < N && fgets(buf, BUF_LINE_SIZE, f)) {
      1,307 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
      7,863 ( 0.00%)  => ./libio/iofgets.c:fgets (46x)
          .                       for (int i = 0; i < N; ++i) {
      1,025 ( 0.00%)                  if (!found[i] && 1 == sscanf(buf, items[i].format, &items[i].value)) {
     34,776 ( 0.00%)  => ./stdio-common/isoc99_sscanf.c:__isoc99_sscanf (89x)
      1,222 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          2 ( 0.00%)                      ++numFound;
          7 ( 0.00%)                      found[i] = 1;
          .                           }
          .                       }
          .                   }
         13 ( 0.00%)          fclose(f);
     10,900 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .               }
         10 ( 0.00%)  }
          .           
          .           namespace rml {
          .           namespace internal {
          .           
          .           /*
          .            * Best estimate of cache line size, for the purpose of avoiding false sharing.
          .            * Too high causes memory overhead, too low causes false-sharing overhead.
          .            * Because, e.g., 32-bit code might run on a 64-bit system with a larger cache line size,
-- line 132 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h
--------------------------------------------------------------------------------
Ir                  

-- line 159 ----------------------------------------
         .           };
         .           
         .           template<typename Arg, typename Compare>
         .           inline void AtomicUpdate(std::atomic<Arg>& location, Arg newVal, const Compare &cmp)
         .           {
         .               static_assert(sizeof(Arg) == sizeof(intptr_t), "Type of argument must match AtomicCompareExchange type.");
         .               Arg old = location.load(std::memory_order_acquire);
         .               for (; cmp(old, newVal); ) {
         6 ( 0.00%)          if (location.compare_exchange_strong(old, newVal))
         .                       break;
         .                   // TODO: do we need backoff after unsuccessful CAS?
         .                   //old = val;
         .               }
         .           }
         .           
         .           // TODO: make BitMaskBasic more general
         .           // TODO: check that BitMaskBasic is not used for synchronization
-- line 175 ----------------------------------------
-- line 180 ----------------------------------------
         .               static const unsigned WORD_LEN = CHAR_BIT*sizeof(uintptr_t);
         .           
         .               std::atomic<uintptr_t> mask[SZ];
         .           
         .           protected:
         .               void set(size_t idx, bool val) {
         .                   MALLOC_ASSERT(idx<NUM, ASSERT_TEXT);
         .           
       578 ( 0.00%)          size_t i = idx / WORD_LEN;
       998 ( 0.00%)          int pos = WORD_LEN - idx % WORD_LEN - 1;
         .                   if (val) {
       755 ( 0.00%)              mask[i].fetch_or(1ULL << pos);
         .                   } else {
       661 ( 0.00%)              mask[i].fetch_and(~(1ULL << pos));
         .                   }
         .               }
         .               int getMinTrue(unsigned startIdx) const {
       298 ( 0.00%)          unsigned idx = startIdx / WORD_LEN;
         .                   int pos;
         .           
       526 ( 0.00%)          if (startIdx % WORD_LEN) {
         .                       // only interested in part of a word, clear bits before startIdx
       292 ( 0.00%)              pos = WORD_LEN - startIdx % WORD_LEN;
       453 ( 0.00%)              uintptr_t actualMask = mask[idx].load(std::memory_order_relaxed) & (((uintptr_t)1<<pos) - 1);
       143 ( 0.00%)              idx++;
         .                       if (-1 != (pos = BitScanRev(actualMask)))
       170 ( 0.00%)                  return idx*WORD_LEN - pos - 1;
         .                   }
         .           
       496 ( 0.00%)          while (idx<SZ)
       242 ( 0.00%)              if (-1 != (pos = BitScanRev(mask[idx++].load(std::memory_order_relaxed))))
       300 ( 0.00%)                  return idx*WORD_LEN - pos - 1;
         .                   return -1;
         .               }
         .           public:
         .               void reset() { for (unsigned i=0; i<SZ; i++) mask[i].store(0, std::memory_order_relaxed); }
         .           };
         .           
         .           template<unsigned NUM>
         .           class BitMaskMin : public BitMaskBasic<NUM> {
-- line 219 ----------------------------------------
-- line 223 ----------------------------------------
         .                   return BitMaskBasic<NUM>::getMinTrue(startIdx);
         .               }
         .           };
         .           
         .           template<unsigned NUM>
         .           class BitMaskMax : public BitMaskBasic<NUM> {
         .           public:
         .               void set(size_t idx, bool val) {
        90 ( 0.00%)          BitMaskBasic<NUM>::set(NUM - 1 - idx, val);
         .               }
         .               int getMaxTrue(unsigned startIdx) const {
        28 ( 0.00%)          int p = BitMaskBasic<NUM>::getMinTrue(NUM-startIdx-1);
        26 ( 0.00%)          return -1==p? -1 : (int)NUM - 1 - p;
         .               }
         .           };
         .           
         .           
         .           // The part of thread-specific data that can be modified by other threads.
         .           // Such modifications must be protected by AllLocalCaches::listLock.
         .           struct TLSRemote {
         .               TLSRemote *next,
-- line 243 ----------------------------------------
-- line 304 ----------------------------------------
         .           public:
         .               typedef MainIndexSelect<4 < sizeof(uintptr_t)>::main_type main_t;
         .           private:
         .               static const main_t invalid = ~main_t(0);
         .               main_t main;      // index in BackRefMain
         .               uint16_t largeObj:1;  // is this object "large"?
         .               uint16_t offset  :15; // offset from beginning of BackRefBlock
         .           public:
       546 ( 0.00%)      BackRefIdx() : main(invalid), largeObj(0), offset(0) {}
         .               bool isInvalid() const { return main == invalid; }
27,772,115 ( 0.07%)      bool isLargeObject() const { return largeObj; }
         .               main_t getMain() const { return main; }
         .               uint16_t getOffset() const { return offset; }
         .           
         .           #if __TBB_USE_THREAD_SANITIZER
         .               friend
         .               __attribute__((no_sanitize("thread")))
         .                BackRefIdx dereference(const BackRefIdx* ptr) {
         .                   BackRefIdx idx;
-- line 322 ----------------------------------------
-- line 323 ----------------------------------------
         .                   idx.main = ptr->main;
         .                   idx.largeObj = ptr->largeObj;
         .                   idx.offset = ptr->offset;
         .                   return idx;
         .               }
         .           #else
         .               friend
         .               BackRefIdx dereference(const BackRefIdx* ptr) {
 7,934,890 ( 0.02%)          return *ptr;
         .               }
         .           #endif
         .           
         .               // only newBackRef can modify BackRefIdx
         .               static BackRefIdx newBackRef(bool largeObj);
         .           };
         .           
         .           // Block header is used during block coalescing
-- line 339 ----------------------------------------
-- line 388 ----------------------------------------
         .               }
         .           
         .               bool ready() const {
         .                   return setDone;
         .               }
         .           
         .               // envName - environment variable to get controlled mode
         .               void initReadEnv(const char *envName, intptr_t defaultVal) {
         2 ( 0.00%)          if (!setDone) {
         .                       // unreferenced formal parameter warning
         .                       tbb::detail::suppress_unused_warning(envName);
         .           #if !__TBB_WIN8UI_SUPPORT
         .                   // TODO: use strtol to get the actual value of the envirable
         4 ( 0.00%)              const char *envVal = getenv(envName);
       402 ( 0.00%)  => ./stdlib/getenv.c:getenv (1x)
         3 ( 0.00%)              if (envVal && !strcmp(envVal, "1"))
         .                           val = 1;
         .                       else
         .           #endif
         1 ( 0.00%)                  val = defaultVal;
         1 ( 0.00%)              setDone = true;
         .                   }
         1 ( 0.00%)      }
         .           };
         .           
         .           // Page type to be used inside MapMemory.
         .           // Regular (4KB aligned), Huge and Transparent Huge Pages (2MB aligned).
         .           enum PageType {
         .               REGULAR = 0,
         .               PREALLOCATED_HUGE_PAGE,
         .               TRANSPARENT_HUGE_PAGE
-- line 417 ----------------------------------------
-- line 440 ----------------------------------------
         .                   fputs("TBBmalloc: huge pages\t", stderr);
         .                   if (!state)
         .                       fputs("not ", stderr);
         .                   fputs(stateName, stderr);
         .                   fputs("\n", stderr);
         .               }
         .           
         .               void parseSystemMemInfo() {
         1 ( 0.00%)          bool hpAvailable  = false;
         .                   bool thpAvailable = false;
         1 ( 0.00%)          long long hugePageSize = -1;
         .           
         .           #if __unix__
         .                   // Check huge pages existence
         1 ( 0.00%)          long long meminfoHugePagesTotal = 0;
         .           
         .                   parseFileItem meminfoItems[] = {
         .                       // Parse system huge page size
         .                       { "Hugepagesize: %lld kB", hugePageSize },
         .                       // Check if there are preallocated huge pages on the system
         .                       // https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
         .                       { "HugePages_Total: %lld", meminfoHugePagesTotal } };
         .           
         .                   parseFile</*BUFF_SIZE=*/100>("/proc/meminfo", meminfoItems);
         .           
         .                   // Double check another system information regarding preallocated
         .                   // huge pages if there are no information in /proc/meminfo
         1 ( 0.00%)          long long vmHugePagesTotal = 0;
         .           
         4 ( 0.00%)          parseFileItem vmItem[] = { { "%lld", vmHugePagesTotal } };
         .           
         .                   // We parse a counter number, it can't be huge
         3 ( 0.00%)          parseFile</*BUFF_SIZE=*/100>("/proc/sys/vm/nr_hugepages", vmItem);
     2,299 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:void parseFile<100, 1>(char const*, parseFileItem const (&) [1]) (1x)
         .           
         7 ( 0.00%)          if (hugePageSize > -1 && (meminfoHugePagesTotal > 0 || vmHugePagesTotal > 0)) {
         .                       MALLOC_ASSERT(hugePageSize != 0, "Huge Page size can't be zero if we found preallocated.");
         .           
         .                       // Any non zero value clearly states that there are preallocated
         .                       // huge pages on the system
         1 ( 0.00%)              hpAvailable = true;
         .                   }
         .           
         .                   // Check if there is transparent huge pages support on the system
         1 ( 0.00%)          long long thpPresent = 'n';
         4 ( 0.00%)          parseFileItem thpItem[] = { { "[alwa%cs] madvise never\n", thpPresent } };
         3 ( 0.00%)          parseFile</*BUFF_SIZE=*/100>("/sys/kernel/mm/transparent_hugepage/enabled", thpItem);
     2,848 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:void parseFile<100, 1>(char const*, parseFileItem const (&) [1]) (1x)
         .           
         5 ( 0.00%)          if (hugePageSize > -1 && thpPresent == 'y') {
         .                       MALLOC_ASSERT(hugePageSize != 0, "Huge Page size can't be zero if we found thp existence.");
         .                       thpAvailable = true;
         .                   }
         .           #endif
         .                   MALLOC_ASSERT(!pageSize, "Huge page size can't be set twice. Double initialization.");
         .           
         .                   // Initialize object variables
         2 ( 0.00%)          pageSize       = hugePageSize * 1024; // was read in KB from meminfo
         1 ( 0.00%)          isHPAvailable  = hpAvailable;
         1 ( 0.00%)          isTHPAvailable = thpAvailable;
         .               }
         .           
         .           public:
         .           
         .               // System information
         .               bool isHPAvailable;
         .               bool isTHPAvailable;
         .           
         .               // User defined value
         .               bool isEnabled;
         .           
         .               void init() {
         .                   parseSystemMemInfo();
         .                   MallocMutex::scoped_lock lock(setModeLock);
         .                   requestedMode.initReadEnv("TBB_MALLOC_USE_HUGE_PAGES", 0);
         9 ( 0.00%)          isEnabled = (isHPAvailable || isTHPAvailable) && requestedMode.get();
         .               }
         .           
         .               // Could be set from user code at any place.
         .               // If we didn't call init() at this place, isEnabled will be false
         .               void setMode(intptr_t newVal) {
         .                   MallocMutex::scoped_lock lock(setModeLock);
         .                   requestedMode.set(newVal);
         .                   isEnabled = (isHPAvailable || isTHPAvailable) && newVal;
-- line 521 ----------------------------------------
-- line 576 ----------------------------------------
         .                                 fixedPool;
         .               TLSKey            tlsPointerKey;  // per-pool TLS key
         .           
         .               bool init(intptr_t poolId, rawAllocType rawAlloc, rawFreeType rawFree,
         .                         size_t granularity, bool keepAllMemory, bool fixedPool);
         .               bool initTLS();
         .           
         .               // i.e., not system default pool for scalable_malloc/scalable_free
       720 ( 0.00%)      bool userPool() const { return rawAlloc; }
         .           
         .                // true if something has been released
         .               bool softCachesCleanup();
         .               bool releaseAllLocalCaches();
         .               bool hardCachesCleanup();
         .               void *remap(void *ptr, size_t oldSize, size_t newSize, size_t alignment);
         .               bool reset() {
         .                   loc.reset();
-- line 592 ----------------------------------------
-- line 665 ----------------------------------------
         .            */
         .               static bool mallocRecursionDetected;
         .           
         .               MallocMutex::scoped_lock* lock_acquired;
         .               char scoped_lock_space[sizeof(MallocMutex::scoped_lock)+1];
         .               
         .           public:
         .           
         3 ( 0.00%)      RecursiveMallocCallProtector() : lock_acquired(nullptr) {
         6 ( 0.00%)          lock_acquired = new (scoped_lock_space) MallocMutex::scoped_lock( rmc_mutex );
         .                   if (canUsePthread)
        10 ( 0.00%)              owner_thread.store(pthread_self(), std::memory_order_relaxed);
       830 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         .                   autoObjPtr.store(&scoped_lock_space, std::memory_order_relaxed);
         .               }
         .               ~RecursiveMallocCallProtector() {
         6 ( 0.00%)          if (lock_acquired) {
         .                       autoObjPtr.store(nullptr, std::memory_order_relaxed);
         3 ( 0.00%)              lock_acquired->~scoped_lock();
         .                   }
         2 ( 0.00%)      }
         .               static bool sameThreadActive() {
50,456,984 ( 0.13%)          if (!autoObjPtr.load(std::memory_order_relaxed)) // fast path
         .                       return false;
         .                   // Some thread has an active recursive call protector; check if the current one.
         .                   // Exact pthread_self based test
         .                   if (canUsePthread) {
        35 ( 0.00%)              if (pthread_equal( owner_thread.load(std::memory_order_relaxed), pthread_self() )) {
        14 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (7x)
         .                           mallocRecursionDetected = true;
         .                           return true;
         .                       } else
         .                           return false;
         .                   }
         .                   // inexact stack size based test
         .                   const uintptr_t threadStackSz = 2*1024*1024;
         .                   int dummy;
-- line 699 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.h
--------------------------------------------------------------------------------
Ir                   

-- line 26 ----------------------------------------
          .           // global state of blocks currently in processing
          .           class BackendSync {
          .               // Class instances should reside in zero-initialized memory!
          .               // The number of blocks currently removed from a bin and not returned back
          .               std::atomic<intptr_t> inFlyBlocks;        // to another
          .               std::atomic<intptr_t> binsModifications;  // incremented on every bin modification
          .               Backend *backend;
          .           public:
          1 ( 0.00%)      void init(Backend *b) { backend = b; }
          .               void blockConsumed() { inFlyBlocks++; }
          .               void binsModified() { binsModifications++; }
          .               void blockReleased() {
          .           #if __TBB_MALLOC_BACKEND_STAT
          .                   MALLOC_ITT_SYNC_RELEASING(&inFlyBlocks);
          .           #endif
          .                   binsModifications++;
          .                   intptr_t prev = inFlyBlocks.fetch_sub(1);
-- line 42 ----------------------------------------
-- line 67 ----------------------------------------
          .               std::atomic<intptr_t>    active;
          .           public:
          .               bool wait() {
          .                   bool rescanBins = false;
          .                   // up to 3 threads can add more memory from OS simultaneously,
          .                   // rest of threads have to wait
          .                   intptr_t prevCnt = active.load(std::memory_order_acquire);
          .                   for (;;) {
          4 ( 0.00%)              if (prevCnt < 3) {
          4 ( 0.00%)                  if (active.compare_exchange_strong(prevCnt, prevCnt + 1)) {
          .                               break;
          .                           }
          .                       } else {
          .                           SpinWaitWhileEq(active, prevCnt);
          .                           rescanBins = true;
          .                           break;
          .                       }
          .                   }
-- line 84 ----------------------------------------
-- line 212 ----------------------------------------
          .                   void init() { leftBound.store(ADDRESS_UPPER_BOUND, std::memory_order_relaxed); }
          .                   void registerAlloc(uintptr_t left, uintptr_t right);
          .                   void registerFree(uintptr_t left, uintptr_t right);
          .                   // as only left and right bounds are kept, we can return true
          .                   // for pointer not allocated by us, if more than single region
          .                   // was requested from OS
          .                   bool inRange(void *ptr) const {
          .                       const uintptr_t p = (uintptr_t)ptr;
100,913,948 ( 0.26%)              return leftBound.load(std::memory_order_relaxed)<=p &&
          .                              p<=rightBound.load(std::memory_order_relaxed);
          .                   }
          .               };
          .           #else
          .               class UsedAddressRange {
          .               public:
          .                   void init() { }
          .                   void registerAlloc(uintptr_t, uintptr_t) {}
-- line 228 ----------------------------------------
-- line 315 ----------------------------------------
          .               // Clean all memory from all caches (extMemPool hard cleanup)
          .               FreeBlock *releaseMemInCaches(intptr_t startModifiedCnt, int *lockedBinsThreshold, int numOfLockedBins);
          .               // Soft heap limit (regular cleanup, then maybe hard cleanup)
          .               void releaseCachesToLimit();
          .           
          .               /*---------------------------------- Utility ----------------------------------*/
          .               // TODO: move inside IndexedBins class
          .               static int sizeToBin(size_t size) {
      1,237 ( 0.00%)          if (size >= maxBinned_HugePage)
        452 ( 0.00%)              return HUGE_BIN;
        918 ( 0.00%)          else if (size < minBinnedSize)
          .                       return NO_BIN;
          .           
      1,072 ( 0.00%)          int bin = (size - minBinnedSize)/freeBinsStep;
          .           
          .                   MALLOC_ASSERT(bin < HUGE_BIN, "Invalid size.");
          .                   return bin;
          .               }
          .               static bool toAlignedBin(FreeBlock *block, size_t size) {
          .                   return isAligned((char*)block + size, slabSize) && size >= slabSize;
          .               }
          .           
-- line 336 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/ext/new_allocator.h
--------------------------------------------------------------------------------
Ir                   

-- line 74 ----------------------------------------
          .                 // 2103. propagate_on_container_move_assignment
          .                 typedef std::true_type propagate_on_container_move_assignment;
          .           #endif
          .           
          .                 _GLIBCXX20_CONSTEXPR
          .                 new_allocator() _GLIBCXX_USE_NOEXCEPT { }
          .           
          .                 _GLIBCXX20_CONSTEXPR
          3 ( 0.00%)        new_allocator(const new_allocator&) _GLIBCXX_USE_NOEXCEPT { }
          .           
          .                 template<typename _Tp1>
          .           	_GLIBCXX20_CONSTEXPR
          .           	new_allocator(const new_allocator<_Tp1>&) _GLIBCXX_USE_NOEXCEPT { }
          .           
          .           #if __cplusplus <= 201703L
      5,587 ( 0.00%)        ~new_allocator() _GLIBCXX_USE_NOEXCEPT { }
          .           
          .                 pointer
          .                 address(reference __x) const _GLIBCXX_NOEXCEPT
          .                 { return std::__addressof(__x); }
          .           
          .                 const_pointer
          .                 address(const_reference __x) const _GLIBCXX_NOEXCEPT
          .                 { return std::__addressof(__x); }
          .           #endif
          .           
          .                 // NB: __n is permitted to be 0.  The C++ standard says nothing
          .                 // about what the return value is when __n == 0.
          .                 _GLIBCXX_NODISCARD _Tp*
          .                 allocate(size_type __n, const void* = static_cast<const void*>(0))
          .                 {
 12,575,252 ( 0.03%)  	if (__n > this->_M_max_size())
          .           	  std::__throw_bad_alloc();
          .           
          .           #if __cpp_aligned_new
          .           	if (alignof(_Tp) > __STDCPP_DEFAULT_NEW_ALIGNMENT__)
          .           	  {
          .           	    std::align_val_t __al = std::align_val_t(alignof(_Tp));
          .           	    return static_cast<_Tp*>(::operator new(__n * sizeof(_Tp), __al));
          .           	  }
          .           #endif
192,421,787 ( 0.50%)  	return static_cast<_Tp*>(::operator new(__n * sizeof(_Tp)));
      1,929 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) (3x)
          .                 }
          .           
          .                 // __p is not permitted to be a null pointer.
          .                 void
          .                 deallocate(_Tp* __p, size_type __t)
          .                 {
          .           #if __cpp_aligned_new
          .           	if (alignof(_Tp) > __STDCPP_DEFAULT_NEW_ALIGNMENT__)
-- line 123 ----------------------------------------
-- line 125 ----------------------------------------
          .           	    ::operator delete(__p,
          .           # if __cpp_sized_deallocation
          .           			      __t * sizeof(_Tp),
          .           # endif
          .           			      std::align_val_t(alignof(_Tp)));
          .           	    return;
          .           	  }
          .           #endif
 78,855,245 ( 0.21%)  	::operator delete(__p
        138 ( 0.00%)  => ???:operator delete(void*, unsigned long) (1x)
          .           #if __cpp_sized_deallocation
          .           			  , __t * sizeof(_Tp)
          .           #endif
          .           			 );
          .                 }
          .           
          .           #if __cplusplus <= 201703L
          .                 size_type
          .                 max_size() const _GLIBCXX_USE_NOEXCEPT
          .                 { return _M_max_size(); }
          .           
          .           #if __cplusplus >= 201103L
          .                 template<typename _Up, typename... _Args>
          .           	void
         13 ( 0.00%)  	construct(_Up* __p, _Args&&... __args)
          .           	noexcept(std::is_nothrow_constructible<_Up, _Args...>::value)
 51,883,961 ( 0.14%)  	{ ::new((void *)__p) _Up(std::forward<_Args>(__args)...); }
     20,299 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (1x)
          .           
          .                 template<typename _Up>
          .           	void
          .           	destroy(_Up* __p)
          .           	noexcept(std::is_nothrow_destructible<_Up>::value)
         63 ( 0.00%)  	{ __p->~_Up(); }
      1,285 ( 0.00%)  => /usr/include/c++/10/bits/stl_deque.h:std::deque<NP::Global::Schedule_state<long long>, std::allocator<NP::Global::Schedule_state<long long> > >::~deque() (3x)
          .           #else
          .                 // _GLIBCXX_RESOLVE_LIB_DEFECTS
          .                 // 402. wrong new expression in [some_] allocator::construct
          .                 void
          .                 construct(pointer __p, const _Tp& __val)
          .                 { ::new((void *)__p) _Tp(__val); }
          .           
          .                 void
-- line 164 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_algo.h
--------------------------------------------------------------------------------
Ir                  

-- line 829 ----------------------------------------
         .           
         .             template<typename _ForwardIterator, typename _Predicate>
         .               _GLIBCXX20_CONSTEXPR
         .               _ForwardIterator
         .               __remove_if(_ForwardIterator __first, _ForwardIterator __last,
         .           		_Predicate __pred)
         .               {
         .                 __first = std::__find_if(__first, __last, __pred);
        44 ( 0.00%)        if (__first == __last)
         .           	return __first;
         .                 _ForwardIterator __result = __first;
         .                 ++__first;
        60 ( 0.00%)        for (; __first != __last; ++__first)
         8 ( 0.00%)  	if (!__pred(__first))
         .           	  {
         .           	    *__result = _GLIBCXX_MOVE(*__first);
         .           	    ++__result;
         .           	  }
         .                 return __result;
         .               }
         .           
         .             /**
-- line 850 ----------------------------------------
-- line 1818 ----------------------------------------
         .               void
         .               __unguarded_linear_insert(_RandomAccessIterator __last,
         .           			      _Compare __comp)
         .               {
         .                 typename iterator_traits<_RandomAccessIterator>::value_type
         .           	__val = _GLIBCXX_MOVE(*__last);
         .                 _RandomAccessIterator __next = __last;
         .                 --__next;
37,725,103 ( 0.10%)        while (__comp(__val, __next))
         .           	{
         5 ( 0.00%)  	  *__last = _GLIBCXX_MOVE(*__next);
         .           	  __last = __next;
         .           	  --__next;
         .           	}
18,862,551 ( 0.05%)        *__last = _GLIBCXX_MOVE(__val);
         .               }
         .           
         .             /// This is a helper function for the sort routine.
         .             template<typename _RandomAccessIterator, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               void
        28 ( 0.00%)      __insertion_sort(_RandomAccessIterator __first,
         .           		     _RandomAccessIterator __last, _Compare __comp)
         .               {
 6,287,510 ( 0.02%)        if (__first == __last) return;
         .           
25,150,121 ( 0.07%)        for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i)
         .           	{
18,862,572 ( 0.05%)  	  if (__comp(__i, __first))
         .           	    {
         .           	      typename iterator_traits<_RandomAccessIterator>::value_type
         .           		__val = _GLIBCXX_MOVE(*__i);
         .           	      _GLIBCXX_MOVE_BACKWARD3(__first, __i, __i + 1);
         5 ( 0.00%)  	      *__first = _GLIBCXX_MOVE(__val);
         .           	    }
         .           	  else
         .           	    std::__unguarded_linear_insert(__i,
         .           				__gnu_cxx::__ops::__val_comp_iter(__comp));
         .           	}
        24 ( 0.00%)      }
         .           
         .             /// This is a helper function for the sort routine.
         .             template<typename _RandomAccessIterator, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               inline void
         .               __unguarded_insertion_sort(_RandomAccessIterator __first,
         .           			       _RandomAccessIterator __last, _Compare __comp)
         .               {
-- line 1865 ----------------------------------------
-- line 1873 ----------------------------------------
         .              *  This controls some aspect of the sort routines.
         .             */
         .             enum { _S_threshold = 16 };
         .           
         .             /// This is a helper function for the sort routine.
         .             template<typename _RandomAccessIterator, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               void
28,293,759 ( 0.07%)      __final_insertion_sort(_RandomAccessIterator __first,
         .           			   _RandomAccessIterator __last, _Compare __comp)
         .               {
 6,287,524 ( 0.02%)        if (__last - __first > int(_S_threshold))
         .           	{
         .           	  std::__insertion_sort(__first, __first + int(_S_threshold), __comp);
         .           	  std::__unguarded_insertion_sort(__first + int(_S_threshold), __last,
         .           					  __comp);
         .           	}
         .                 else
        16 ( 0.00%)  	std::__insertion_sort(__first, __last, __comp);
       194 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}>) [clone .isra.0] (1x)
       187 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}>) [clone .isra.0] (1x)
       149 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}>) [clone .isra.0] (1x)
       125 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}>) [clone .isra.0] (1x)
25,150,008 ( 0.07%)      }
         .           
         .             /// This is a helper function...
         .             template<typename _RandomAccessIterator, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               _RandomAccessIterator
         .               __unguarded_partition(_RandomAccessIterator __first,
         .           			  _RandomAccessIterator __last,
         .           			  _RandomAccessIterator __pivot, _Compare __comp)
-- line 1900 ----------------------------------------
-- line 1941 ----------------------------------------
         .             /// This is a helper function for the sort routine.
         .             template<typename _RandomAccessIterator, typename _Size, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               void
         .               __introsort_loop(_RandomAccessIterator __first,
         .           		     _RandomAccessIterator __last,
         .           		     _Size __depth_limit, _Compare __comp)
         .               {
 6,287,524 ( 0.02%)        while (__last - __first > int(_S_threshold))
         .           	{
         .           	  if (__depth_limit == 0)
         .           	    {
         .           	      std::__partial_sort(__first, __last, __last, __comp);
         .           	      return;
         .           	    }
         .           	  --__depth_limit;
         .           	  _RandomAccessIterator __cut =
         .           	    std::__unguarded_partition_pivot(__first, __last, __comp);
         .           	  std::__introsort_loop(__cut, __last, __depth_limit, __comp);
         .           	  __last = __cut;
         .           	}
         1 ( 0.00%)      }
         .           
         .             // sort
         .           
         .             template<typename _RandomAccessIterator, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               inline void
         .               __sort(_RandomAccessIterator __first, _RandomAccessIterator __last,
         .           	   _Compare __comp)
         .               {
 6,287,522 ( 0.02%)        if (__first != __last)
         .           	{
12,575,037 ( 0.03%)  	  std::__introsort_loop(__first, __last,
        10 ( 0.00%)  => /usr/include/c++/10/bits/stl_iterator.h:void std::__introsort_loop<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (2x)
         9 ( 0.00%)  				std::__lg(__last - __first) * 2,
         .           				__comp);
 9,431,253 ( 0.02%)  	  std::__final_insertion_sort(__first, __last, __comp);
        70 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (1x)
         .           	}
         .               }
         .           
         .             template<typename _RandomAccessIterator, typename _Size, typename _Compare>
         .               _GLIBCXX20_CONSTEXPR
         .               void
         .               __introselect(_RandomAccessIterator __first, _RandomAccessIterator __nth,
         .           		  _RandomAccessIterator __last, _Size __depth_limit,
-- line 1985 ----------------------------------------
-- line 2048 ----------------------------------------
         .               __upper_bound(_ForwardIterator __first, _ForwardIterator __last,
         .           		  const _Tp& __val, _Compare __comp)
         .               {
         .                 typedef typename iterator_traits<_ForwardIterator>::difference_type
         .           	_DistanceType;
         .           
         .                 _DistanceType __len = std::distance(__first, __last);
         .           
   215,388 ( 0.00%)        while (__len > 0)
         .           	{
   260,812 ( 0.00%)  	  _DistanceType __half = __len >> 1;
         .           	  _ForwardIterator __middle = __first;
         .           	  std::advance(__middle, __half);
   173,396 ( 0.00%)  	  if (__comp(__val, __middle))
         .           	    __len = __half;
         .           	  else
         .           	    {
         .           	      __first = __middle;
         .           	      ++__first;
   120,628 ( 0.00%)  	      __len = __len - __half - 1;
         .           	    }
         .           	}
         .                 return __first;
         .               }
         .           
         .             /**
         .              *  @brief Finds the last position in which @p __val could be inserted
         .              *         without changing the ordering.
-- line 2075 ----------------------------------------
-- line 4306 ----------------------------------------
         .               {
         .                 // concept requirements
         .                 __glibcxx_function_requires(_InputIteratorConcept<_InputIterator>)
         .                 __glibcxx_function_requires(_OutputIteratorConcept<_OutputIterator,
         .           	    // "the type returned by a _UnaryOperation"
         .           	    __typeof__(__unary_op(*__first))>)
         .                 __glibcxx_requires_valid_range(__first, __last);
         .           
       135 ( 0.00%)        for (; __first != __last; ++__first, (void)++__result)
       180 ( 0.00%)  	*__result = __unary_op(*__first);
       855 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
       473 ( 0.00%)  => ./ctype/ctype.c:tolower (43x)
         .                 return __result;
         .               }
         .           
         .             /**
         .              *  @brief Perform an operation on corresponding elements of two sequences.
         .              *  @ingroup mutating_algorithms
         .              *  @param  __first1     An input iterator.
         .              *  @param  __last1      An input iterator.
-- line 4323 ----------------------------------------
-- line 5629 ----------------------------------------
         .               _GLIBCXX14_CONSTEXPR
         .               _ForwardIterator
         .               __min_element(_ForwardIterator __first, _ForwardIterator __last,
         .           		  _Compare __comp)
         .               {
         .                 if (__first == __last)
         .           	return __first;
         .                 _ForwardIterator __result = __first;
 6,653,112 ( 0.02%)        while (++__first != __last)
         .           	if (__comp(__first, __result))
         .           	  __result = __first;
         .                 return __result;
         .               }
         .           
         .             /**
         .              *  @brief  Return the minimum element in a range.
         .              *  @ingroup sorting_algorithms
-- line 5645 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_uninitialized.h
--------------------------------------------------------------------------------
Ir                  

-- line 79 ----------------------------------------
         .             template<bool _TrivialValueTypes>
         .               struct __uninitialized_copy
         .               {
         .                 template<typename _InputIterator, typename _ForwardIterator>
         .                   static _ForwardIterator
         .                   __uninit_copy(_InputIterator __first, _InputIterator __last,
         .           		      _ForwardIterator __result)
         .                   {
        32 ( 0.00%)  	  _ForwardIterator __cur = __result;
         .           	  __try
         .           	    {
    73,260 ( 0.00%)  	      for (; __first != __last; ++__first, (void)++__cur)
         .           		std::_Construct(std::__addressof(*__cur), *__first);
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__result, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 98 ----------------------------------------
-- line 227 ----------------------------------------
         .                 template<typename _ForwardIterator, typename _Size, typename _Tp>
         .                   static _ForwardIterator
         .                   __uninit_fill_n(_ForwardIterator __first, _Size __n,
         .           			const _Tp& __x)
         .                   {
         .           	  _ForwardIterator __cur = __first;
         .           	  __try
         .           	    {
        12 ( 0.00%)  	      for (; __n > 0; --__n, (void) ++__cur)
         .           		std::_Construct(std::__addressof(*__cur), __x);
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__first, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 243 ----------------------------------------
-- line 558 ----------------------------------------
         .               {
         .                 template<typename _ForwardIterator, typename _Size>
         .                   static _ForwardIterator
         .                   __uninit_default_n(_ForwardIterator __first, _Size __n)
         .                   {
         .           	  _ForwardIterator __cur = __first;
         .           	  __try
         .           	    {
     7,560 ( 0.00%)  	      for (; __n > 0; --__n, (void) ++__cur)
         .           		std::_Construct(std::__addressof(*__cur));
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__first, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 574 ----------------------------------------
-- line 983 ----------------------------------------
         .               struct __is_bitwise_relocatable
         .               : is_trivial<_Tp> { };
         .           
         .             template <typename _Tp, typename _Up>
         .               inline __enable_if_t<std::__is_bitwise_relocatable<_Tp>::value, _Tp*>
         .               __relocate_a_1(_Tp* __first, _Tp* __last,
         .           		   _Tp* __result, allocator<_Up>&) noexcept
         .               {
    74,968 ( 0.00%)        ptrdiff_t __count = __last - __first;
37,939,926 ( 0.10%)        if (__count > 0)
63,102,522 ( 0.17%)  	__builtin_memmove(__result, __first, __count * sizeof(_Tp));
    25,585 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms (12x)
    37,484 ( 0.00%)        return __result + __count;
         .               }
         .           
         .             template <typename _InputIterator, typename _ForwardIterator,
         .           	    typename _Allocator>
         .               inline _ForwardIterator
         .               __relocate_a_1(_InputIterator __first, _InputIterator __last,
         .           		   _ForwardIterator __result, _Allocator& __alloc)
         .               noexcept(noexcept(std::__relocate_object_a(std::addressof(*__result),
-- line 1002 ----------------------------------------
-- line 1004 ----------------------------------------
         .           					       __alloc)))
         .               {
         .                 typedef typename iterator_traits<_InputIterator>::value_type
         .           	_ValueType;
         .                 typedef typename iterator_traits<_ForwardIterator>::value_type
         .           	_ValueType2;
         .                 static_assert(std::is_same<_ValueType, _ValueType2>::value,
         .           	  "relocation is only possible for values of the same type");
    17,186 ( 0.00%)        _ForwardIterator __cur = __result;
 1,150,029 ( 0.00%)        for (; __first != __last; ++__first, (void)++__cur)
         .           	std::__relocate_object_a(std::__addressof(*__cur),
         .           				 std::__addressof(*__first), __alloc);
         .                 return __cur;
         .               }
         .           
         .             template <typename _InputIterator, typename _ForwardIterator,
         .           	    typename _Allocator>
         .               inline _ForwardIterator
-- line 1021 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 76 ----------------------------------------
          .           
          .           // In case there is no std::get_new_handler function
          .           // which provides synchronized access to std::new_handler
          .           #if !__TBB_CPP11_GET_NEW_HANDLER_PRESENT
          .           static ProxyMutex new_lock;
          .           #endif
          .           
          .           static inline void* InternalOperatorNew(size_t sz) {
 50,456,952 ( 0.13%)      void* res = scalable_malloc(sz);
2,786,166,300 ( 7.29%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (25,228,476x)
          .           #if TBB_USE_EXCEPTIONS
 50,456,952 ( 0.13%)      while (!res) {
          .                   std::new_handler handler;
          .           #if __TBB_CPP11_GET_NEW_HANDLER_PRESENT
          .                   handler = std::get_new_handler();
          .           #else
          .                   {
          .                       ProxyMutex::scoped_lock lock(new_lock);
          .                       handler = std::set_new_handler(0);
          .                       std::set_new_handler(handler);
-- line 94 ----------------------------------------
-- line 156 ----------------------------------------
          .               *orig_libc_realloc;
          .           
          .           // We already tried to find ptr to original functions.
          .           static std::atomic<bool> origFuncSearched{false};
          .           
          .           inline void InitOrigPointers()
          .           {
          .               // race is OK here, as different threads found same functions
 50,456,968 ( 0.13%)      if (!origFuncSearched.load(std::memory_order_acquire)) {
          9 ( 0.00%)          orig_free = dlsym(RTLD_NEXT, "free");
      4,794 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          5 ( 0.00%)          orig_realloc = dlsym(RTLD_NEXT, "realloc");
        873 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_msize = dlsym(RTLD_NEXT, "malloc_usable_size");
      1,027 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_libc_free = dlsym(RTLD_NEXT, "__libc_free");
        905 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_libc_realloc = dlsym(RTLD_NEXT, "__libc_realloc");
        972 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          .           
          .                   origFuncSearched.store(true, std::memory_order_release);
          .               }
          .           }
          .           
          .           /*** replacements for malloc and the family ***/
          .           extern "C" {
          .           #elif MALLOC_ZONE_OVERLOAD_ENABLED
-- line 177 ----------------------------------------
-- line 180 ----------------------------------------
          .           #define ZONE_ARG struct _malloc_zone_t *,
          .           #define PREFIX(name) impl_##name
          .           // not interested in original functions for zone overload
          .           inline void InitOrigPointers() {}
          .           
          .           #endif // MALLOC_UNIXLIKE_OVERLOAD_ENABLED and MALLOC_ZONE_OVERLOAD_ENABLED
          .           
          .           void *PREFIX(malloc)(ZONE_ARG size_t size) __THROW
         11 ( 0.00%)  {
         26 ( 0.00%)      return scalable_malloc(size);
      3,357 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (10x)
      1,677 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         22 ( 0.00%)  }
          .           
          .           void *PREFIX(calloc)(ZONE_ARG size_t num, size_t size) __THROW
          2 ( 0.00%)  {
          8 ( 0.00%)      return scalable_calloc(num, size);
    147,962 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          4 ( 0.00%)  }
          .           
          .           void PREFIX(free)(ZONE_ARG void *object) __THROW
         16 ( 0.00%)  {
          .               InitOrigPointers();
         36 ( 0.00%)      __TBB_malloc_safer_free(object, (void (*)(void*))orig_free);
      9,049 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (7x)
      1,022 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         16 ( 0.00%)  }
          .           
          .           void *PREFIX(realloc)(ZONE_ARG void* ptr, size_t sz) __THROW
          .           {
          .               InitOrigPointers();
          .               return __TBB_malloc_safer_realloc(ptr, sz, orig_realloc);
          .           }
          .           
          .           /* The older *NIX interface for aligned allocations;
-- line 209 ----------------------------------------
-- line 303 ----------------------------------------
          .               return __TBB_malloc_safer_realloc(ptr, size, orig_libc_realloc);
          .           }
          .           #endif // !__ANDROID__
          .           
          .           } /* extern "C" */
          .           
          .           /*** replacements for global operators new and delete ***/
          .           
 50,456,952 ( 0.13%)  void* operator new(size_t sz) {
          .               return InternalOperatorNew(sz);
 50,456,976 ( 0.13%)  }
      1,959 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (3x)
          .           void* operator new[](size_t sz) {
          .               return InternalOperatorNew(sz);
          .           }
 50,456,952 ( 0.13%)  void operator delete(void* ptr) noexcept {
          .               InitOrigPointers();
100,913,904 ( 0.26%)      __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
2,491,834,781 ( 6.52%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (25,228,476x)
 50,456,952 ( 0.13%)  }
          .           void operator delete[](void* ptr) noexcept {
          .               InitOrigPointers();
          .               __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
          .           }
          .           void* operator new(size_t sz, const std::nothrow_t&) noexcept {
          .               return scalable_malloc(sz);
          .           }
          .           void* operator new[](std::size_t sz, const std::nothrow_t&) noexcept {
-- line 328 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_tree.h
--------------------------------------------------------------------------------
Ir                   

-- line 106 ----------------------------------------
          .               _Rb_tree_color	_M_color;
          .               _Base_ptr		_M_parent;
          .               _Base_ptr		_M_left;
          .               _Base_ptr		_M_right;
          .           
          .               static _Base_ptr
          .               _S_minimum(_Base_ptr __x) _GLIBCXX_NOEXCEPT
          .               {
         24 ( 0.00%)        while (__x->_M_left != 0) __x = __x->_M_left;
          .                 return __x;
          .               }
          .           
          .               static _Const_Base_ptr
          .               _S_minimum(_Const_Base_ptr __x) _GLIBCXX_NOEXCEPT
          .               {
          .                 while (__x->_M_left != 0) __x = __x->_M_left;
          .                 return __x;
          .               }
          .           
          .               static _Base_ptr
          .               _S_maximum(_Base_ptr __x) _GLIBCXX_NOEXCEPT
          .               {
         20 ( 0.00%)        while (__x->_M_right != 0) __x = __x->_M_right;
          .                 return __x;
          .               }
          .           
          .               static _Const_Base_ptr
          .               _S_maximum(_Const_Base_ptr __x) _GLIBCXX_NOEXCEPT
          .               {
          .                 while (__x->_M_right != 0) __x = __x->_M_right;
          .                 return __x;
-- line 136 ----------------------------------------
-- line 167 ----------------------------------------
          .             // Helper type to manage default initialization of node count and header.
          .             struct _Rb_tree_header
          .             {
          .               _Rb_tree_node_base	_M_header;
          .               size_t		_M_node_count; // Keeps track of size of tree.
          .           
          .               _Rb_tree_header() _GLIBCXX_NOEXCEPT
          .               {
         73 ( 0.00%)        _M_header._M_color = _S_red;
          .                 _M_reset();
          .               }
          .           
          .           #if __cplusplus >= 201103L
          .               _Rb_tree_header(_Rb_tree_header&& __x) noexcept
          .               {
          .                 if (__x._M_header._M_parent != nullptr)
          .           	_M_move_data(__x);
-- line 183 ----------------------------------------
-- line 200 ----------------------------------------
          .                 _M_node_count = __from._M_node_count;
          .           
          .                 __from._M_reset();
          .               }
          .           
          .               void
          .               _M_reset()
          .               {
         73 ( 0.00%)        _M_header._M_parent = 0;
        146 ( 0.00%)        _M_header._M_left = &_M_header;
         73 ( 0.00%)        _M_header._M_right = &_M_header;
         73 ( 0.00%)        _M_node_count = 0;
          .               }
          .             };
          .           
          .             template<typename _Val>
          .               struct _Rb_tree_node : public _Rb_tree_node_base
          .               {
          .                 typedef _Rb_tree_node<_Val>* _Link_type;
          .           
-- line 219 ----------------------------------------
-- line 294 ----------------------------------------
          .           	_Self __tmp = *this;
          .           	_M_node = _Rb_tree_increment(_M_node);
          .           	return __tmp;
          .                 }
          .           
          .                 _Self&
          .                 operator--() _GLIBCXX_NOEXCEPT
          .                 {
      2,003 ( 0.00%)  	_M_node = _Rb_tree_decrement(_M_node);
      1,097 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
        269 ( 0.00%)  => ???:std::_Rb_tree_decrement(std::_Rb_tree_node_base*) (18x)
          .           	return *this;
          .                 }
          .           
          .                 _Self
          .                 operator--(int) _GLIBCXX_NOEXCEPT
          .                 {
          .           	_Self __tmp = *this;
          .           	_M_node = _Rb_tree_decrement(_M_node);
-- line 310 ----------------------------------------
-- line 360 ----------------------------------------
          .           
          .                 pointer
          .                 operator->() const _GLIBCXX_NOEXCEPT
          .                 { return static_cast<_Link_type>(_M_node)->_M_valptr(); }
          .           
          .                 _Self&
          .                 operator++() _GLIBCXX_NOEXCEPT
          .                 {
         36 ( 0.00%)  	_M_node = _Rb_tree_increment(_M_node);
      1,098 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
        101 ( 0.00%)  => ???:std::_Rb_tree_increment(std::_Rb_tree_node_base const*) (7x)
          .           	return *this;
          .                 }
          .           
          .                 _Self
          .                 operator++(int) _GLIBCXX_NOEXCEPT
          .                 {
          .           	_Self __tmp = *this;
 20,021,282 ( 0.05%)  	_M_node = _Rb_tree_increment(_M_node);
 71,269,868 ( 0.19%)  => ???:std::_Rb_tree_increment(std::_Rb_tree_node_base const*) (5,000,291x)
  5,000,301 ( 0.01%)  	return __tmp;
          .                 }
          .           
          .                 _Self&
          .                 operator--() _GLIBCXX_NOEXCEPT
          .                 {
          .           	_M_node = _Rb_tree_decrement(_M_node);
          .           	return *this;
          .                 }
-- line 385 ----------------------------------------
-- line 394 ----------------------------------------
          .           
          .                 friend bool
          .                 operator==(const _Self& __x, const _Self& __y) _GLIBCXX_NOEXCEPT
          .                 { return __x._M_node == __y._M_node; }
          .           
          .           #if ! __cpp_lib_three_way_comparison
          .                 friend bool
          .                 operator!=(const _Self& __x, const _Self& __y) _GLIBCXX_NOEXCEPT
          3 ( 0.00%)        { return __x._M_node != __y._M_node; }
          .           #endif
          .           
          .                 _Base_ptr _M_node;
          .               };
          .           
          .             void
          .             _Rb_tree_insert_and_rebalance(const bool __insert_left,
          .           				_Rb_tree_node_base* __x,
-- line 410 ----------------------------------------
-- line 655 ----------------------------------------
          .           	_M_put_node(__p);
          .                 }
          .           
          .                 template<typename _NodeGen>
          .           	_Link_type
          .           	_M_clone_node(_Const_Link_type __x, _NodeGen& __node_gen)
          .           	{
          .           	  _Link_type __tmp = __node_gen(*__x->_M_valptr());
         30 ( 0.00%)  	  __tmp->_M_color = __x->_M_color;
         15 ( 0.00%)  	  __tmp->_M_left = 0;
         15 ( 0.00%)  	  __tmp->_M_right = 0;
          .           	  return __tmp;
          .           	}
          .           
          .               protected:
          .           #if _GLIBCXX_INLINE_VERSION
          .                 template<typename _Key_compare>
          .           #else
          .                 // Unused _Is_pod_comparator is kept as it is part of mangled name.
-- line 673 ----------------------------------------
-- line 721 ----------------------------------------
          .           
          .               protected:
          .                 _Base_ptr&
          .                 _M_root() _GLIBCXX_NOEXCEPT
          .                 { return this->_M_impl._M_header._M_parent; }
          .           
          .                 _Const_Base_ptr
          .                 _M_root() const _GLIBCXX_NOEXCEPT
         33 ( 0.00%)        { return this->_M_impl._M_header._M_parent; }
          .           
          .                 _Base_ptr&
          .                 _M_leftmost() _GLIBCXX_NOEXCEPT
          .                 { return this->_M_impl._M_header._M_left; }
          .           
          .                 _Const_Base_ptr
          .                 _M_leftmost() const _GLIBCXX_NOEXCEPT
          .                 { return this->_M_impl._M_header._M_left; }
-- line 737 ----------------------------------------
-- line 741 ----------------------------------------
          .                 { return this->_M_impl._M_header._M_right; }
          .           
          .                 _Const_Base_ptr
          .                 _M_rightmost() const _GLIBCXX_NOEXCEPT
          .                 { return this->_M_impl._M_header._M_right; }
          .           
          .                 _Link_type
          .                 _M_begin() _GLIBCXX_NOEXCEPT
     10,168 ( 0.00%)        { return static_cast<_Link_type>(this->_M_impl._M_header._M_parent); }
          .           
          .                 _Const_Link_type
          .                 _M_begin() const _GLIBCXX_NOEXCEPT
          .                 {
          .           	return static_cast<_Const_Link_type>
     59,307 ( 0.00%)  	  (this->_M_impl._M_header._M_parent);
          .                 }
          .           
          .                 _Base_ptr
          .                 _M_end() _GLIBCXX_NOEXCEPT
     10,263 ( 0.00%)        { return &this->_M_impl._M_header; }
          .           
          .                 _Const_Base_ptr
          .                 _M_end() const _GLIBCXX_NOEXCEPT
     59,352 ( 0.00%)        { return &this->_M_impl._M_header; }
          .           
          .                 static const _Key&
          .                 _S_key(_Const_Link_type __x)
          .                 {
          .           #if __cplusplus >= 201103L
          .           	// If we're asking for the key we're presumably using the comparison
          .           	// object, and so this is a good place to sanity check it.
          .           	static_assert(__is_invocable<_Compare&, const _Key&, const _Key&>{},
-- line 772 ----------------------------------------
-- line 781 ----------------------------------------
          .           	      "comparison object must be invocable as const");
          .           # endif // C++17
          .           #endif // C++11
          .           
          .           	return _KeyOfValue()(*__x->_M_valptr());
          .                 }
          .           
          .                 static _Link_type
      7,723 ( 0.00%)        _S_left(_Base_ptr __x) _GLIBCXX_NOEXCEPT
    135,768 ( 0.00%)        { return static_cast<_Link_type>(__x->_M_left); }
          .           
          .                 static _Const_Link_type
         92 ( 0.00%)        _S_left(_Const_Base_ptr __x) _GLIBCXX_NOEXCEPT
 72,070,263 ( 0.19%)        { return static_cast<_Const_Link_type>(__x->_M_left); }
          .           
          .                 static _Link_type
          .                 _S_right(_Base_ptr __x) _GLIBCXX_NOEXCEPT
    128,091 ( 0.00%)        { return static_cast<_Link_type>(__x->_M_right); }
          .           
          .                 static _Const_Link_type
          .                 _S_right(_Const_Base_ptr __x) _GLIBCXX_NOEXCEPT
 72,070,305 ( 0.19%)        { return static_cast<_Const_Link_type>(__x->_M_right); }
          .           
          .                 static const _Key&
          .                 _S_key(_Const_Base_ptr __x)
          .                 { return _S_key(static_cast<_Const_Link_type>(__x)); }
          .           
          .                 static _Base_ptr
          .                 _S_minimum(_Base_ptr __x) _GLIBCXX_NOEXCEPT
          .                 { return _Rb_tree_node_base::_S_minimum(__x); }
-- line 810 ----------------------------------------
-- line 889 ----------------------------------------
          .                 template<typename _NodeGen>
          .           	_Link_type
          .           	_M_copy(_Const_Link_type __x, _Base_ptr __p, _NodeGen&);
          .           
          .                 template<typename _NodeGen>
          .           	_Link_type
          .           	_M_copy(const _Rb_tree& __x, _NodeGen& __gen)
          .           	{
          5 ( 0.00%)  	  _Link_type __root = _M_copy(__x._M_begin(), _M_end(), __gen);
      2,952 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >* std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_copy<std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Alloc_node>(std::_Rb_tree_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const*, std::_Rb_tree_node_base*, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Alloc_node&) [clone .isra.0] (1x)
        611 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree_node<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >* std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_M_copy<std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node>(std::_Rb_tree_node<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > const*, std::_Rb_tree_node_base*, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node&) [clone .isra.0] (1x)
          6 ( 0.00%)  	  _M_leftmost() = _S_minimum(__root);
          2 ( 0.00%)  	  _M_rightmost() = _S_maximum(__root);
          4 ( 0.00%)  	  _M_impl._M_node_count = __x._M_impl._M_node_count;
          .           	  return __root;
          .           	}
          .           
          .                 _Link_type
          .                 _M_copy(const _Rb_tree& __x)
          .                 {
          .           	_Alloc_node __an(*this);
          .           	return _M_copy(__x, __an);
-- line 908 ----------------------------------------
-- line 937 ----------------------------------------
          .           
          .                 _Rb_tree(const _Compare& __comp,
          .           	       const allocator_type& __a = allocator_type())
          .                 : _M_impl(__comp, _Node_allocator(__a)) { }
          .           
          .                 _Rb_tree(const _Rb_tree& __x)
          .                 : _M_impl(__x._M_impl)
          .                 {
         66 ( 0.00%)  	if (__x._M_root() != 0)
          2 ( 0.00%)  	  _M_root() = _M_copy(__x);
          .                 }
          .           
          .           #if __cplusplus >= 201103L
          .                 _Rb_tree(const allocator_type& __a)
          .                 : _M_impl(_Node_allocator(__a))
          .                 { }
          .           
          .                 _Rb_tree(const _Rb_tree& __x, const allocator_type& __a)
-- line 954 ----------------------------------------
-- line 983 ----------------------------------------
          .           	_Rb_tree(std::declval<_Rb_tree&&>(), std::declval<_Node_allocator&&>(),
          .           		 std::declval<typename _Alloc_traits::is_always_equal>())) )
          .                 : _Rb_tree(std::move(__x), std::move(__a),
          .           		 typename _Alloc_traits::is_always_equal{})
          .                 { }
          .           #endif
          .           
          .                 ~_Rb_tree() _GLIBCXX_NOEXCEPT
         10 ( 0.00%)        { _M_erase(_M_begin()); }
      1,636 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_erase(std::_Rb_tree_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >*) [clone .isra.0] (2x)
          .           
          .                 _Rb_tree&
          .                 operator=(const _Rb_tree& __x);
          .           
          .                 // Accessors.
          .                 _Compare
          .                 key_comp() const
          .                 { return _M_impl._M_key_compare; }
          .           
          .                 iterator
          .                 begin() _GLIBCXX_NOEXCEPT
          .                 { return iterator(this->_M_impl._M_header._M_left); }
          .           
          .                 const_iterator
          .                 begin() const _GLIBCXX_NOEXCEPT
          4 ( 0.00%)        { return const_iterator(this->_M_impl._M_header._M_left); }
          .           
          .                 iterator
          .                 end() _GLIBCXX_NOEXCEPT
          .                 { return iterator(&this->_M_impl._M_header); }
          .           
          .                 const_iterator
          .                 end() const _GLIBCXX_NOEXCEPT
      5,080 ( 0.00%)        { return const_iterator(&this->_M_impl._M_header); }
          .           
          .                 reverse_iterator
          .                 rbegin() _GLIBCXX_NOEXCEPT
          .                 { return reverse_iterator(end()); }
          .           
          .                 const_reverse_iterator
          .                 rbegin() const _GLIBCXX_NOEXCEPT
          .                 { return const_reverse_iterator(end()); }
-- line 1023 ----------------------------------------
-- line 1795 ----------------------------------------
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .           #if __cplusplus >= 201103L
          .               template<typename _Arg, typename _NodeGen>
          .           #else
          .               template<typename _NodeGen>
          .           #endif
          .                 typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
      7,518 ( 0.00%)        _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .                 _M_insert_(_Base_ptr __x, _Base_ptr __p,
          .           #if __cplusplus >= 201103L
          .           		 _Arg&& __v,
          .           #else
          .           		 const _Val& __v,
          .           #endif
          .           		 _NodeGen& __node_gen)
          .                 {
     15,126 ( 0.00%)  	bool __insert_left = (__x != 0 || __p == _M_end()
     22,590 ( 0.00%)  			      || _M_impl._M_key_compare(_KeyOfValue()(__v),
          .           							_S_key(__p)));
          .           
          .           	_Link_type __z = __node_gen(_GLIBCXX_FORWARD(_Arg, __v));
          .           
     45,310 ( 0.00%)  	_Rb_tree_insert_and_rebalance(__insert_left, __z, __p,
      1,246 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
        498 ( 0.00%)  => ???:std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&) (29x)
          .           				      this->_M_impl._M_header);
      7,581 ( 0.00%)  	++_M_impl._M_node_count;
          .           	return iterator(__z);
          .                 }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .           #if __cplusplus >= 201103L
          .               template<typename _Arg>
          .           #endif
-- line 1828 ----------------------------------------
-- line 1869 ----------------------------------------
          .           	}
          .                 return _M_insert_lower(__y, _GLIBCXX_FORWARD(_Arg, __v));
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KoV,
          .           	   typename _Compare, typename _Alloc>
          .               template<typename _NodeGen>
          .                 typename _Rb_tree<_Key, _Val, _KoV, _Compare, _Alloc>::_Link_type
         56 ( 0.00%)        _Rb_tree<_Key, _Val, _KoV, _Compare, _Alloc>::
          .                 _M_copy(_Const_Link_type __x, _Base_ptr __p, _NodeGen& __node_gen)
          .                 {
          .           	// Structural copy. __x and __p must be non-null.
          .           	_Link_type __top = _M_clone_node(__x, __node_gen);
          8 ( 0.00%)  	__top->_M_parent = __p;
          .           
          .           	__try
          .           	  {
         24 ( 0.00%)  	    if (__x->_M_right)
         12 ( 0.00%)  	      __top->_M_right = _M_copy(_S_right(__x), __top, __node_gen);
        211 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree_node<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >* std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_M_copy<std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node>(std::_Rb_tree_node<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > const*, std::_Rb_tree_node_base*, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node&) [clone .isra.0]'2 (1x)
          .           	    __p = __top;
          .           	    __x = _S_left(__x);
          .           
         34 ( 0.00%)  	    while (__x != 0)
          .           	      {
          .           		_Link_type __y = _M_clone_node(__x, __node_gen);
          7 ( 0.00%)  		__p->_M_left = __y;
          7 ( 0.00%)  		__y->_M_parent = __p;
         21 ( 0.00%)  		if (__x->_M_right)
          6 ( 0.00%)  		  __y->_M_right = _M_copy(_S_right(__x), __y, __node_gen);
        992 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >* std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_copy<std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Alloc_node>(std::_Rb_tree_node<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const*, std::_Rb_tree_node_base*, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_Alloc_node&) [clone .isra.0]'2 (2x)
          .           		__p = __y;
          .           		__x = _S_left(__x);
          .           	      }
          .           	  }
          .           	__catch(...)
          .           	  {
          .           	    _M_erase(__top);
          .           	    __throw_exception_again;
          .           	  }
          .           	return __top;
         56 ( 0.00%)        }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               void
      9,402 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_erase(_Link_type __x)
          .               {
          .                 // Erase without rebalancing.
     43,435 ( 0.00%)        while (__x != 0)
          .           	{
     15,545 ( 0.00%)  	  _M_erase(_S_right(__x));
    314,079 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree<unsigned long, std::pair<unsigned long const, NP::Job<long long> const*>, std::_Select1st<std::pair<unsigned long const, NP::Job<long long> const*> >, std::less<unsigned long>, std::allocator<std::pair<unsigned long const, NP::Job<long long> const*> > >::_M_erase(std::_Rb_tree_node<std::pair<unsigned long const, NP::Job<long long> const*> >*) [clone .isra.0] (10x)
          .           	  _Link_type __y = _S_left(__x);
          .           	  _M_drop_node(__x);
          .           	  __x = __y;
          .           	}
     12,937 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           		      _Compare, _Alloc>::iterator
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_lower_bound(_Link_type __x, _Base_ptr __y,
          .           		   const _Key& __k)
          .               {
        463 ( 0.00%)        while (__x != 0)
        202 ( 0.00%)  	if (!_M_impl._M_key_compare(_S_key(__x), __k))
         59 ( 0.00%)  	  __y = __x, __x = _S_left(__x);
          .           	else
          .           	  __x = _S_right(__x);
          .                 return iterator(__y);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           		      _Compare, _Alloc>::const_iterator
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_lower_bound(_Const_Link_type __x, _Const_Base_ptr __y,
          .           		   const _Key& __k) const
          .               {
    153,257 ( 0.00%)        while (__x != 0)
     64,532 ( 0.00%)  	if (!_M_impl._M_key_compare(_S_key(__x), __k))
         92 ( 0.00%)  	  __y = __x, __x = _S_left(__x);
          .           	else
          .           	  __x = _S_right(__x);
          .                 return const_iterator(__y);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue,
-- line 1960 ----------------------------------------
-- line 1974 ----------------------------------------
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           		      _Compare, _Alloc>::const_iterator
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_upper_bound(_Const_Link_type __x, _Const_Base_ptr __y,
          .           		   const _Key& __k) const
          .               {
175,210,438 ( 0.46%)        while (__x != 0)
144,076,286 ( 0.38%)  	if (_M_impl._M_key_compare(__k, _S_key(__x)))
          .           	  __y = __x, __x = _S_left(__x);
          .           	else
          .           	  __x = _S_right(__x);
          .                 return const_iterator(__y);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
-- line 1991 ----------------------------------------
-- line 2083 ----------------------------------------
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               pair<typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr,
          .           	 typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr>
         27 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_get_insert_unique_pos(const key_type& __k)
          .               {
          .                 typedef pair<_Base_ptr, _Base_ptr> _Res;
          .                 _Link_type __x = _M_begin();
          .                 _Base_ptr __y = _M_end();
          .                 bool __comp = true;
    154,875 ( 0.00%)        while (__x != 0)
          .           	{
          .           	  __y = __x;
          .           	  __comp = _M_impl._M_key_compare(__k, _S_key(__x));
     76,150 ( 0.00%)  	  __x = __comp ? _S_left(__x) : _S_right(__x);
          .           	}
          .                 iterator __j = iterator(__y);
          5 ( 0.00%)        if (__comp)
          .           	{
      1,353 ( 0.00%)  	  if (__j == begin())
          6 ( 0.00%)  	    return _Res(__x, __y);
          .           	  else
          .           	    --__j;
          .           	}
      5,014 ( 0.00%)        if (_M_impl._M_key_compare(_S_key(__j._M_node), __k))
          .           	return _Res(__x, __y);
          .                 return _Res(__j._M_node, 0);
         24 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               pair<typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr,
          .           	 typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr>
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_get_insert_equal_pos(const key_type& __k)
          .               {
          .                 typedef pair<_Base_ptr, _Base_ptr> _Res;
          .                 _Link_type __x = _M_begin();
          .                 _Base_ptr __y = _M_end();
    346,820 ( 0.00%)        while (__x != 0)
          .           	{
          .           	  __y = __x;
    173,404 ( 0.00%)  	  __x = _M_impl._M_key_compare(__k, _S_key(__x)) ?
          .           		_S_left(__x) : _S_right(__x);
          .           	}
          .                 return _Res(__x, __y);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .           #if __cplusplus >= 201103L
          .               template<typename _Arg>
          .           #endif
          .               pair<typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::iterator, bool>
        300 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .           #if __cplusplus >= 201103L
          .               _M_insert_unique(_Arg&& __v)
          .           #else
          .               _M_insert_unique(const _Val& __v)
          .           #endif
          .               {
          .                 typedef pair<iterator, bool> _Res;
          .                 pair<_Base_ptr, _Base_ptr> __res
          .           	= _M_get_insert_unique_pos(_KeyOfValue()(__v));
          .           
          2 ( 0.00%)        if (__res.second)
          .           	{
          .           	  _Alloc_node __an(*this);
          .           	  return _Res(_M_insert_(__res.first, __res.second,
          .           				 _GLIBCXX_FORWARD(_Arg, __v), __an),
         60 ( 0.00%)  		      true);
          .           	}
          .           
          .                 return _Res(iterator(__res.first), false);
        268 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .           #if __cplusplus >= 201103L
          .               template<typename _Arg>
          .           #endif
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
     67,689 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .           #if __cplusplus >= 201103L
          .               _M_insert_equal(_Arg&& __v)
          .           #else
          .               _M_insert_equal(const _Val& __v)
          .           #endif
          .               {
          .                 pair<_Base_ptr, _Base_ptr> __res
          .           	= _M_get_insert_equal_pos(_KeyOfValue()(__v));
          .                 _Alloc_node __an(*this);
          .                 return _M_insert_(__res.first, __res.second,
          .           			_GLIBCXX_FORWARD(_Arg, __v), __an);
     67,689 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               pair<typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr,
          .           	 typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			   _Compare, _Alloc>::_Base_ptr>
        108 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_get_insert_hint_unique_pos(const_iterator __position,
          .           				  const key_type& __k)
          .               {
          .                 iterator __pos = __position._M_const_cast();
          .                 typedef pair<_Base_ptr, _Base_ptr> _Res;
          .           
          .                 // end()
        105 ( 0.00%)        if (__pos._M_node == _M_end())
          .           	{
          .           	  if (size() > 0
         21 ( 0.00%)  	      && _M_impl._M_key_compare(_S_key(_M_rightmost()), __k))
          .           	    return _Res(0, _M_rightmost());
          .           	  else
          3 ( 0.00%)  	    return _M_get_insert_unique_pos(__k);
         26 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_get_insert_unique_pos(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
          .           	}
         81 ( 0.00%)        else if (_M_impl._M_key_compare(__k, _S_key(__pos._M_node)))
          .           	{
          .           	  // First, try before...
          .           	  iterator __before = __pos;
         66 ( 0.00%)  	  if (__pos._M_node == _M_leftmost()) // begin()
         18 ( 0.00%)  	    return _Res(_M_leftmost(), _M_leftmost());
         52 ( 0.00%)  	  else if (_M_impl._M_key_compare(_S_key((--__before)._M_node), __k))
          .           	    {
         47 ( 0.00%)  	      if (_S_right(__before._M_node) == 0)
         42 ( 0.00%)  		return _Res(0, __before._M_node);
          .           	      else
          .           		return _Res(__pos._M_node, __pos._M_node);
          .           	    }
          .           	  else
          .           	    return _M_get_insert_unique_pos(__k);
          .           	}
          .                 else if (_M_impl._M_key_compare(_S_key(__pos._M_node), __k))
          .           	{
          .           	  // ... then try after.
          .           	  iterator __after = __pos;
          .           	  if (__pos._M_node == _M_rightmost())
          4 ( 0.00%)  	    return _Res(0, _M_rightmost());
          .           	  else if (_M_impl._M_key_compare(__k, _S_key((++__after)._M_node)))
          .           	    {
         10 ( 0.00%)  	      if (_S_right(__pos._M_node) == 0)
          .           		return _Res(0, __pos._M_node);
          .           	      else
          .           		return _Res(__after._M_node, __after._M_node);
          .           	    }
          .           	  else
         10 ( 0.00%)  	    return _M_get_insert_unique_pos(__k);
         52 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, optparse::Option const*>, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, optparse::Option const*> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, optparse::Option const*> > >::_M_get_insert_unique_pos(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (2x)
          .           	}
          .                 else
          .           	// Equivalent keys.
          .           	return _Res(__pos._M_node, 0);
         95 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .           #if __cplusplus >= 201103L
          .               template<typename _Arg, typename _NodeGen>
          .           #else
          .               template<typename _NodeGen>
          .           #endif
-- line 2252 ----------------------------------------
-- line 2352 ----------------------------------------
          .           
          .           	return _M_insert_equal_lower(_GLIBCXX_FORWARD(_Arg, __v));
          .                 }
          .           
          .           #if __cplusplus >= 201103L
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
      2,506 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_insert_node(_Base_ptr __x, _Base_ptr __p, _Link_type __z)
          .               {
      5,146 ( 0.00%)        bool __insert_left = (__x != 0 || __p == _M_end()
     10,080 ( 0.00%)  			    || _M_impl._M_key_compare(_S_key(__z),
          .           						      _S_key(__p)));
          .           
     12,763 ( 0.00%)        _Rb_tree_insert_and_rebalance(__insert_left, __z, __p,
      1,035 ( 0.00%)  => ???:std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&) (27x)
          .           				    this->_M_impl._M_header);
      2,546 ( 0.00%)        ++_M_impl._M_node_count;
          .                 return iterator(__z);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               _M_insert_lower_node(_Base_ptr __p, _Link_type __z)
-- line 2377 ----------------------------------------
-- line 2403 ----------------------------------------
          .                 return _M_insert_lower_node(__y, __z);
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               template<typename... _Args>
          .                 pair<typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           			     _Compare, _Alloc>::iterator, bool>
     17,549 ( 0.00%)        _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .                 _M_emplace_unique(_Args&&... __args)
          .                 {
          .           	_Link_type __z = _M_create_node(std::forward<_Args>(__args)...);
          .           
          .           	__try
          .           	  {
          .           	    typedef pair<iterator, bool> _Res;
          .           	    auto __res = _M_get_insert_unique_pos(_S_key(__z));
      1,224 ( 0.00%)  	    if (__res.second)
      5,014 ( 0.00%)  	      return _Res(_M_insert_node(__res.first, __res.second, __z), true);
          .           	
          .           	    _M_drop_node(__z);
          .           	    return _Res(iterator(__res.first), false);
          .           	  }
          .           	__catch(...)
          .           	  {
          .           	    _M_drop_node(__z);
          .           	    __throw_exception_again;
          .           	  }
     15,042 ( 0.00%)        }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               template<typename... _Args>
          .                 typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
          .                 _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .                 _M_emplace_equal(_Args&&... __args)
          .                 {
-- line 2439 ----------------------------------------
-- line 2450 ----------------------------------------
          .           	    __throw_exception_again;
          .           	  }
          .                 }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               template<typename... _Args>
          .                 typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
        444 ( 0.00%)        _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .                 _M_emplace_hint_unique(const_iterator __pos, _Args&&... __args)
          .                 {
          .           	_Link_type __z = _M_create_node(std::forward<_Args>(__args)...);
          .           
          .           	__try
          .           	  {
         72 ( 0.00%)  	    auto __res = _M_get_insert_hint_unique_pos(__pos, _S_key(__z));
      1,161 ( 0.00%)  => /usr/include/c++/10/bits/stl_tree.h:std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (12x)
          .           
         28 ( 0.00%)  	    if (__res.second)
          .           	      return _M_insert_node(__res.first, __res.second, __z);
          .           
          .           	    _M_drop_node(__z);
          .           	    return iterator(__res.first);
          .           	  }
          .           	__catch(...)
          .           	  {
          .           	    _M_drop_node(__z);
          .           	    __throw_exception_again;
          .           	  }
        339 ( 0.00%)        }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               template<typename... _Args>
          .                 typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator
          .                 _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .                 _M_emplace_hint_equal(const_iterator __pos, _Args&&... __args)
          .                 {
-- line 2486 ----------------------------------------
-- line 2555 ----------------------------------------
          .           	      || _M_impl._M_key_compare(__k,
          .           					_S_key(__j._M_node))) ? end() : __j;
          .               }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue,
          .           		      _Compare, _Alloc>::const_iterator
        371 ( 0.00%)      _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               find(const _Key& __k) const
          .               {
          .                 const_iterator __j = _M_lower_bound(_M_begin(), _M_end(), __k);
          .                 return (__j == end()
         82 ( 0.00%)  	      || _M_impl._M_key_compare(__k,
         86 ( 0.00%)  					_S_key(__j._M_node))) ? end() : __j;
        477 ( 0.00%)      }
          .           
          .             template<typename _Key, typename _Val, typename _KeyOfValue,
          .           	   typename _Compare, typename _Alloc>
          .               typename _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type
          .               _Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::
          .               count(const _Key& __k) const
          .               {
          .                 pair<const_iterator, const_iterator> __p = equal_range(__k);
-- line 2578 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/atomic_base.h
--------------------------------------------------------------------------------
Ir                   

-- line 188 ----------------------------------------
          .               atomic_flag() noexcept = default;
          .               ~atomic_flag() noexcept = default;
          .               atomic_flag(const atomic_flag&) = delete;
          .               atomic_flag& operator=(const atomic_flag&) = delete;
          .               atomic_flag& operator=(const atomic_flag&) volatile = delete;
          .           
          .               // Conversion to ATOMIC_FLAG_INIT.
          .               constexpr atomic_flag(bool __i) noexcept
         31 ( 0.00%)        : __atomic_flag_base{ _S_init(__i) }
          .               { }
          .           
          .               _GLIBCXX_ALWAYS_INLINE bool
          .               test_and_set(memory_order __m = memory_order_seq_cst) noexcept
          .               {
      3,686 ( 0.00%)        return __atomic_test_and_set (&_M_i, int(__m));
          .               }
          .           
          .               _GLIBCXX_ALWAYS_INLINE bool
          .               test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept
          .               {
          .                 return __atomic_test_and_set (&_M_i, int(__m));
          .               }
          .           
-- line 210 ----------------------------------------
-- line 211 ----------------------------------------
          .               _GLIBCXX_ALWAYS_INLINE void
          .               clear(memory_order __m = memory_order_seq_cst) noexcept
          .               {
          .                 memory_order __b = __m & __memory_order_mask;
          .                 __glibcxx_assert(__b != memory_order_consume);
          .                 __glibcxx_assert(__b != memory_order_acquire);
          .                 __glibcxx_assert(__b != memory_order_acq_rel);
          .           
      1,106 ( 0.00%)        __atomic_clear (&_M_i, int(__m));
          .               }
          .           
          .               _GLIBCXX_ALWAYS_INLINE void
          .               clear(memory_order __m = memory_order_seq_cst) volatile noexcept
          .               {
          .                 memory_order __b = __m & __memory_order_mask;
          .                 __glibcxx_assert(__b != memory_order_consume);
          .                 __glibcxx_assert(__b != memory_order_acquire);
-- line 227 ----------------------------------------
-- line 278 ----------------------------------------
          .               public:
          .                 __atomic_base() noexcept = default;
          .                 ~__atomic_base() noexcept = default;
          .                 __atomic_base(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
          .           
          .                 // Requires __int_type convertible to _M_i.
        247 ( 0.00%)        constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }
          .           
          .                 operator __int_type() const noexcept
          .                 { return load(); }
          .           
          .                 operator __int_type() const volatile noexcept
          .                 { return load(); }
          .           
          .                 __int_type
-- line 294 ----------------------------------------
-- line 396 ----------------------------------------
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           	__glibcxx_assert(__b != memory_order_consume);
          .           
 50,462,741 ( 0.13%)  	__atomic_store_n(&_M_i, __i, int(__m));
         11 ( 0.00%)        }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__int_type __i,
          .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 413 ----------------------------------------
-- line 418 ----------------------------------------
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 load(memory_order __m = memory_order_seq_cst) const noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           
227,059,890 ( 0.59%)  	return __atomic_load_n(&_M_i, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 434 ----------------------------------------
-- line 501 ----------------------------------------
          .           			      memory_order __m1, memory_order __m2) noexcept
          .                 {
          .           	memory_order __b2 = __m2 & __memory_order_mask;
          .           	memory_order __b1 = __m1 & __memory_order_mask;
          .           	__glibcxx_assert(__b2 != memory_order_release);
          .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
          .           	__glibcxx_assert(__b2 <= __b1);
          .           
      2,049 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0,
          .           					   int(__m1), int(__m2));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE bool
          .                 compare_exchange_strong(__int_type& __i1, __int_type __i2,
          .           			      memory_order __m1,
          .           			      memory_order __m2) volatile noexcept
          .                 {
-- line 517 ----------------------------------------
-- line 540 ----------------------------------------
          .                 {
          .           	return compare_exchange_strong(__i1, __i2, __m,
          .           				       __cmpexch_failure_order(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_add(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
      1,549 ( 0.00%)        { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_add(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_sub(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
        323 ( 0.00%)        { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_sub(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_and(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
        390 ( 0.00%)        { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_and(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_or(__int_type __i,
          .           	       memory_order __m = memory_order_seq_cst) noexcept
        754 ( 0.00%)        { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_or(__int_type __i,
          .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_xor(__int_type __i,
-- line 586 ----------------------------------------
-- line 613 ----------------------------------------
          .               public:
          .                 __atomic_base() noexcept = default;
          .                 ~__atomic_base() noexcept = default;
          .                 __atomic_base(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
          .           
          .                 // Requires __pointer_type convertible to _M_p.
         55 ( 0.00%)        constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }
          .           
          .                 operator __pointer_type() const noexcept
          .                 { return load(); }
          .           
          .                 operator __pointer_type() const volatile noexcept
          .                 { return load(); }
          .           
          .                 __pointer_type
-- line 629 ----------------------------------------
-- line 717 ----------------------------------------
          .           	    memory_order __m = memory_order_seq_cst) noexcept
          .                 {
          .                   memory_order __b = __m & __memory_order_mask;
          .           
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           	__glibcxx_assert(__b != memory_order_consume);
          .           
     19,397 ( 0.00%)  	__atomic_store_n(&_M_p, __p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__pointer_type __p,
          .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
-- line 733 ----------------------------------------
-- line 739 ----------------------------------------
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 load(memory_order __m = memory_order_seq_cst) const noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           
313,871,966 ( 0.82%)  	return __atomic_load_n(&_M_p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 755 ----------------------------------------
-- line 756 ----------------------------------------
          .           
          .           	return __atomic_load_n(&_M_p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 exchange(__pointer_type __p,
          .           	       memory_order __m = memory_order_seq_cst) noexcept
          .                 {
     15,961 ( 0.00%)  	return __atomic_exchange_n(&_M_p, __p, int(__m));
          .                 }
          .           
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 exchange(__pointer_type __p,
          .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	return __atomic_exchange_n(&_M_p, __p, int(__m));
-- line 772 ----------------------------------------
-- line 778 ----------------------------------------
          .           			      memory_order __m2) noexcept
          .                 {
          .           	memory_order __b2 = __m2 & __memory_order_mask;
          .           	memory_order __b1 = __m1 & __memory_order_mask;
          .           	__glibcxx_assert(__b2 != memory_order_release);
          .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
          .           	__glibcxx_assert(__b2 <= __b1);
          .           
      1,050 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0,
          .           					   int(__m1), int(__m2));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE bool
          .                 compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,
          .           			      memory_order __m1,
          .           			      memory_order __m2) volatile noexcept
          .                 {
-- line 794 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_algobase.h
--------------------------------------------------------------------------------
Ir                     

-- line 176 ----------------------------------------
            .           	&& __are_same<_ValueType1&, _ReferenceType1>::__value
            .           	&& __are_same<_ValueType2&, _ReferenceType2>::__value>::
            .           	iter_swap(__a, __b);
            .           #else
            .                 // _GLIBCXX_RESOLVE_LIB_DEFECTS
            .                 // 187. iter_swap underspecified
            .                 swap(*__a, *__b);
            .           #endif
            3 ( 0.00%)      }
            .           
            .             /**
            .              *  @brief Swap the elements of two sequences.
            .              *  @ingroup mutating_algorithms
            .              *  @param  __first1  A forward iterator.
            .              *  @param  __last1   A forward iterator.
            .              *  @param  __first2  A forward iterator.
            .              *  @return   An iterator equal to @p first2+(last1-first1).
-- line 192 ----------------------------------------
-- line 251 ----------------------------------------
            .             template<typename _Tp>
            .               _GLIBCXX14_CONSTEXPR
            .               inline const _Tp&
            .               max(const _Tp& __a, const _Tp& __b)
            .               {
            .                 // concept requirements
            .                 __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
            .                 //return  __a < __b ? __b : __a;
   25,152,548 ( 0.07%)        if (__a < __b)
            .           	return __b;
            .                 return __a;
            .               }
            .           
            .             /**
            .              *  @brief This does what you think it does.
            .              *  @ingroup sorting_algorithms
            .              *  @param  __a  A thing of arbitrary type.
-- line 267 ----------------------------------------
-- line 370 ----------------------------------------
            .               struct __copy_move<false, false, random_access_iterator_tag>
            .               {
            .                 template<typename _II, typename _OI>
            .           	_GLIBCXX20_CONSTEXPR
            .           	static _OI
            .           	__copy_m(_II __first, _II __last, _OI __result)
            .           	{
            .           	  typedef typename iterator_traits<_II>::difference_type _Distance;
          151 ( 0.00%)  	  for(_Distance __n = __last - __first; __n > 0; --__n)
            .           	    {
            .           	      *__result = *__first;
            .           	      ++__first;
            .           	      ++__result;
            .           	    }
            .           	  return __result;
            .           	}
            .               };
-- line 386 ----------------------------------------
-- line 416 ----------------------------------------
            .           	{
            .           #if __cplusplus >= 201103L
            .           	  using __assignable = conditional<_IsMove,
            .           					   is_move_assignable<_Tp>,
            .           					   is_copy_assignable<_Tp>>;
            .           	  // trivial types can have deleted assignment
            .           	  static_assert( __assignable::type::value, "type is not assignable" );
            .           #endif
    9,432,027 ( 0.02%)  	  const ptrdiff_t _Num = __last - __first;
    6,287,556 ( 0.02%)  	  if (_Num)
   18,863,165 ( 0.05%)  	    __builtin_memmove(__result, __first, sizeof(_Tp) * _Num);
           22 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms (2x)
           42 ( 0.00%)  	  return __result + _Num;
            .           	}
            .               };
            .           
            .             // Helpers for streambuf iterators (either istream or ostream).
            .             // NB: avoid including <iosfwd>, relatively large.
            .             template<typename _CharT>
            .               struct char_traits;
            .           
-- line 435 ----------------------------------------
-- line 642 ----------------------------------------
            .               {
            .                 template<typename _BI1, typename _BI2>
            .           	_GLIBCXX20_CONSTEXPR
            .           	static _BI2
            .           	__copy_move_b(_BI1 __first, _BI1 __last, _BI2 __result)
            .           	{
            .           	  typename iterator_traits<_BI1>::difference_type
            .           	    __n = __last - __first;
        5,040 ( 0.00%)  	  for (; __n > 0; --__n)
            .           	    *--__result = *--__last;
            .           	  return __result;
            .           	}
            .               };
            .           
            .           #if __cplusplus >= 201103L
            .             template<>
            .               struct __copy_move_backward<true, false, random_access_iterator_tag>
-- line 658 ----------------------------------------
-- line 681 ----------------------------------------
            .           	{
            .           #if __cplusplus >= 201103L
            .           	  using __assignable = conditional<_IsMove,
            .           					   is_move_assignable<_Tp>,
            .           					   is_copy_assignable<_Tp>>;
            .           	  // trivial types can have deleted assignment
            .           	  static_assert( __assignable::type::value, "type is not assignable" );
            .           #endif
       14,160 ( 0.00%)  	  const ptrdiff_t _Num = __last - __first;
       14,178 ( 0.00%)  	  if (_Num)
    6,308,800 ( 0.02%)  	    __builtin_memmove(__result - _Num, __first, sizeof(_Tp) * _Num);
           12 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms (1x)
            .           	  return __result - _Num;
            .           	}
            .               };
            .           
            .             template<bool _IsMove, typename _BI1, typename _BI2>
            .               _GLIBCXX20_CONSTEXPR
            .               inline _BI2
            .               __copy_move_backward_a2(_BI1 __first, _BI1 __last, _BI2 __result)
-- line 699 ----------------------------------------
-- line 1365 ----------------------------------------
            .             { return (int)sizeof(int) * __CHAR_BIT__  - 1 - __builtin_clz(__n); }
            .           
            .             inline _GLIBCXX_CONSTEXPR unsigned
            .             __lg(unsigned __n)
            .             { return (int)sizeof(int) * __CHAR_BIT__  - 1 - __builtin_clz(__n); }
            .           
            .             inline _GLIBCXX_CONSTEXPR long
            .             __lg(long __n)
   15,718,796 ( 0.04%)    { return (int)sizeof(long) * __CHAR_BIT__ - 1 - __builtin_clzl(__n); }
            .           
            .             inline _GLIBCXX_CONSTEXPR unsigned long
            .             __lg(unsigned long __n)
            .             { return (int)sizeof(long) * __CHAR_BIT__ - 1 - __builtin_clzl(__n); }
            .           
            .             inline _GLIBCXX_CONSTEXPR long long
            .             __lg(long long __n)
            .             { return (int)sizeof(long long) * __CHAR_BIT__ - 1 - __builtin_clzll(__n); }
-- line 1381 ----------------------------------------
-- line 1904 ----------------------------------------
            .           
            .             /// This is an overload used by find algos for the Input Iterator case.
            .             template<typename _InputIterator, typename _Predicate>
            .               _GLIBCXX20_CONSTEXPR
            .               inline _InputIterator
            .               __find_if(_InputIterator __first, _InputIterator __last,
            .           	      _Predicate __pred, input_iterator_tag)
            .               {
           15 ( 0.00%)        while (__first != __last && !__pred(__first))
            .           	++__first;
            .                 return __first;
            .               }
            .           
            .             /// This is an overload used by find algos for the RAI case.
            .             template<typename _RandomAccessIterator, typename _Predicate>
            .               _GLIBCXX20_CONSTEXPR
            .               _RandomAccessIterator
            .               __find_if(_RandomAccessIterator __first, _RandomAccessIterator __last,
            .           	      _Predicate __pred, random_access_iterator_tag)
            .               {
            .                 typename iterator_traits<_RandomAccessIterator>::difference_type
    5,000,323 ( 0.01%)  	__trip_count = (__last - __first) >> 2;
            .           
2,326,782,572 ( 6.09%)        for (; __trip_count > 0; --__trip_count)
            .           	{
            .           	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
            .           
            .           	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
-- line 1935 ----------------------------------------
-- line 1938 ----------------------------------------
            .           	    return __first;
            .           	  ++__first;
            .           
            .           	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
            .           	}
            .           
    9,996,704 ( 0.03%)        switch (__last - __first)
            .           	{
            .           	case 3:
            .           	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
            .           	  // FALLTHRU
            .           	case 2:
           20 ( 0.00%)  	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
            .           	  // FALLTHRU
            .           	case 1:
           50 ( 0.00%)  	  if (__pred(__first))
            .           	    return __first;
            .           	  ++__first;
            .           	  // FALLTHRU
            .           	case 0:
            .           	default:
            .           	  return __last;
            .           	}
            .               }
-- line 1967 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/hashtable_policy.h
--------------------------------------------------------------------------------
Ir                  

-- line 210 ----------------------------------------
         .              *  template parameter of class template _Hashtable controls whether
         .              *  nodes also store a hash code. In some cases (e.g. strings) this
         .              *  may be a performance win.
         .              */
         .             struct _Hash_node_base
         .             {
         .               _Hash_node_base* _M_nxt;
         .           
 3,153,784 ( 0.01%)      _Hash_node_base() noexcept : _M_nxt() { }
         .           
         .               _Hash_node_base(_Hash_node_base* __next) noexcept : _M_nxt(__next) { }
         .             };
         .           
         .             /**
         .              *  struct _Hash_node_value_base
         .              *
         .              *  Node type with the value to store.
-- line 226 ----------------------------------------
-- line 261 ----------------------------------------
         .              *  Base class is __detail::_Hash_node_value_base.
         .              */
         .             template<typename _Value>
         .               struct _Hash_node<_Value, true> : _Hash_node_value_base<_Value>
         .               {
         .                 std::size_t  _M_hash_code;
         .           
         .                 _Hash_node*
 3,176,238 ( 0.01%)        _M_next() const noexcept
 3,176,238 ( 0.01%)        { return static_cast<_Hash_node*>(this->_M_nxt); }
         .               };
         .           
         .             /**
         .              *  Specialization for nodes without caches, struct _Hash_node.
         .              *
         .              *  Base class is __detail::_Hash_node_value_base.
         .              */
         .             template<typename _Value>
         .               struct _Hash_node<_Value, false> : _Hash_node_value_base<_Value>
         .               {
         .                 _Hash_node*
         1 ( 0.00%)        _M_next() const noexcept
         1 ( 0.00%)        { return static_cast<_Hash_node*>(this->_M_nxt); }
         .               };
         .           
         .             /// Base class for node iterators.
         .             template<typename _Value, bool _Cache_hash_code>
         .               struct _Node_iterator_base
         .               {
         .                 using __node_type = _Hash_node<_Value, _Cache_hash_code>;
         .           
         .                 __node_type*  _M_cur;
         .           
         .                 _Node_iterator_base(__node_type* __p) noexcept
     7,521 ( 0.00%)        : _M_cur(__p) { }
         .           
         .                 void
         .                 _M_incr() noexcept
         .                 { _M_cur = _M_cur->_M_next(); }
         .               };
         .           
         .             template<typename _Value, bool _Cache_hash_code>
         .               inline bool
-- line 303 ----------------------------------------
-- line 333 ----------------------------------------
         .                 using reference = typename std::conditional<__constant_iterators,
         .           						  const _Value&, _Value&>::type;
         .           
         .                 _Node_iterator() noexcept
         .                 : __base_type(0) { }
         .           
         .                 explicit
         .                 _Node_iterator(__node_type* __p) noexcept
     5,014 ( 0.00%)        : __base_type(__p) { }
         .           
         .                 reference
         .                 operator*() const noexcept
         .                 { return this->_M_cur->_M_v(); }
         .           
         .                 pointer
         .                 operator->() const noexcept
         .                 { return this->_M_cur->_M_valptr(); }
-- line 349 ----------------------------------------
-- line 381 ----------------------------------------
         .                 typedef const _Value*				pointer;
         .                 typedef const _Value&				reference;
         .           
         .                 _Node_const_iterator() noexcept
         .                 : __base_type(0) { }
         .           
         .                 explicit
         .                 _Node_const_iterator(__node_type* __p) noexcept
     2,507 ( 0.00%)        : __base_type(__p) { }
         .           
         .                 _Node_const_iterator(const _Node_iterator<_Value, __constant_iterators,
         .           			   __cache>& __x) noexcept
         .                 : __base_type(__x._M_cur) { }
         .           
         .                 reference
         .                 operator*() const noexcept
         .                 { return this->_M_cur->_M_v(); }
-- line 397 ----------------------------------------
-- line 425 ----------------------------------------
         .             {
         .               typedef std::size_t first_argument_type;
         .               typedef std::size_t second_argument_type;
         .               typedef std::size_t result_type;
         .           
         .               result_type
         .               operator()(first_argument_type __num,
         .           	       second_argument_type __den) const noexcept
36,066,521 ( 0.09%)      { return __num % __den; }
         .             };
         .           
         .             /// Default ranged hash function H.  In principle it should be a
         .             /// function object composed from objects of type H1 and H2 such that
         .             /// h(k, N) = h2(h1(k), N), but that would mean making extra copies of
         .             /// h1 and h2.  So instead we'll just use a tag to tell class template
         .             /// hashtable to do that composition.
         .             struct _Default_ranged_hash { };
-- line 441 ----------------------------------------
-- line 442 ----------------------------------------
         .           
         .             /// Default value for rehash policy.  Bucket size is (usually) the
         .             /// smallest prime that keeps the load factor small enough.
         .             struct _Prime_rehash_policy
         .             {
         .               using __has_load_factor = true_type;
         .           
         .               _Prime_rehash_policy(float __z = 1.0) noexcept
        13 ( 0.00%)      : _M_max_load_factor(__z), _M_next_resize(0) { }
         .           
         .               float
         .               max_load_factor() const noexcept
         .               { return _M_max_load_factor; }
         .           
         .               // Return a bucket size no smaller than n.
         .               std::size_t
         .               _M_next_bkt(std::size_t __n) const;
-- line 458 ----------------------------------------
-- line 720 ----------------------------------------
         .                 __node._M_node = nullptr;
         .                 return __pos->second;
         .               }
         .           
         .             template<typename _Key, typename _Pair, typename _Alloc, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash,
         .           	   typename _RehashPolicy, typename _Traits>
         .               auto
24,190,103 ( 0.06%)      _Map_base<_Key, _Pair, _Alloc, _Select1st, _Equal,
         .           	      _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::
         .               operator[](key_type&& __k)
         .               -> mapped_type&
         .               {
         .                 __hashtable* __h = static_cast<__hashtable*>(this);
         .                 __hash_code __code = __h->_M_hash_code(__k);
         .                 std::size_t __bkt = __h->_M_bucket_index(__k, __code);
         .                 if (__node_type* __node = __h->_M_find_node(__bkt, __k, __code))
 3,455,729 ( 0.01%)  	return __node->_M_v().second;
         .           
         .                 typename __hashtable::_Scoped_node __node {
         .           	__h,
         .           	std::piecewise_construct,
         .           	std::forward_as_tuple(std::move(__k)),
         .           	std::tuple<>()
         .                 };
         .                 auto __pos
         .           	= __h->_M_insert_unique_node(__k, __bkt, __code, __node._M_node);
         .                 __node._M_node = nullptr;
         .                 return __pos->second;
20,734,374 ( 0.05%)      }
         .           
         .             template<typename _Key, typename _Pair, typename _Alloc, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash,
         .           	   typename _RehashPolicy, typename _Traits>
         .               auto
         .               _Map_base<_Key, _Pair, _Alloc, _Select1st, _Equal,
         .           	      _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::
         .               at(const key_type& __k)
-- line 757 ----------------------------------------
-- line 1383 ----------------------------------------
         .                 _M_bucket_index(const _Key&, __hash_code __c,
         .           		      std::size_t __bkt_count) const
         .                 { return _M_h2()(__c, __bkt_count); }
         .           
         .                 std::size_t
         .                 _M_bucket_index(const __node_type* __p, std::size_t __bkt_count) const
         .           	noexcept( noexcept(declval<const _H2&>()((__hash_code)0,
         .           						 (std::size_t)0)) )
   807,041 ( 0.00%)        { return _M_h2()(__p->_M_hash_code, __bkt_count); }
         .           
         .                 void
         .                 _M_store_code(__node_type* __n, __hash_code __c) const
 3,153,778 ( 0.01%)        { __n->_M_hash_code = __c; }
         .           
         .                 void
         .                 _M_copy_code(__node_type* __to, const __node_type* __from) const
         .                 { __to->_M_hash_code = __from->_M_hash_code; }
         .           
         .                 void
         .                 _M_swap(_Hash_code_base& __x)
         .                 {
-- line 1403 ----------------------------------------
-- line 1779 ----------------------------------------
         .                  { return true; }
         .                 };
         .           
         .               template<typename _Ptr2>
         .                 struct _Equal_hash_code<_Hash_node<_Ptr2, true>>
         .                 {
         .                  static bool
         .                  _S_equals(__hash_code __c, const _Hash_node<_Ptr2, true>& __n)
 8,509,042 ( 0.02%)         { return __c == __n._M_hash_code; }
         .                 };
         .           
         .             protected:
         .               _Hashtable_base() = default;
         .               _Hashtable_base(const _ExtractKey& __ex, const _H1& __h1, const _H2& __h2,
         .           		    const _Hash& __hash, const _Equal& __eq)
         .               : __hash_code_base(__ex, __h1, __h2, __hash), _EqualEBO(__eq)
         .               { }
-- line 1795 ----------------------------------------
-- line 1796 ----------------------------------------
         .           
         .               bool
         .               _M_equals(const _Key& __k, __hash_code __c, __node_type* __n) const
         .               {
         .                 static_assert(__is_invocable<const _Equal&, const _Key&, const _Key&>{},
         .           	  "key equality predicate must be invocable with two arguments of "
         .           	  "key type");
         .                 return _Equal_hash_code<__node_type>::_S_equals(__c, *__n)
 8,545,606 ( 0.02%)  	&& _M_eq()(__k, this->_M_extract()(__n->_M_v()));
         .               }
         .           
         .               void
         .               _M_swap(_Hashtable_base& __x)
         .               {
         .                 __hash_code_base::_M_swap(__x);
         .                 std::swap(_EqualEBO::_M_get(), __x._EqualEBO::_M_get());
         .               }
-- line 1812 ----------------------------------------
-- line 2063 ----------------------------------------
         .                 __n->~__node_type();
         .                 __node_alloc_traits::deallocate(_M_node_allocator(), __ptr, 1);
         .               }
         .           
         .             template<typename _NodeAlloc>
         .               void
         .               _Hashtable_alloc<_NodeAlloc>::_M_deallocate_nodes(__node_type* __n)
         .               {
 6,320,081 ( 0.02%)        while (__n)
         .           	{
         .           	  __node_type* __tmp = __n;
         .           	  __n = __n->_M_next();
         .           	  _M_deallocate_node(__tmp);
         .           	}
         .               }
         .           
         .             template<typename _NodeAlloc>
         .               typename _Hashtable_alloc<_NodeAlloc>::__bucket_type*
         .               _Hashtable_alloc<_NodeAlloc>::_M_allocate_buckets(std::size_t __bkt_count)
         .               {
         .                 __bucket_alloc_type __alloc(_M_node_allocator());
         .           
         .                 auto __ptr = __bucket_alloc_traits::allocate(__alloc, __bkt_count);
         .                 __bucket_type* __p = std::__to_address(__ptr);
       234 ( 0.00%)        __builtin_memset(__p, 0, __bkt_count * sizeof(__bucket_type));
   225,109 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (26x)
       873 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
         .                 return __p;
         .               }
         .           
         .             template<typename _NodeAlloc>
         .               void
         .               _Hashtable_alloc<_NodeAlloc>::_M_deallocate_buckets(__bucket_type* __bkts,
         .           							std::size_t __bkt_count)
         .               {
-- line 2095 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/predefined_ops.h
--------------------------------------------------------------------------------
Ir                     

-- line 35 ----------------------------------------
            .           namespace __ops
            .           {
            .             struct _Iter_less_iter
            .             {
            .               template<typename _Iterator1, typename _Iterator2>
            .                 _GLIBCXX14_CONSTEXPR
            .                 bool
            .                 operator()(_Iterator1 __it1, _Iterator2 __it2) const
   12,575,011 ( 0.03%)        { return *__it1 < *__it2; }
            .             };
            .           
            .             _GLIBCXX14_CONSTEXPR
            .             inline _Iter_less_iter
            .             __iter_less_iter()
            .             { return _Iter_less_iter(); }
            .           
            .             struct _Iter_less_val
-- line 51 ----------------------------------------
-- line 88 ----------------------------------------
            .               _GLIBCXX20_CONSTEXPR
            .               explicit
            .               _Val_less_iter(_Iter_less_iter) { }
            .           
            .               template<typename _Value, typename _Iterator>
            .                 _GLIBCXX20_CONSTEXPR
            .                 bool
            .                 operator()(_Value& __val, _Iterator __it) const
   18,862,512 ( 0.05%)        { return __val < *__it; }
            .             };
            .           
            .             _GLIBCXX20_CONSTEXPR
            .             inline _Val_less_iter
            .             __val_less_iter()
            .             { return _Val_less_iter(); }
            .           
            .             _GLIBCXX20_CONSTEXPR
-- line 104 ----------------------------------------
-- line 148 ----------------------------------------
            .                 _Iter_comp_iter(_Compare __comp)
            .           	: _M_comp(_GLIBCXX_MOVE(__comp))
            .                 { }
            .           
            .                 template<typename _Iterator1, typename _Iterator2>
            .                   _GLIBCXX14_CONSTEXPR
            .                   bool
            .                   operator()(_Iterator1 __it1, _Iterator2 __it2)
    2,218,589 ( 0.01%)          { return bool(_M_comp(*__it1, *__it2)); }
            .               };
            .           
            .             template<typename _Compare>
            .               _GLIBCXX14_CONSTEXPR
            .               inline _Iter_comp_iter<_Compare>
            .               __iter_comp_iter(_Compare __comp)
            .               { return _Iter_comp_iter<_Compare>(_GLIBCXX_MOVE(__comp)); }
            .           
-- line 164 ----------------------------------------
-- line 230 ----------------------------------------
            .           	: _M_comp(std::move(__comp._M_comp))
            .                 { }
            .           #endif
            .           
            .                 template<typename _Value, typename _Iterator>
            .           	_GLIBCXX20_CONSTEXPR
            .           	bool
            .           	operator()(_Value& __val, _Iterator __it)
       83,323 ( 0.00%)  	{ return bool(_M_comp(__val, *__it)); }
            .               };
            .           
            .             template<typename _Compare>
            .               _GLIBCXX20_CONSTEXPR
            .               inline _Val_comp_iter<_Compare>
            .               __val_comp_iter(_Compare __comp)
            .               { return _Val_comp_iter<_Compare>(_GLIBCXX_MOVE(__comp)); }
            .           
-- line 246 ----------------------------------------
-- line 308 ----------------------------------------
            .                 _Iter_pred(_Predicate __pred)
            .           	: _M_pred(_GLIBCXX_MOVE(__pred))
            .                 { }
            .           
            .                 template<typename _Iterator>
            .           	_GLIBCXX20_CONSTEXPR
            .           	bool
            .           	operator()(_Iterator __it)
4,633,846,428 (12.12%)  	{ return bool(_M_pred(*__it)); }
            .               };
            .           
            .             template<typename _Predicate>
            .               _GLIBCXX20_CONSTEXPR
            .               inline _Iter_pred<_Predicate>
            .               __pred_iter(_Predicate __pred)
            .               { return _Iter_pred<_Predicate>(_GLIBCXX_MOVE(__pred)); }
            .           
-- line 324 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_iterator.h
--------------------------------------------------------------------------------
Ir                  

-- line 970 ----------------------------------------
         .           #if __cplusplus > 201703L && __cpp_lib_concepts
         .                 using iterator_concept = std::__detail::__iter_concept<_Iterator>;
         .           #endif
         .           
         .                 _GLIBCXX_CONSTEXPR __normal_iterator() _GLIBCXX_NOEXCEPT
         .                 : _M_current(_Iterator()) { }
         .           
         .                 explicit _GLIBCXX20_CONSTEXPR
        30 ( 0.00%)        __normal_iterator(const _Iterator& __i) _GLIBCXX_NOEXCEPT
84,411,648 ( 0.22%)        : _M_current(__i) { }
         .           
         .                 // Allow iterator to const_iterator conversion
         .                 template<typename _Iter>
         .                   _GLIBCXX20_CONSTEXPR
         .                   __normal_iterator(const __normal_iterator<_Iter,
         .           			  typename __enable_if<
         .                 	       (std::__are_same<_Iter, typename _Container::pointer>::__value),
         .           		      _Container>::__type>& __i) _GLIBCXX_NOEXCEPT
-- line 987 ----------------------------------------
-- line 997 ----------------------------------------
         .                 pointer
         .                 operator->() const _GLIBCXX_NOEXCEPT
         .                 { return _M_current; }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator&
         .                 operator++() _GLIBCXX_NOEXCEPT
         .                 {
14,669,056 ( 0.04%)  	++_M_current;
 1,666,363 ( 0.00%)  	return *this;
         .                 }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator
         .                 operator++(int) _GLIBCXX_NOEXCEPT
20,491,351 ( 0.05%)        { return __normal_iterator(_M_current++); }
         .           
         .                 // Bidirectional iterator requirements
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator&
         5 ( 0.00%)        operator--() _GLIBCXX_NOEXCEPT
         .                 {
 3,151,308 ( 0.01%)  	--_M_current;
         .           	return *this;
         .                 }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator
         .                 operator--(int) _GLIBCXX_NOEXCEPT
         .                 { return __normal_iterator(_M_current--); }
         .           
-- line 1027 ----------------------------------------
-- line 1029 ----------------------------------------
         .                 _GLIBCXX20_CONSTEXPR
         .                 reference
         .                 operator[](difference_type __n) const _GLIBCXX_NOEXCEPT
         .                 { return _M_current[__n]; }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator&
         .                 operator+=(difference_type __n) _GLIBCXX_NOEXCEPT
    97,698 ( 0.00%)        { _M_current += __n; return *this; }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator
         .                 operator+(difference_type __n) const _GLIBCXX_NOEXCEPT
 6,320,684 ( 0.02%)        { return __normal_iterator(_M_current + __n); }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator&
         .                 operator-=(difference_type __n) _GLIBCXX_NOEXCEPT
         .                 { _M_current -= __n; return *this; }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 __normal_iterator
-- line 1050 ----------------------------------------
-- line 1105 ----------------------------------------
         .               operator!=(const __normal_iterator<_IteratorL, _Container>& __lhs,
         .           	       const __normal_iterator<_IteratorR, _Container>& __rhs)
         .               _GLIBCXX_NOEXCEPT
         .               { return __lhs.base() != __rhs.base(); }
         .           
         .             template<typename _Iterator, typename _Container>
         .               _GLIBCXX20_CONSTEXPR
         .               inline bool
         1 ( 0.00%)      operator!=(const __normal_iterator<_Iterator, _Container>& __lhs,
         .           	       const __normal_iterator<_Iterator, _Container>& __rhs)
         .               _GLIBCXX_NOEXCEPT
 3,143,750 ( 0.01%)      { return __lhs.base() != __rhs.base(); }
         .           
         .             // Random access iterator requirements
         .             template<typename _IteratorL, typename _IteratorR, typename _Container>
         .               inline bool
         .               operator<(const __normal_iterator<_IteratorL, _Container>& __lhs,
         .           	      const __normal_iterator<_IteratorR, _Container>& __rhs)
         .               _GLIBCXX_NOEXCEPT
         .               { return __lhs.base() < __rhs.base(); }
-- line 1124 ----------------------------------------
-- line 1189 ----------------------------------------
         .               operator-(const __normal_iterator<_IteratorL, _Container>& __lhs,
         .           	      const __normal_iterator<_IteratorR, _Container>& __rhs) noexcept
         .               -> decltype(__lhs.base() - __rhs.base())
         .           #else
         .               inline typename __normal_iterator<_IteratorL, _Container>::difference_type
         .               operator-(const __normal_iterator<_IteratorL, _Container>& __lhs,
         .           	      const __normal_iterator<_IteratorR, _Container>& __rhs)
         .           #endif
    19,992 ( 0.00%)      { return __lhs.base() - __rhs.base(); }
         .           
         .             template<typename _Iterator, typename _Container>
         .               _GLIBCXX20_CONSTEXPR
         .               inline typename __normal_iterator<_Iterator, _Container>::difference_type
         .               operator-(const __normal_iterator<_Iterator, _Container>& __lhs,
         .           	      const __normal_iterator<_Iterator, _Container>& __rhs)
         .               _GLIBCXX_NOEXCEPT
48,864,860 ( 0.13%)      { return __lhs.base() - __rhs.base(); }
         .           
         .             template<typename _Iterator, typename _Container>
         .               _GLIBCXX20_CONSTEXPR
         .               inline __normal_iterator<_Iterator, _Container>
         .               operator+(typename __normal_iterator<_Iterator, _Container>::difference_type
         .           	      __n, const __normal_iterator<_Iterator, _Container>& __i)
         .               _GLIBCXX_NOEXCEPT
         .               { return __normal_iterator<_Iterator, _Container>(__i.base() + __n); }
-- line 1213 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_vector.h
--------------------------------------------------------------------------------
Ir                   

-- line 90 ----------------------------------------
          .           
          .                 struct _Vector_impl_data
          .                 {
          .           	pointer _M_start;
          .           	pointer _M_finish;
          .           	pointer _M_end_of_storage;
          .           
          .           	_Vector_impl_data() _GLIBCXX_NOEXCEPT
 22,047,103 ( 0.06%)  	: _M_start(), _M_finish(), _M_end_of_storage()
          .           	{ }
          .           
          .           #if __cplusplus >= 201103L
          .           	_Vector_impl_data(_Vector_impl_data&& __x) noexcept
          3 ( 0.00%)  	: _M_start(__x._M_start), _M_finish(__x._M_finish),
         11 ( 0.00%)  	  _M_end_of_storage(__x._M_end_of_storage)
          6 ( 0.00%)  	{ __x._M_start = __x._M_finish = __x._M_end_of_storage = pointer(); }
          .           #endif
          .           
          .           	void
          .           	_M_copy_data(_Vector_impl_data const& __x) _GLIBCXX_NOEXCEPT
          .           	{
          .           	  _M_start = __x._M_start;
          .           	  _M_finish = __x._M_finish;
          .           	  _M_end_of_storage = __x._M_end_of_storage;
-- line 113 ----------------------------------------
-- line 328 ----------------------------------------
          .                 _Vector_base(const allocator_type& __a, _Vector_base&& __x)
          .                 : _M_impl(_Tp_alloc_type(__a), std::move(__x._M_impl))
          .                 { }
          .           #endif
          .           
          .                 ~_Vector_base() _GLIBCXX_NOEXCEPT
          .                 {
          .           	_M_deallocate(_M_impl._M_start,
 15,739,473 ( 0.04%)  		      _M_impl._M_end_of_storage - _M_impl._M_start);
          .                 }
          .           
          .               public:
          .                 _Vector_impl _M_impl;
          .           
          .                 pointer
 22,006,272 ( 0.06%)        _M_allocate(size_t __n)
          .                 {
          .           	typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;
 59,847,564 ( 0.16%)  	return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();
          .                 }
          .           
          .                 void
          .                 _M_deallocate(pointer __p, size_t __n)
          .                 {
          .           	typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;
 25,237,036 ( 0.07%)  	if (__p)
          .           	  _Tr::deallocate(_M_impl, __p, __n);
          .                 }
          .           
          .               protected:
          .                 void
          .                 _M_create_storage(size_t __n)
          .                 {
         67 ( 0.00%)  	this->_M_impl._M_start = this->_M_allocate(__n);
          .           	this->_M_impl._M_finish = this->_M_impl._M_start;
         50 ( 0.00%)  	this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;
          .                 }
          .               };
          .           
          .             /**
          .              *  @brief A standard container which offers fixed time access to
          .              *  individual elements in any order.
          .              *
          .              *  @ingroup sequences
-- line 371 ----------------------------------------
-- line 502 ----------------------------------------
          .                  *  @brief  Creates a %vector with default constructed elements.
          .                  *  @param  __n  The number of elements to initially create.
          .                  *  @param  __a  An allocator.
          .                  *
          .                  *  This constructor fills the %vector with @a __n default
          .                  *  constructed elements.
          .                  */
          .                 explicit
          9 ( 0.00%)        vector(size_type __n, const allocator_type& __a = allocator_type())
          .                 : _Base(_S_check_init_len(__n, __a), __a)
         12 ( 0.00%)        { _M_default_initialize(__n); }
          .           
          .                 /**
          .                  *  @brief  Creates a %vector with copies of an exemplar element.
          .                  *  @param  __n  The number of elements to initially create.
          .                  *  @param  __value  An element to copy.
          .                  *  @param  __a  An allocator.
          .                  *
          .                  *  This constructor fills the %vector with @a __n copies of @a __value.
-- line 520 ----------------------------------------
-- line 545 ----------------------------------------
          .                  *
          .                  *  All the elements of @a __x are copied, but any unused capacity in
          .                  *  @a __x  will not be copied
          .                  *  (i.e. capacity() == size() in the new %vector).
          .                  *
          .                  *  The newly-created %vector uses a copy of the allocator object used
          .                  *  by @a __x (unless the allocator traits dictate a different object).
          .                  */
         65 ( 0.00%)        vector(const vector& __x)
          .                 : _Base(__x.size(),
          .           	_Alloc_traits::_S_select_on_copy(__x._M_get_Tp_allocator()))
          .                 {
          8 ( 0.00%)  	this->_M_impl._M_finish =
          .           	  std::__uninitialized_copy_a(__x.begin(), __x.end(),
          .           				      this->_M_impl._M_start,
          .           				      _M_get_Tp_allocator());
         52 ( 0.00%)        }
          .           
          .           #if __cplusplus >= 201103L
          .                 /**
          .                  *  @brief  %Vector move constructor.
          .                  *
          .                  *  The newly-created %vector contains the exact contents of the
          .                  *  moved instance.
          .                  *  The contents of the moved instance are a valid, but unspecified
-- line 569 ----------------------------------------
-- line 645 ----------------------------------------
          .                  *  constructor N times (where N is distance(first,last)) and do
          .                  *  no memory reallocation.  But if only input iterators are
          .                  *  used, then this will do at most 2N calls to the copy
          .                  *  constructor, and logN memory reallocations.
          .                  */
          .           #if __cplusplus >= 201103L
          .                 template<typename _InputIterator,
          .           	       typename = std::_RequireInputIter<_InputIterator>>
         16 ( 0.00%)  	vector(_InputIterator __first, _InputIterator __last,
          .           	       const allocator_type& __a = allocator_type())
          .           	: _Base(__a)
          .           	{
          .           	  _M_range_initialize(__first, __last,
          .           			      std::__iterator_category(__first));
         16 ( 0.00%)  	}
          .           #else
          .                 template<typename _InputIterator>
          .           	vector(_InputIterator __first, _InputIterator __last,
          .           	       const allocator_type& __a = allocator_type())
          .           	: _Base(__a)
          .           	{
          .           	  // Check whether it's an integral type.  If so, it's not an iterator.
          .           	  typedef typename std::__is_integer<_InputIterator>::__type _Integral;
-- line 667 ----------------------------------------
-- line 670 ----------------------------------------
          .           #endif
          .           
          .                 /**
          .                  *  The dtor only erases the elements, and note that if the
          .                  *  elements themselves are pointers, the pointed-to memory is
          .                  *  not touched in any way.  Managing the pointer is the user's
          .                  *  responsibility.
          .                  */
         24 ( 0.00%)        ~vector() _GLIBCXX_NOEXCEPT
          .                 {
  3,161,655 ( 0.01%)  	std::_Destroy(this->_M_impl._M_start, this->_M_impl._M_finish,
          .           		      _M_get_Tp_allocator());
          .           	_GLIBCXX_ASAN_ANNOTATE_BEFORE_DEALLOC;
         19 ( 0.00%)        }
          .           
          .                 /**
          .                  *  @brief  %Vector assignment operator.
          .                  *  @param  __x  A %vector of identical element and allocator types.
          .                  *
          .                  *  All the elements of @a __x are copied, but any unused capacity in
          .                  *  @a __x will not be copied.
          .                  *
-- line 691 ----------------------------------------
-- line 910 ----------------------------------------
          .                 const_reverse_iterator
          .                 crend() const noexcept
          .                 { return const_reverse_iterator(begin()); }
          .           #endif
          .           
          .                 // [23.2.4.2] capacity
          .                 /**  Returns the number of elements in the %vector.  */
          .                 size_type
     20,603 ( 0.00%)        size() const _GLIBCXX_NOEXCEPT
178,420,162 ( 0.47%)        { return size_type(this->_M_impl._M_finish - this->_M_impl._M_start); }
          .           
          .                 /**  Returns the size() of the largest possible %vector.  */
          .                 size_type
          .                 max_size() const _GLIBCXX_NOEXCEPT
          .                 { return _S_max_size(_M_get_Tp_allocator()); }
          .           
          .           #if __cplusplus >= 201103L
          .                 /**
-- line 927 ----------------------------------------
-- line 1038 ----------------------------------------
          .                  *  Note that data access with this operator is unchecked and
          .                  *  out_of_range lookups are not defined. (For checked lookups
          .                  *  see at().)
          .                  */
          .                 reference
          .                 operator[](size_type __n) _GLIBCXX_NOEXCEPT
          .                 {
          .           	__glibcxx_requires_subscript(__n);
 13,442,366 ( 0.04%)  	return *(this->_M_impl._M_start + __n);
          .                 }
          .           
          .                 /**
          .                  *  @brief  Subscript access to the data contained in the %vector.
          .                  *  @param __n The index of the element for which data should be
          .                  *  accessed.
          .                  *  @return  Read-only (constant) reference to data.
          .                  *
          .                  *  This operator allows for easy, array-style, data access.
          .                  *  Note that data access with this operator is unchecked and
          .                  *  out_of_range lookups are not defined. (For checked lookups
          .                  *  see at().)
          .                  */
          .                 const_reference
          4 ( 0.00%)        operator[](size_type __n) const _GLIBCXX_NOEXCEPT
          .                 {
          .           	__glibcxx_requires_subscript(__n);
  4,469,775 ( 0.01%)  	return *(this->_M_impl._M_start + __n);
          .                 }
          .           
          .               protected:
          .                 /// Safety check used only from at().
          .                 void
          .                 _M_range_check(size_type __n) const
          .                 {
          .           	if (__n >= this->size())
-- line 1072 ----------------------------------------
-- line 1181 ----------------------------------------
          .                  *  element at the end of the %vector and assigns the given data
          .                  *  to it.  Due to the nature of a %vector this operation can be
          .                  *  done in constant time if the %vector has preallocated space
          .                  *  available.
          .                  */
          .                 void
          .                 push_back(const value_type& __x)
          .                 {
  6,796,654 ( 0.02%)  	if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
          .           	  {
          .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
          .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
          .           				     __x);
 10,979,553 ( 0.03%)  	    ++this->_M_impl._M_finish;
          .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
          .           	  }
          .           	else
    108,676 ( 0.00%)  	  _M_realloc_insert(end(), __x);
      1,670 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::_M_realloc_insert<long long const&>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long long const&) (6x)
          .                 }
          .           
          .           #if __cplusplus >= 201103L
          .                 void
          .                 push_back(value_type&& __x)
      7,560 ( 0.00%)        { emplace_back(std::move(__x)); }
     53,164 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<unsigned long, std::allocator<unsigned long> >::emplace_back<unsigned long>(unsigned long&&) (2,507x)
          .           
          .                 template<typename... _Args>
          .           #if __cplusplus > 201402L
          .           	reference
          .           #else
          .           	void
          .           #endif
          .           	emplace_back(_Args&&... __args);
-- line 1212 ----------------------------------------
-- line 1574 ----------------------------------------
          .                 // Called by the second initialize_dispatch above
          .                 template<typename _ForwardIterator>
          .           	void
          .           	_M_range_initialize(_ForwardIterator __first, _ForwardIterator __last,
          .           			    std::forward_iterator_tag)
          .           	{
          .           	  const size_type __n = std::distance(__first, __last);
          .           	  this->_M_impl._M_start
         19 ( 0.00%)  	    = this->_M_allocate(_S_check_init_len(__n, _M_get_Tp_allocator()));
         38 ( 0.00%)  	  this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;
         18 ( 0.00%)  	  this->_M_impl._M_finish =
          .           	    std::__uninitialized_copy_a(__first, __last,
          .           					this->_M_impl._M_start,
          .           					_M_get_Tp_allocator());
          .           	}
          .           
          .                 // Called by the first initialize_dispatch above and by the
          .                 // vector(n,value,a) constructor.
          .                 void
          .                 _M_fill_initialize(size_type __n, const value_type& __value)
          .                 {
          1 ( 0.00%)  	this->_M_impl._M_finish =
          .           	  std::__uninitialized_fill_n_a(this->_M_impl._M_start, __n, __value,
          .           					_M_get_Tp_allocator());
          .                 }
          .           
          .           #if __cplusplus >= 201103L
          .                 // Called by the vector(n) constructor.
          .                 void
          .                 _M_default_initialize(size_type __n)
          .                 {
          3 ( 0.00%)  	this->_M_impl._M_finish =
          .           	  std::__uninitialized_default_n_a(this->_M_impl._M_start, __n,
          .           					   _M_get_Tp_allocator());
          .                 }
          .           #endif
          .           
          .                 // Internal assign functions follow.  The *_aux functions do the actual
          .                 // assignment work for the range versions.
          .           
-- line 1613 ----------------------------------------
-- line 1750 ----------------------------------------
          .                 _M_emplace_aux(const_iterator __position, value_type&& __v)
          .                 { return _M_insert_rval(__position, std::move(__v)); }
          .           #endif
          .           
          .                 // Called by _M_fill_insert, _M_insert_aux etc.
          .                 size_type
          .                 _M_check_len(size_type __n, const char* __s) const
          .                 {
170,381,112 ( 0.45%)  	if (max_size() - size() < __n)
          .           	  __throw_length_error(__N(__s));
          .           
          .           	const size_type __len = size() + (std::max)(size(), __n);
    116,174 ( 0.00%)  	return (__len < size() || __len > max_size()) ? max_size() : __len;
          .                 }
          .           
          .                 // Called by constructors to check initial size.
          .                 static size_type
          .                 _S_check_init_len(size_type __n, const allocator_type& __a)
          .                 {
         21 ( 0.00%)  	if (__n > _S_max_size(_Tp_alloc_type(__a)))
          .           	  __throw_length_error(
          .           	      __N("cannot create std::vector larger than max_size()"));
          .           	return __n;
          .                 }
          .           
          .                 static size_type
          .                 _S_max_size(const _Tp_alloc_type& __a) _GLIBCXX_NOEXCEPT
          .                 {
-- line 1777 ----------------------------------------
-- line 1790 ----------------------------------------
          .                 // _M_assign_aux.
          .                 void
          .                 _M_erase_at_end(pointer __pos) _GLIBCXX_NOEXCEPT
          .                 {
          .           	if (size_type __n = this->_M_impl._M_finish - __pos)
          .           	  {
          .           	    std::_Destroy(__pos, this->_M_impl._M_finish,
          .           			  _M_get_Tp_allocator());
         22 ( 0.00%)  	    this->_M_impl._M_finish = __pos;
          .           	    _GLIBCXX_ASAN_ANNOTATE_SHRINK(__n);
          .           	  }
          .                 }
          .           
          .                 iterator
          .                 _M_erase(iterator __position);
          .           
          .                 iterator
-- line 1806 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 34 ----------------------------------------
          .               std::atomic<int> allocatedCount; // the number of objects allocated
          .               BackRefIdx::main_t myNum;   // the index in the main
          .               MallocMutex   blockMutex;
          .               // true if this block has been added to the listForUse chain,
          .               // modifications protected by mainMutex
          .               std::atomic<bool> addedToForUse;
          .           
          .               BackRefBlock(const BackRefBlock *blockToUse, intptr_t num) :
          8 ( 0.00%)          nextForUse(nullptr), bumpPtr((FreeObject*)((uintptr_t)blockToUse + slabSize - sizeof(void*))),
          .                   freeList(nullptr), nextRawMemBlock(nullptr), allocatedCount(0), myNum(num),
         16 ( 0.00%)          addedToForUse(false) {
          4 ( 0.00%)          memset(&blockMutex, 0, sizeof(MallocMutex));
          .           
          .                   MALLOC_ASSERT(!(num >> CHAR_BIT*sizeof(BackRefIdx::main_t)),
          .                                 "index in BackRefMain must fit to BackRefIdx::main");
          .               }
          .               // clean all but header
         24 ( 0.00%)      void zeroSet() { memset(this+1, 0, BackRefBlock::bytes-sizeof(BackRefBlock)); }
     49,008 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (3x)
     17,195 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .               static const int bytes = slabSize;
          .           };
          .           
          .           // max number of backreference pointers in slab block
          .           static const int BR_MAX_CNT = (BackRefBlock::bytes-sizeof(BackRefBlock))/sizeof(void*);
          .           
          .           struct BackRefMain {
          .           /* On 64-bit systems a slab block can hold up to ~2K back pointers to slab blocks
-- line 59 ----------------------------------------
-- line 92 ----------------------------------------
          .           
          .           static MallocMutex mainMutex;
          .           static std::atomic<BackRefMain*> backRefMain;
          .           
          .           bool initBackRefMain(Backend *backend)
          .           {
          .               bool rawMemUsed;
          .               BackRefMain *main =
          6 ( 0.00%)          (BackRefMain*)backend->getBackRefSpace(BackRefMain::mainSize,
      1,817 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::getBackRefSpace(unsigned long, bool*) (1x)
          .                                                            &rawMemUsed);
          2 ( 0.00%)      if (! main)
          .                   return false;
          1 ( 0.00%)      main->backend = backend;
          .               main->listForUse.store(nullptr, std::memory_order_relaxed);
          1 ( 0.00%)      main->allRawMemBlocks = nullptr;
          2 ( 0.00%)      main->rawMemUsed = rawMemUsed;
          .               main->lastUsed = -1;
          1 ( 0.00%)      memset(&main->requestNewSpaceMutex, 0, sizeof(MallocMutex));
         19 ( 0.00%)      for (int i=0; i<BackRefMain::leaves; i++) {
          .                   BackRefBlock *bl = (BackRefBlock*)((uintptr_t)main + BackRefMain::bytes + i*BackRefBlock::bytes);
          .                   bl->zeroSet();
         12 ( 0.00%)          main->initEmptyBackRefBlock(bl);
         56 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefMain::initEmptyBackRefBlock(rml::internal::BackRefBlock*) (4x)
          8 ( 0.00%)          if (i)
          .                       main->addToForUseList(bl);
          .                   else // active leaf is not needed in listForUse
          .                       main->active.store(bl, std::memory_order_relaxed);
          .               }
          .               // backRefMain is read in getBackRef, so publish it in consistent state
          .               backRefMain.store(main, std::memory_order_release);
          .               return true;
          .           }
-- line 122 ----------------------------------------
-- line 135 ----------------------------------------
          .                   backend->putBackRefSpace(backRefMain.load(std::memory_order_relaxed), BackRefMain::mainSize,
          .                                            backRefMain.load(std::memory_order_relaxed)->rawMemUsed);
          .               }
          .           }
          .           #endif
          .           
          .           void BackRefMain::addToForUseList(BackRefBlock *bl)
          .           {
          3 ( 0.00%)      bl->nextForUse = listForUse.load(std::memory_order_relaxed);
          .               listForUse.store(bl, std::memory_order_relaxed);
          .               bl->addedToForUse.store(true, std::memory_order_relaxed);
          .           }
          .           
          .           void BackRefMain::initEmptyBackRefBlock(BackRefBlock *newBl)
          .           {
          4 ( 0.00%)      intptr_t nextLU = lastUsed+1;
          .               new (newBl) BackRefBlock(newBl, nextLU);
          .               MALLOC_ASSERT(nextLU < dataSz, nullptr);
          4 ( 0.00%)      backRefBl[nextLU] = newBl;
          .               // lastUsed is read in getBackRef, and access to backRefBl[lastUsed]
          .               // is possible only after checking backref against current lastUsed
          .               lastUsed.store(nextLU, std::memory_order_release);
          4 ( 0.00%)  }
          .           
          .           bool BackRefMain::requestNewSpace()
          .           {
          .               bool isRawMemUsed;
          .               static_assert(!(blockSpaceSize % BackRefBlock::bytes),
          .                                    "Must request space for whole number of blocks.");
          .           
          .               if (BackRefMain::dataSz <= lastUsed + 1) // no space in main
-- line 165 ----------------------------------------
-- line 207 ----------------------------------------
          .               return true;
          .           }
          .           
          .           BackRefBlock *BackRefMain::findFreeBlock()
          .           {
          .               BackRefBlock* active_block = active.load(std::memory_order_acquire);
          .               MALLOC_ASSERT(active_block, ASSERT_TEXT);
          .           
        446 ( 0.00%)      if (active_block->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT)
          .                   return active_block;
          .           
          .               if (listForUse.load(std::memory_order_relaxed)) { // use released list
          .                   MallocMutex::scoped_lock lock(mainMutex);
          .           
          .                   if (active_block->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT) {
          .                       active_block = listForUse.load(std::memory_order_relaxed);
          .                       if (active_block) {
-- line 223 ----------------------------------------
-- line 233 ----------------------------------------
          .               return active.load(std::memory_order_acquire); // reread because of requestNewSpace
          .           }
          .           
          .           void *getBackRef(BackRefIdx backRefIdx)
          .           {
          .               // !backRefMain means no initialization done, so it can't be valid memory
          .               // see addEmptyBackRefBlock for fences around lastUsed
          .               if (!(backRefMain.load(std::memory_order_acquire))
 25,228,615 ( 0.07%)          || backRefIdx.getMain() > (backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_acquire))
151,371,690 ( 0.40%)          || backRefIdx.getOffset() >= BR_MAX_CNT)
          .               {
        256 ( 0.00%)          return nullptr;
          .               }
          .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)(
          .                       (uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
 25,228,487 ( 0.07%)              + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(std::atomic<void*>)
          .                   );
          .               return backRefEntry.load(std::memory_order_relaxed);
 25,228,487 ( 0.07%)  }
          .           
          .           void setBackRef(BackRefIdx backRefIdx, void *newPtr)
          .           {
          .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
          .                                            && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
        363 ( 0.00%)      ((std::atomic<void*>*)((uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
      3,095 ( 0.00%)          + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(void*)))->store(newPtr, std::memory_order_relaxed);
          .           }
          .           
          .           BackRefIdx BackRefIdx::newBackRef(bool largeObj)
      2,007 ( 0.00%)  {
          .               BackRefBlock *blockToUse;
          .               void **toUse;
          .               BackRefIdx res;
          .               bool lastBlockFirstUsed = false;
          .           
          .               do {
          .                   MALLOC_ASSERT(backRefMain.load(std::memory_order_relaxed), ASSERT_TEXT);
          .                   blockToUse = backRefMain.load(std::memory_order_relaxed)->findFreeBlock();
        446 ( 0.00%)          if (!blockToUse)
          .                       return BackRefIdx();
          .                   toUse = nullptr;
          .                   { // the block is locked to find a reference
        223 ( 0.00%)              MallocMutex::scoped_lock lock(blockToUse->blockMutex);
          .           
        669 ( 0.00%)              if (blockToUse->freeList) {
          .                           toUse = (void**)blockToUse->freeList;
          .                           blockToUse->freeList = blockToUse->freeList->next;
          .                           MALLOC_ASSERT(!blockToUse->freeList ||
          .                                         ((uintptr_t)blockToUse->freeList>=(uintptr_t)blockToUse
          .                                          && (uintptr_t)blockToUse->freeList <
          .                                          (uintptr_t)blockToUse + slabSize), ASSERT_TEXT);
        446 ( 0.00%)              } else if (blockToUse->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT) {
        223 ( 0.00%)                  toUse = (void**)blockToUse->bumpPtr;
          .                           blockToUse->bumpPtr =
        446 ( 0.00%)                      (FreeObject*)((uintptr_t)blockToUse->bumpPtr - sizeof(void*));
          .                           if (blockToUse->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT-1) {
          .                               MALLOC_ASSERT((uintptr_t)blockToUse->bumpPtr
          .                                             < (uintptr_t)blockToUse+sizeof(BackRefBlock),
          .                                             ASSERT_TEXT);
        892 ( 0.00%)                      blockToUse->bumpPtr = nullptr;
          .                           }
          .                       }
        669 ( 0.00%)              if (toUse) {
        446 ( 0.00%)                  if (!blockToUse->allocatedCount.load(std::memory_order_relaxed) &&
          .                               !backRefMain.load(std::memory_order_relaxed)->listForUse.load(std::memory_order_relaxed)) {
          .                               lastBlockFirstUsed = true;
          .                           }
        223 ( 0.00%)                  blockToUse->allocatedCount.store(blockToUse->allocatedCount.load(std::memory_order_relaxed) + 1, std::memory_order_relaxed);
          .                       }
          .                   } // end of lock scope
          .               } while (!toUse);
          .               // The first thread that uses the last block requests new space in advance;
          .               // possible failures are ignored.
          2 ( 0.00%)      if (lastBlockFirstUsed)
          .                   backRefMain.load(std::memory_order_relaxed)->requestNewSpace();
          .           
          .               res.main = blockToUse->myNum;
        223 ( 0.00%)      uintptr_t offset =
        446 ( 0.00%)          ((uintptr_t)toUse - ((uintptr_t)blockToUse + sizeof(BackRefBlock)))/sizeof(void*);
          .               // Is offset too big?
          .               MALLOC_ASSERT(!(offset >> 15), ASSERT_TEXT);
          .               res.offset = offset;
          .               if (largeObj) res.largeObj = largeObj;
          .           
      1,561 ( 0.00%)      return res;
      1,784 ( 0.00%)  }
          .           
          .           void removeBackRef(BackRefIdx backRefIdx)
        935 ( 0.00%)  {
          .               MALLOC_ASSERT(!backRefIdx.isInvalid(), ASSERT_TEXT);
          .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
          .                             && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
        374 ( 0.00%)      BackRefBlock *currBlock = backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()];
          .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)((uintptr_t)currBlock + sizeof(BackRefBlock)
        561 ( 0.00%)                                          + backRefIdx.getOffset()*sizeof(std::atomic<void*>));
          .               MALLOC_ASSERT(((uintptr_t)&backRefEntry >(uintptr_t)currBlock &&
          .                              (uintptr_t)&backRefEntry <(uintptr_t)currBlock + slabSize), ASSERT_TEXT);
          .               {
        187 ( 0.00%)          MallocMutex::scoped_lock lock(currBlock->blockMutex);
          .           
          .                   backRefEntry.store(currBlock->freeList, std::memory_order_relaxed);
          .           #if MALLOC_DEBUG
          .                   uintptr_t backRefEntryValue = (uintptr_t)backRefEntry.load(std::memory_order_relaxed);
          .                   MALLOC_ASSERT(!backRefEntryValue ||
          .                                 (backRefEntryValue > (uintptr_t)currBlock
          .                                  && backRefEntryValue < (uintptr_t)currBlock + slabSize), ASSERT_TEXT);
          .           #endif
        187 ( 0.00%)          currBlock->freeList = (FreeObject*)&backRefEntry;
        187 ( 0.00%)          currBlock->allocatedCount.store(currBlock->allocatedCount.load(std::memory_order_relaxed)-1, std::memory_order_relaxed);
          .               }
          .               // TODO: do we need double-check here?
        748 ( 0.00%)      if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
          .                   currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed)) {
          .                   MallocMutex::scoped_lock lock(mainMutex);
          .           
          .                   if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
          .                       currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed))
          .                       backRefMain.load(std::memory_order_relaxed)->addToForUseList(currBlock);
          .               }
      1,122 ( 0.00%)  }
          .           
          .           /********* End of backreferences ***********************/
          .           
          .           } // namespace internal
          .           } // namespace rml
          .           

--------------------------------------------------------------------------------
-- Auto-annotated source: include/jobs.hpp
--------------------------------------------------------------------------------
Ir                      

-- line 13 ----------------------------------------
             .           
             .           	typedef std::size_t hash_value_t;
             .           
             .           	struct JobID {
             .           		unsigned long job;
             .           		unsigned long task;
             .           
             .           		JobID(unsigned long j_id, unsigned long t_id)
         5,014 ( 0.00%)  		: job(j_id), task(t_id)
             .           		{
             .           		}
             .           
             .           		bool operator==(const JobID& other) const
             .           		{
10,930,688,706 (28.59%)  			return this->task == other.task && this->job == other.job;
             .           		}
             .           
             .           		friend std::ostream& operator<< (std::ostream& stream, const JobID& id)
             .           		{
             .           			stream << "T" << id.task << "J" << id.job;
             .           			return stream;
             .           		}
             .           	};
             .           
       175,724 ( 0.00%)  	template<class Time> class Job {
             .           
             .           	public:
             .           		typedef std::vector<Job<Time>> Job_set;
             .           		typedef Time Priority; // Make it a time value to support EDF
             .           
             .           	private:
             .           		Interval<Time> arrival;
             .           		Interval<Time> cost;
-- line 45 ----------------------------------------
-- line 46 ----------------------------------------
             .           		Time deadline;
             .           		Priority priority;
             .           		JobID id;
             .           		hash_value_t key;
             .           
             .           		void compute_hash() {
             .           			auto h = std::hash<Time>{};
             .           			key = h(arrival.from());
         5,014 ( 0.00%)  			key = (key << 4) ^ h(id.task);
         5,014 ( 0.00%)  			key = (key << 4) ^ h(arrival.until());
         5,014 ( 0.00%)  			key = (key << 4) ^ h(cost.from());
         5,014 ( 0.00%)  			key = (key << 4) ^ h(deadline);
         5,014 ( 0.00%)  			key = (key << 4) ^ h(cost.upto());
         5,014 ( 0.00%)  			key = (key << 4) ^ h(id.job);
         7,521 ( 0.00%)  			key = (key << 4) ^ h(priority);
             .           		}
             .           
             .           	public:
             .           
             .           		Job(unsigned long id,
             .           			Interval<Time> arr, Interval<Time> cost,
             .           			Time dl, Priority prio,
             .           			unsigned long tid = 0)
             .           		: arrival(arr), cost(cost),
         7,521 ( 0.00%)  		  deadline(dl), priority(prio), id(id, tid)
             .           		{
             .           			compute_hash();
             .           		}
             .           
             .           		hash_value_t get_key() const
             .           		{
             8 ( 0.00%)  			return key;
             .           		}
             .           
             .           		Time earliest_arrival() const
             .           		{
    13,345,685 ( 0.03%)  			return arrival.from();
             .           		}
             .           
             .           		Time latest_arrival() const
             .           		{
    30,719,984 ( 0.08%)  			return arrival.until();
             .           		}
             .           
             .                   void set_arrival(Interval<Time> arr)
             .                   {
        10,028 ( 0.00%)              arrival = arr;
             .                   }
             .           
             .           		const Interval<Time>& arrival_window() const
             .           		{
             .           			return arrival;
             .           		}
             .           
             .           		Time least_cost() const
             .           		{
             .           			return cost.from();
             .           		}
             .           
             .           		//for clarity keep the notation similar... so minimal and maximal
             .           		Time minimal_cost() const
             .           		{
     3,151,256 ( 0.01%)  			return cost.from();
             .           		}
             .           
             .           		Time maximal_cost() const
             .           		{
    13,522,824 ( 0.04%)  			return cost.upto();
             .           		}
             .           
             .           		const Interval<Time>& get_cost() const
             .           		{
             .           			return cost;
             .           		}
             .           
             .           		Priority get_priority() const
             .           		{
         5,014 ( 0.00%)  			return priority;
             .           		}
             .           
        15,042 ( 0.00%)  		Time get_deadline() const
             .           		{
         2,507 ( 0.00%)  			return deadline;
             .           		}
             .           
             .           		bool exceeds_deadline(Time t) const
             .           		{
             .           			return t > deadline
     6,295,021 ( 0.02%)  			       && (t - deadline) >
             .           			          Time_model::constants<Time>::deadline_miss_tolerance();
             .           		}
             .           
             .           		JobID get_id() const
             .           		{
    13,350,298 ( 0.03%)  			return id;
             .           		}
             .           
             .           		unsigned long get_job_id() const
             .           		{
             .           			return id.job;
             .           		}
             .           
             .           		unsigned long get_task_id() const
-- line 148 ----------------------------------------
-- line 152 ----------------------------------------
             .           
             .           		bool is(const JobID& search_id) const
             .           		{
             .           			return this->id == search_id;
             .           		}
             .           
             .           		bool higher_priority_than(const Job &other) const
             .           		{
     2,221,034 ( 0.01%)  			return priority < other.priority
             .           			       // first tie-break by task ID
     2,212,467 ( 0.01%)  			       || (priority == other.priority
             .           			           && id.task < other.id.task)
             .           			       // second, tie-break by job ID
     4,439,218 ( 0.01%)  			       || (priority == other.priority
             .           			           && id.task == other.id.task
             .           			           && id.job < other.id.job);
             .           		}
             .           
             .           		bool priority_at_least_that_of(const Job &other) const
             .           		{
             .           			return priority <= other.priority;
             .           		}
             .           
             .           		bool priority_exceeds(Priority prio_level) const
             .           		{
            32 ( 0.00%)  			return priority < prio_level;
             .           		}
             .           
             .           		bool priority_at_least(Priority prio_level) const
             .           		{
         2,499 ( 0.00%)  			return priority <= prio_level;
             .           		}
             .           
             .           		Interval<Time> scheduling_window() const
             .           		{
             .           			// inclusive interval, so take off one epsilon
             .           			return Interval<Time>{
             .           			                earliest_arrival(),
         5,014 ( 0.00%)  			                deadline - Time_model::constants<Time>::epsilon()};
             .           		}
             .           
             .           		static Interval<Time> scheduling_window(const Job& j)
             .           		{
             .           			return j.scheduling_window();
             .           		}
             .           
             .           		friend std::ostream& operator<< (std::ostream& stream, const Job& j)
-- line 198 ----------------------------------------
-- line 253 ----------------------------------------
             .           		}
             .           	};
             .           
             .           	template<> struct hash<NP::JobID>
             .           	{
             .           		std::size_t operator()(NP::JobID const& id) const
             .           		{
             .           			hash<unsigned long> h;
    29,928,855 ( 0.08%)  			return (h(id.job) << 4) ^ h(id.task);
             .           		}
             .           
             .           	};
             .           }
             .           
             .           #endif

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/hashtable.h
--------------------------------------------------------------------------------
Ir                  

-- line 268 ----------------------------------------
         .           	// Allocate a node and construct an element within it.
         .           	template<typename... _Args>
         .           	  _Scoped_node(__hashtable_alloc* __h, _Args&&... __args)
         .           	  : _M_h(__h),
         .           	    _M_node(__h->_M_allocate_node(std::forward<_Args>(__args)...))
         .           	  { }
         .           
         .           	// Destroy element and deallocate node.
 3,151,273 ( 0.01%)  	~_Scoped_node() { if (_M_node) _M_h->_M_deallocate_node(_M_node); };
         .           
         .           	_Scoped_node(const _Scoped_node&) = delete;
         .           	_Scoped_node& operator=(const _Scoped_node&) = delete;
         .           
         .           	__hashtable_alloc* _M_h;
         .           	__node_type* _M_node;
         .                 };
         .           
-- line 284 ----------------------------------------
-- line 372 ----------------------------------------
         .                 // which is not allocated so that we can have those operations noexcept
         .                 // qualified.
         .                 // Note that we can't leave hashtable with 0 bucket without adding
         .                 // numerous checks in the code to avoid 0 modulus.
         .                 __bucket_type		_M_single_bucket	= nullptr;
         .           
         .                 bool
         .                 _M_uses_single_bucket(__bucket_type* __bkts) const
         7 ( 0.00%)        { return __builtin_expect(__bkts == &_M_single_bucket, false); }
         .           
         .                 bool
         .                 _M_uses_single_bucket() const
         .                 { return _M_uses_single_bucket(_M_buckets); }
         .           
         .                 __hashtable_alloc&
         .                 _M_base_alloc() { return *this; }
         .           
         .                 __bucket_type*
         .                 _M_allocate_buckets(size_type __bkt_count)
         .                 {
        92 ( 0.00%)  	if (__builtin_expect(__bkt_count == 1, false))
         .           	  {
         .           	    _M_single_bucket = nullptr;
        46 ( 0.00%)  	    return &_M_single_bucket;
         .           	  }
         .           
         .           	return __hashtable_alloc::_M_allocate_buckets(__bkt_count);
         .                 }
         .           
         .                 void
         .                 _M_deallocate_buckets(__bucket_type* __bkts, size_type __bkt_count)
         .                 {
       106 ( 0.00%)  	if (_M_uses_single_bucket(__bkts))
         .           	  return;
         .           
         .           	__hashtable_alloc::_M_deallocate_buckets(__bkts, __bkt_count);
         .                 }
         .           
         .                 void
         .                 _M_deallocate_buckets()
       106 ( 0.00%)        { _M_deallocate_buckets(_M_buckets, _M_bucket_count); }
         .           
         .                 // Gets bucket begin, deals with the fact that non-empty buckets contain
         .                 // their before begin node.
         .                 __node_type*
         .                 _M_bucket_begin(size_type __bkt) const;
         .           
         .                 __node_type*
         .                 _M_begin() const
     2,555 ( 0.00%)        { return static_cast<__node_type*>(_M_before_begin._M_nxt); }
         .           
         .                 // Assign *this using another _Hashtable instance. Whether elements
         .                 // are copied or moved depends on the _Ht reference.
         .                 template<typename _Ht>
         .           	void
         .           	_M_assign_elements(_Ht&&);
         .           
         .                 template<typename _Ht, typename _NodeGenerator>
-- line 429 ----------------------------------------
-- line 443 ----------------------------------------
         .           		 const _Equal& __eq, const _ExtractKey& __exk,
         .           		 const allocator_type& __a)
         .                 : __hashtable_base(__exk, __h1, __h2, __h, __eq),
         .           	__hashtable_alloc(__node_alloc_type(__a))
         .                 { }
         .           
         .               public:
         .                 // Constructor, destructor, assignment, swap
        28 ( 0.00%)        _Hashtable() = default;
         .                 _Hashtable(size_type __bkt_count_hint,
         .           		 const _H1&, const _H2&, const _Hash&,
         .           		 const _Equal&, const _ExtractKey&,
         .           		 const allocator_type&);
         .           
         .                 template<typename _InputIterator>
         .           	_Hashtable(_InputIterator __first, _InputIterator __last,
         .           		   size_type __bkt_count_hint,
-- line 459 ----------------------------------------
-- line 674 ----------------------------------------
         .               protected:
         .                 // Bucket index computation helpers.
         .                 size_type
         .                 _M_bucket_index(__node_type* __n) const noexcept
         .                 { return __hash_code_base::_M_bucket_index(__n, _M_bucket_count); }
         .           
         .                 size_type
         .                 _M_bucket_index(const key_type& __k, __hash_code __c) const
 6,622,044 ( 0.02%)        { return __hash_code_base::_M_bucket_index(__k, __c, _M_bucket_count); }
         .           
         .                 // Find and insert helper functions and types
         .                 // Find the node before the one matching the criteria.
         .                 __node_base*
         .                 _M_find_before_node(size_type, const key_type&, __hash_code) const;
         .           
         .                 __node_type*
         .                 _M_find_node(size_type __bkt, const key_type& __key,
-- line 690 ----------------------------------------
-- line 780 ----------------------------------------
         .                 iterator
         .                 _M_erase(size_type __bkt, __node_base* __prev_n, __node_type* __n);
         .           
         .               public:
         .                 // Emplace
         .                 template<typename... _Args>
         .           	__ireturn_type
         .           	emplace(_Args&&... __args)
12,607,583 ( 0.03%)  	{ return _M_emplace(__unique_keys(), std::forward<_Args>(__args)...); }
   707,322 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, unsigned long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, unsigned long&>(std::integral_constant<bool, true>, NP::JobID&&, unsigned long&) [clone .constprop.0] (2,499x)
         .           
         .                 template<typename... _Args>
         .           	iterator
         .           	emplace_hint(const_iterator __hint, _Args&&... __args)
         .           	{
         .           	  return _M_emplace(__hint, __unique_keys(),
         .           			    std::forward<_Args>(__args)...);
         .           	}
-- line 796 ----------------------------------------
-- line 1375 ----------------------------------------
         .           	  __ht.clear();
         .           	}
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
        20 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               ~_Hashtable() noexcept
         .               {
         .                 clear();
         .                 _M_deallocate_buckets();
        16 ( 0.00%)      }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
-- line 1397 ----------------------------------------
-- line 1439 ----------------------------------------
         .           	  = &__x._M_before_begin;
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               auto
     5,014 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               find(const key_type& __k)
         .               -> iterator
         .               {
         .                 __hash_code __code = this->_M_hash_code(__k);
         .                 std::size_t __bkt = _M_bucket_index(__k, __code);
         .                 __node_type* __p = _M_find_node(__bkt, __k, __code);
         .                 return __p ? iterator(__p) : end();
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               auto
     2,507 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               find(const key_type& __k) const
         .               -> const_iterator
         .               {
         .                 __hash_code __code = this->_M_hash_code(__k);
         .                 std::size_t __bkt = _M_bucket_index(__k, __code);
         .                 __node_type* __p = _M_find_node(__bkt, __k, __code);
         .                 return __p ? const_iterator(__p) : end();
-- line 1471 ----------------------------------------
-- line 1565 ----------------------------------------
         .           	   typename _Traits>
         .               auto
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               _M_find_before_node(size_type __bkt, const key_type& __k,
         .           			__hash_code __code) const
         .               -> __node_base*
         .               {
13,244,089 ( 0.03%)        __node_base* __prev_p = _M_buckets[__bkt];
13,244,088 ( 0.03%)        if (!__prev_p)
         .           	return nullptr;
         .           
 4,254,521 ( 0.01%)        for (__node_type* __p = static_cast<__node_type*>(__prev_p->_M_nxt);;
         .           	   __p = __p->_M_next())
         .           	{
         .           	  if (this->_M_equals(__k, __code, __p))
         .           	    return __prev_p;
         .           
 4,035,220 ( 0.01%)  	  if (!__p->_M_nxt || _M_bucket_index(__p->_M_next()) != __bkt)
         .           	    break;
         .           	  __prev_p = __p;
         .           	}
         .                 return nullptr;
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               _M_insert_bucket_begin(size_type __bkt, __node_type* __node)
         .               {
15,768,897 ( 0.04%)        if (_M_buckets[__bkt])
         .           	{
         .           	  // Bucket is not empty, we just need to insert the new node
         .           	  // after the bucket before begin.
 1,576,138 ( 0.00%)  	  __node->_M_nxt = _M_buckets[__bkt]->_M_nxt;
 1,576,138 ( 0.00%)  	  _M_buckets[__bkt]->_M_nxt = __node;
         .           	}
         .                 else
         .           	{
         .           	  // The bucket is empty, the new node is inserted at the
         .           	  // beginning of the singly-linked list and the bucket will
         .           	  // contain _M_before_begin pointer.
 4,731,420 ( 0.01%)  	  __node->_M_nxt = _M_before_begin._M_nxt;
 2,365,710 ( 0.01%)  	  _M_before_begin._M_nxt = __node;
 7,097,129 ( 0.02%)  	  if (__node->_M_nxt)
         .           	    // We must update former begin bucket that is pointing to
         .           	    // _M_before_begin.
 2,363,205 ( 0.01%)  	    _M_buckets[_M_bucket_index(__node->_M_next())] = __node;
 7,097,130 ( 0.02%)  	  _M_buckets[__bkt] = &_M_before_begin;
         .           	}
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
-- line 1625 ----------------------------------------
-- line 1659 ----------------------------------------
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               template<typename... _Args>
         .                 auto
25,210,168 ( 0.07%)        _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           		 _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .                 _M_emplace(true_type, _Args&&... __args)
         .                 -> pair<iterator, bool>
         .                 {
         .           	// First build the node to get access to the hash code
         .           	_Scoped_node __node { this, std::forward<_Args>(__args)...  };
         .           	const key_type& __k = this->_M_extract()(__node._M_node->_M_v());
         .           	__hash_code __code = this->_M_hash_code(__k);
         .           	size_type __bkt = _M_bucket_index(__k, __code);
         .           	if (__node_type* __p = _M_find_node(__bkt, __k, __code))
         .           	  // There is already an equivalent node, no insertion
         .           	  return std::make_pair(iterator(__p), false);
         .           
         .           	// Insert the node
22,076,446 ( 0.06%)  	auto __pos = _M_insert_unique_node(__k, __bkt, __code, __node._M_node);
   287,682 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, unsigned long>, true>*, unsigned long) [clone .isra.0] (2,507x)
         .           	__node._M_node = nullptr;
 3,151,271 ( 0.01%)  	return { __pos, true };
25,210,168 ( 0.07%)        }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               template<typename... _Args>
         .                 auto
         .                 _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
-- line 1693 ----------------------------------------
-- line 1706 ----------------------------------------
         .           	return __pos;
         .                 }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               auto
31,537,780 ( 0.08%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               _M_insert_unique_node(const key_type& __k, size_type __bkt,
         .           			  __hash_code __code, __node_type* __node,
         .           			  size_type __n_elt)
         .               -> iterator
         .               {
 6,307,558 ( 0.02%)        const __rehash_state& __saved_state = _M_rehash_policy._M_state();
         .                 std::pair<bool, std::size_t> __do_rehash
15,768,901 ( 0.04%)  	= _M_rehash_policy._M_need_rehash(_M_bucket_count, _M_element_count,
    56,572 ( 0.00%)  => ???:std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const (7,520x)
     2,257 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
         .           					  __n_elt);
         .           
 6,307,558 ( 0.02%)        if (__do_rehash.first)
         .           	{
       137 ( 0.00%)  	  _M_rehash(__do_rehash.second, __saved_state);
   478,446 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&) (27x)
         .           	  __bkt = _M_bucket_index(__k, __code);
         .           	}
         .           
         .                 this->_M_store_code(__node, __code);
         .           
         .                 // Always insert at the beginning of the bucket.
         .                 _M_insert_bucket_begin(__bkt, __node);
 3,153,779 ( 0.01%)        ++_M_element_count;
         .                 return iterator(__node);
22,076,491 ( 0.06%)      }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               auto
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
-- line 1746 ----------------------------------------
-- line 2023 ----------------------------------------
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               clear() noexcept
         .               {
         .                 this->_M_deallocate_nodes(_M_begin());
    20,056 ( 0.00%)        __builtin_memset(_M_buckets, 0, _M_bucket_count * sizeof(__bucket_type));
        14 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (1x)
     2,510 ( 0.00%)        _M_element_count = 0;
     2,509 ( 0.00%)        _M_before_begin._M_nxt = nullptr;
         2 ( 0.00%)      }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
-- line 2042 ----------------------------------------
-- line 2056 ----------------------------------------
         .           	_M_rehash_policy._M_reset(__saved_state);
         .               }
         .           
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
       322 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               _M_rehash(size_type __bkt_count, const __rehash_state& __state)
         .               {
         .                 __try
         .           	{
         .           	  _M_rehash_aux(__bkt_count, __unique_keys());
         .           	}
         .                 __catch(...)
         .           	{
         .           	  // A failure here means that buckets allocation failed.  We only
         .           	  // have to restore hash policy previous state.
         .           	  _M_rehash_policy._M_reset(__state);
         .           	  __throw_exception_again;
         .           	}
       276 ( 0.00%)      }
         .           
         .             // Rehash when there is no equivalent elements.
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
         .               void
         .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
         .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
         .               _M_rehash_aux(size_type __bkt_count, true_type)
         .               {
         .                 __bucket_type* __new_buckets = _M_allocate_buckets(__bkt_count);
         .                 __node_type* __p = _M_begin();
        46 ( 0.00%)        _M_before_begin._M_nxt = nullptr;
        40 ( 0.00%)        std::size_t __bbegin_bkt = 0;
    45,048 ( 0.00%)        while (__p)
         .           	{
         .           	  __node_type* __next = __p->_M_next();
         .           	  std::size_t __bkt
         .           	    = __hash_code_base::_M_bucket_index(__p, __bkt_count);
    89,840 ( 0.00%)  	  if (!__new_buckets[__bkt])
         .           	    {
    32,582 ( 0.00%)  	      __p->_M_nxt = _M_before_begin._M_nxt;
    16,291 ( 0.00%)  	      _M_before_begin._M_nxt = __p;
    16,371 ( 0.00%)  	      __new_buckets[__bkt] = &_M_before_begin;
    32,582 ( 0.00%)  	      if (__p->_M_nxt)
    16,251 ( 0.00%)  		__new_buckets[__bbegin_bkt] = __p;
    16,331 ( 0.00%)  	      __bbegin_bkt = __bkt;
         .           	    }
         .           	  else
         .           	    {
    12,338 ( 0.00%)  	      __p->_M_nxt = __new_buckets[__bkt]->_M_nxt;
    12,338 ( 0.00%)  	      __new_buckets[__bkt]->_M_nxt = __p;
         .           	    }
         .           	  __p = __next;
         .           	}
         .           
         .                 _M_deallocate_buckets();
        46 ( 0.00%)        _M_bucket_count = __bkt_count;
        46 ( 0.00%)        _M_buckets = __new_buckets;
         .               }
         .           
         .             // Rehash when there can be equivalent elements, preserve their relative
         .             // order.
         .             template<typename _Key, typename _Value,
         .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
         .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
         .           	   typename _Traits>
-- line 2127 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 180 ----------------------------------------
          .           
          .           class ThreadId {
          .           #if USE_PTHREAD
          .               std::atomic<pthread_t> tid;
          .           #else
          .               std::atomic<DWORD>     tid;
          .           #endif
          .           public:
        478 ( 0.00%)      ThreadId() : tid(GetMyTID()) {}
        478 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (239x)
          .           #if USE_PTHREAD
 50,451,138 ( 0.13%)      bool isCurrentThreadId() const { return pthread_equal(pthread_self(), tid.load(std::memory_order_relaxed)); }
 50,451,138 ( 0.13%)  => ./nptl/pthread_self.c:pthread_self (25,225,569x)
          .           #else
          .               bool isCurrentThreadId() const { return GetCurrentThreadId() == tid.load(std::memory_order_relaxed); }
          .           #endif
          .               ThreadId& operator=(const ThreadId& other) {
          .                   tid.store(other.tid.load(std::memory_order_relaxed), std::memory_order_relaxed);
          .                   return *this;
          .               }
          .               static bool init() { return true; }
-- line 198 ----------------------------------------
-- line 207 ----------------------------------------
          .           
          .           bool TLSKey::init()
          .           {
          .           #if USE_WINTHREAD
          .               TLS_pointer_key = TlsAlloc();
          .               if (TLS_pointer_key == TLS_ALLOC_FAILURE)
          .                   return false;
          .           #else
          9 ( 0.00%)      int status = pthread_key_create( &TLS_pointer_key, mallocThreadShutdownNotification );
      1,152 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          2 ( 0.00%)      if ( status )
          .                   return false;
          .           #endif /* USE_WINTHREAD */
          .               return true;
          .           }
          .           
          .           bool TLSKey::destroy()
          .           {
          .           #if USE_WINTHREAD
-- line 224 ----------------------------------------
-- line 227 ----------------------------------------
          .               int status1 = pthread_key_delete(TLS_pointer_key);
          .           #endif /* USE_WINTHREAD */
          .               MALLOC_ASSERT(!status1, "The memory manager cannot delete tls key.");
          .               return status1==0;
          .           }
          .           
          .           inline TLSData* TLSKey::getThreadMallocTLS() const
          .           {
100,930,114 ( 0.26%)      return (TLSData *)TlsGetValue_func( TLS_pointer_key );
428,884,228 ( 1.12%)  => ./nptl/pthread_getspecific.c:pthread_getspecific (25,228,484x)
      1,178 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .           }
          .           
          .           inline void TLSKey::setThreadMallocTLS( TLSData * newvalue ) {
          .               RecursiveMallocCallProtector scoped;
         13 ( 0.00%)      TlsSetValue_func( TLS_pointer_key, newvalue );
         32 ( 0.00%)  => ./nptl/pthread_setspecific.c:pthread_setspecific (1x)
          .           }
          .           
          .           /* The 'next' field in the block header has to maintain some invariants:
          .            *   it needs to be on a 16K boundary and the first field in the block.
          .            *   Any value stored there needs to have the lower 14 bits set to 0
          .            *   so that various assert work. This means that if you want to smash this memory
          .            *   for debugging purposes you will need to obey this invariant.
          .            * The total size of the header needs to be a power of 2 to simplify
-- line 248 ----------------------------------------
-- line 346 ----------------------------------------
          .           
          .           // Use inheritance to guarantee that a user data start on next cache line.
          .           // Can't use member for it, because when LocalBlockFields already on cache line,
          .           // we must have no additional memory consumption for all compilers.
          .           class Block : public LocalBlockFields,
          .                         Padding<2*blockHeaderAlignment - sizeof(LocalBlockFields)> {
          .           public:
          .               bool empty() const {
 50,451,180 ( 0.13%)          if (allocatedCount > 0) return false;
          .                   MALLOC_ASSERT(!isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT);
          .                   return true;
          .               }
          .               inline FreeObject* allocate();
          .               inline FreeObject *allocateFromFreeList();
          .           
          .               inline bool adjustFullness();
          .               void adjustPositionInBin(Bin* bin = nullptr);
-- line 362 ----------------------------------------
-- line 369 ----------------------------------------
          .               void privatizePublicFreeList( bool reset = true );
          .               void restoreBumpPtr();
          .               void privatizeOrphaned(TLSData *tls, unsigned index);
          .               bool readyToShare();
          .               void shareOrphaned(intptr_t binTag, unsigned index);
          .               unsigned int getSize() const {
          .                   MALLOC_ASSERT(isStartupAllocObject() || objectSize<minLargeObjectSize,
          .                                 "Invalid object size");
          9 ( 0.00%)          return isStartupAllocObject()? 0 : objectSize;
          .               }
          .               const BackRefIdx *getBackRefIdx() const { return &backRefIdx; }
          .               inline bool isOwnedByCurrentThread() const;
          3 ( 0.00%)      bool isStartupAllocObject() const { return objectSize == startupAllocObjSizeMark; }
          .               inline FreeObject *findObjectToFree(const void *object) const;
          .               void checkFreePrecond(const void *object) const {
          .           #if MALLOC_DEBUG
          .                   const char *msg = "Possible double free or heap corruption.";
          .                   // small objects are always at least sizeof(size_t) Byte aligned,
          .                   // try to check this before this dereference as for invalid objects
          .                   // this may be unreadable
          .                   MALLOC_ASSERT(isAligned(object, sizeof(size_t)), "Try to free invalid small object");
-- line 389 ----------------------------------------
-- line 453 ----------------------------------------
          .           class Bin {
          .           private:
          .           public:
          .               Block *activeBlk;
          .               std::atomic<Block*> mailbox;
          .               MallocMutex mailLock;
          .           
          .           public:
 25,225,601 ( 0.07%)      inline Block* getActiveBlock() const { return activeBlk; }
         21 ( 0.00%)      void resetActiveBlock() { activeBlk = nullptr; }
          .               inline void setActiveBlock(Block *block);
          .               inline Block* setPreviousBlockActive();
          .               Block* getPrivatizedFreeListBlock();
          .               void moveBlockToFront(Block *block);
          .               bool cleanPublicFreeLists();
          .               void processEmptyBlock(Block *block, bool poolTheBlock);
          .               void addPublicFreeListBlock(Block* block);
          .           
-- line 470 ----------------------------------------
-- line 546 ----------------------------------------
          .                   ResOfGet() = delete;
          .               public:
          .                   Block* block;
          .                   bool   lastAccMiss;
          .                   ResOfGet(Block *b, bool lastMiss) : block(b), lastAccMiss(lastMiss) {}
          .               };
          .           
          .               // allocated in zero-initialized memory
          1 ( 0.00%)      FreeBlockPool(Backend *bknd) : backend(bknd) {}
          .               ResOfGet getBlock();
          .               void returnBlock(Block *block);
          .               bool externalCleanup(); // can be called by another thread
          .           };
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           class LocalLOCImpl {
          .           private:
-- line 562 ----------------------------------------
-- line 586 ----------------------------------------
          .           public:
          .               Bin           bin[numBlockBinLimit];
          .               FreeBlockPool freeSlabBlocks;
          .               LocalLOC      lloc;
          .               unsigned      currCacheIdx;
          .           private:
          .               std::atomic<bool> unused;
          .           public:
         95 ( 0.00%)      TLSData(MemoryPool *mPool, Backend *bknd) : memPool(mPool), freeSlabBlocks(bknd) {}
          .               MemoryPool *getMemPool() const { return memPool; }
          .               Bin* getAllocationBin(size_t size);
          .               void release();
          .               bool externalCleanup(bool cleanOnlyUnused, bool cleanBins) {
          .                   if (!unused.load(std::memory_order_relaxed) && cleanOnlyUnused) return false;
          .                   // Heavy operation in terms of synchronization complexity,
          .                   // should be called only for the current thread
          .                   bool released = cleanBins ? cleanupBlockBins() : false;
          .                   // both cleanups to be called, and the order is not important
          4 ( 0.00%)          return released | lloc.externalCleanup(&memPool->extMemPool) | freeSlabBlocks.externalCleanup();
      2,325 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::externalCleanup() (1x)
          .               }
          .               bool cleanupBlockBins();
          .               void markUsed() { unused.store(false, std::memory_order_relaxed); } // called by owner when TLS touched
          .               void markUnused() { unused.store(true, std::memory_order_relaxed); } // can be called by not owner thread
          .           };
          .           
          .           TLSData *TLSKey::createTLS(MemoryPool *memPool, Backend *backend)
         11 ( 0.00%)  {
          .               MALLOC_ASSERT( sizeof(TLSData) >= sizeof(Bin) * numBlockBins + sizeof(FreeBlockPool), ASSERT_TEXT );
          1 ( 0.00%)      TLSData* tls = (TLSData*) memPool->bootStrapBlocks.allocate(memPool, sizeof(TLSData));
          .               if ( !tls )
          .                   return nullptr;
          .               new(tls) TLSData(memPool, backend);
          .               /* the block contains zeroes after bootStrapMalloc, so bins are initialized */
          .           #if MALLOC_DEBUG
          .               for (uint32_t i = 0; i < numBlockBinLimit; i++)
          .                   tls->bin[i].verifyInitState();
          .           #endif
          .               setThreadMallocTLS(tls);
          .               memPool->extMemPool.allLocalCaches.registerThread(tls);
          .               return tls;
          9 ( 0.00%)  }
          .           
          .           bool TLSData::cleanupBlockBins()
          .           {
          .               bool released = false;
          .               for (uint32_t i = 0; i < numBlockBinLimit; i++) {
          .                   released |= bin[i].cleanPublicFreeLists();
          .                   // After cleaning public free lists, only the active block might be empty.
          .                   // Do not use processEmptyBlock because it will just restore bumpPtr.
-- line 634 ----------------------------------------
-- line 651 ----------------------------------------
          .               if (TLSData *tlsData = tlsPointerKey.getThreadMallocTLS())
          .                   released |= tlsData->cleanupBlockBins();
          .           
          .               return released;
          .           }
          .           
          .           void AllLocalCaches::registerThread(TLSRemote *tls)
          .           {
          1 ( 0.00%)      tls->prev = nullptr;
          1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
          .               MALLOC_ASSERT(head!=tls, ASSERT_TEXT);
          2 ( 0.00%)      tls->next = head;
          2 ( 0.00%)      if (head)
          .                   head->prev = tls;
          1 ( 0.00%)      head = tls;
          .               MALLOC_ASSERT(head->next!=head, ASSERT_TEXT);
          .           }
          .           
          .           void AllLocalCaches::unregisterThread(TLSRemote *tls)
          .           {
          1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
          .               MALLOC_ASSERT(head, "Can't unregister thread: no threads are registered.");
          2 ( 0.00%)      if (head == tls)
          3 ( 0.00%)          head = tls->next;
          2 ( 0.00%)      if (tls->next)
          1 ( 0.00%)          tls->next->prev = tls->prev;
          2 ( 0.00%)      if (tls->prev)
          .                   tls->prev->next = tls->next;
          .               MALLOC_ASSERT(!tls->next || tls->next->next!=tls->next, ASSERT_TEXT);
          .           }
          .           
          .           bool AllLocalCaches::cleanup(bool cleanOnlyUnused)
          .           {
          .               bool released = false;
          .               {
-- line 685 ----------------------------------------
-- line 688 ----------------------------------------
          .                       released |= static_cast<TLSData*>(curr)->externalCleanup(cleanOnlyUnused, /*cleanBins=*/false);
          .               }
          .               return released;
          .           }
          .           
          .           void AllLocalCaches::markUnused()
          .           {
          .               bool locked;
          2 ( 0.00%)      MallocMutex::scoped_lock lock(listLock, /*block=*/false, &locked);
          4 ( 0.00%)      if (!locked) // not wait for marking if someone doing something with it
          .                   return;
          .           
         14 ( 0.00%)      for (TLSRemote *curr=head; curr; curr=curr->next)
          .                   static_cast<TLSData*>(curr)->markUnused();
          .           }
          .           
          .           #if MALLOC_CHECK_RECURSION
          .           MallocMutex RecursiveMallocCallProtector::rmc_mutex;
          .           std::atomic<pthread_t> RecursiveMallocCallProtector::owner_thread;
          .           std::atomic<void*> RecursiveMallocCallProtector::autoObjPtr;
          .           bool        RecursiveMallocCallProtector::mallocRecursionDetected;
-- line 708 ----------------------------------------
-- line 768 ----------------------------------------
          .           #endif
          .           static inline unsigned int highestBitPos(unsigned int n)
          .           {
          .               MALLOC_ASSERT( n>=64 && n<1024, ASSERT_TEXT ); // only needed for bsr array lookup, but always true
          .               unsigned int pos;
          .           #if __ARCH_x86_32||__ARCH_x86_64
          .           
          .           # if __unix__||__APPLE__||__MINGW32__
     19,615 ( 0.00%)      __asm__ ("bsr %1,%0" : "=r"(pos) : "r"(n));
          .           # elif (_WIN32 && (!_WIN64 || __INTEL_COMPILER))
          .               __asm
          .               {
          .                   bsr eax, n
          .                   mov pos, eax
          .               }
          .           # elif _WIN64 && _MSC_VER>=1400
          .               _BitScanReverse((unsigned long*)&pos, (unsigned long)n);
-- line 784 ----------------------------------------
-- line 796 ----------------------------------------
          .               static unsigned int bsr[16] = {0/*N/A*/,6,7,7,8,8,8,8,9,9,9,9,9,9,9,9};
          .               pos = bsr[ n>>6 ];
          .           #endif /* __ARCH_* */
          .               return pos;
          .           }
          .           
          .           unsigned int getSmallObjectIndex(unsigned int size)
          .           {
 72,641,056 ( 0.19%)      unsigned int result = (size-1)>>3;
          .               if (sizeof(void*)==8) {
          .                   // For 64-bit malloc, 16 byte alignment is needed except for bin 0.
181,602,744 ( 0.47%)          if (result) result |= 1; // 0,1,3,5,7; bins 2,4,6 are not aligned to 16 bytes
          .               }
          .               return result;
          .           }
          .           
          .           /*
          .            * Depending on indexRequest, for a given size return either the index into the bin
          .            * for objects of this size, or the actual size of objects in this bin.
          .            */
          .           template<bool indexRequest>
          .           static unsigned int getIndexOrObjectSize (unsigned int size)
          .           {
 72,697,534 ( 0.19%)      if (size <= maxSmallObjectSize) { // selection from 8/16/24/32/40/48/56/64
          .                   unsigned int index = getSmallObjectIndex( size );
          .                    /* Bin 0 is for 8 bytes, bin 1 is for 16, and so forth */
          .                   return indexRequest ? index : (index+1)<<3;
          .               }
     56,408 ( 0.00%)      else if (size <= maxSegregatedObjectSize ) { // 80/96/112/128 / 160/192/224/256 / 320/384/448/512 / 640/768/896/1024
     19,615 ( 0.00%)          unsigned int order = highestBitPos(size-1); // which group of bin sizes?
          .                   MALLOC_ASSERT( 6<=order && order<=9, ASSERT_TEXT );
          .                   if (indexRequest)
     77,808 ( 0.00%)              return minSegregatedObjectIndex - (4*6) - 4 + (4*order) + ((size-1)>>(order-2));
          .                   else {
        815 ( 0.00%)              unsigned int alignment = 128 >> (9-order); // alignment in the group
          .                       MALLOC_ASSERT( alignment==16 || alignment==32 || alignment==64 || alignment==128, ASSERT_TEXT );
          .                       return alignUp(size,alignment);
          .                   }
          .               }
          .               else {
     17,181 ( 0.00%)          if( size <= fittingSize3 ) {
      9,110 ( 0.00%)              if( size <= fittingSize2 ) {
         18 ( 0.00%)                  if( size <= fittingSize1 )
          .                               return indexRequest ? minFittingIndex : fittingSize1;
          .                           else
     22,760 ( 0.00%)                      return indexRequest ? minFittingIndex+1 : fittingSize2;
          .                       } else
      4,552 ( 0.00%)                  return indexRequest ? minFittingIndex+2 : fittingSize3;
          .                   } else {
      8,068 ( 0.00%)              if( size <= fittingSize5 ) {
         15 ( 0.00%)                  if( size <= fittingSize4 )
          .                               return indexRequest ? minFittingIndex+3 : fittingSize4;
          .                           else
     16,124 ( 0.00%)                      return indexRequest ? minFittingIndex+4 : fittingSize5;
          .                       } else {
          .                           MALLOC_ASSERT( 0,ASSERT_TEXT ); // this should not happen
      4,031 ( 0.00%)                  return ~0U;
          .                       }
          .                   }
          .               }
      4,031 ( 0.00%)  }
          .           
          .           static unsigned int getIndex (unsigned int size)
          .           {
 61,574,341 ( 0.16%)      return getIndexOrObjectSize</*indexRequest=*/true>(size);
227,071,424 ( 0.59%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) (25,225,572x)
          .           }
          .           
          .           static unsigned int getObjectSize (unsigned int size)
          .           {
          .               return getIndexOrObjectSize</*indexRequest=*/false>(size);
          .           }
          .           
          .           
-- line 868 ----------------------------------------
-- line 870 ----------------------------------------
          .           {
          .               FreeObject *result;
          .           
          .               MALLOC_ASSERT( size == sizeof(TLSData), ASSERT_TEXT );
          .           
          .               { // Lock with acquire
          .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
          .           
          3 ( 0.00%)          if( bootStrapObjectList) {
          .                       result = bootStrapObjectList;
          .                       bootStrapObjectList = bootStrapObjectList->next;
          .                   } else {
          3 ( 0.00%)              if (!bootStrapBlock) {
          4 ( 0.00%)                  bootStrapBlock = memPool->getEmptyBlock(size);
        584 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (1x)
          2 ( 0.00%)                  if (!bootStrapBlock) return nullptr;
          .                       }
          1 ( 0.00%)              result = bootStrapBlock->bumpPtr;
          5 ( 0.00%)              bootStrapBlock->bumpPtr = (FreeObject *)((uintptr_t)bootStrapBlock->bumpPtr - bootStrapBlock->objectSize);
          3 ( 0.00%)              if ((uintptr_t)bootStrapBlock->bumpPtr < (uintptr_t)bootStrapBlock+sizeof(Block)) {
          .                           bootStrapBlock->bumpPtr = nullptr;
          .                           bootStrapBlock->next = bootStrapBlockUsed;
          .                           bootStrapBlockUsed = bootStrapBlock;
          .                           bootStrapBlock = nullptr;
          .                       }
          .                   }
          .               } // Unlock with release
        115 ( 0.00%)      memset (result, 0, size);
          .               return (void*)result;
          .           }
          .           
          .           void BootStrapBlocks::free(void* ptr)
          .           {
          .               MALLOC_ASSERT( ptr, ASSERT_TEXT );
          .               { // Lock with acquire
          .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
          3 ( 0.00%)          ((FreeObject*)ptr)->next = bootStrapObjectList;
          1 ( 0.00%)          bootStrapObjectList = (FreeObject*)ptr;
          .               } // Unlock with release
          .           }
          .           
          .           void BootStrapBlocks::reset()
          .           {
          .               bootStrapBlock = bootStrapBlockUsed = nullptr;
          .               bootStrapObjectList = nullptr;
          .           }
-- line 914 ----------------------------------------
-- line 927 ----------------------------------------
          .           LifoList::LifoList( ) : top(nullptr)
          .           {
          .               // MallocMutex assumes zero initialization
          .               memset(&lock, 0, sizeof(MallocMutex));
          .           }
          .           
          .           void LifoList::push(Block *block)
          .           {
          3 ( 0.00%)      MallocMutex::scoped_lock scoped_cs(lock);
          3 ( 0.00%)      block->next = top.load(std::memory_order_relaxed);
          .               top.store(block, std::memory_order_relaxed);
          .           }
          .           
          .           Block *LifoList::pop()
          .           {
        238 ( 0.00%)      Block* block = nullptr;
        476 ( 0.00%)      if (top.load(std::memory_order_relaxed)) {
          .                   MallocMutex::scoped_lock scoped_cs(lock);
          .                   block = top.load(std::memory_order_relaxed);
          .                   if (block) {
          .                       top.store(block->next, std::memory_order_relaxed);
          .                   }
          .               }
          .               return block;
          .           }
-- line 951 ----------------------------------------
-- line 980 ----------------------------------------
          .                        backend->returnLargeObject(lmb);
          .                    }
          .                }
          .           }
          .           
          .           TLSData* MemoryPool::getTLS(bool create)
          .           {
          .               TLSData* tls = extMemPool.tlsPointerKey.getThreadMallocTLS();
 50,456,970 ( 0.13%)      if (create && !tls)
 50,456,975 ( 0.13%)          tls = extMemPool.tlsPointerKey.createTLS(this, &extMemPool.backend);
      2,123 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::TLSKey::createTLS(rml::internal::MemoryPool*, rml::internal::Backend*) (1x)
          .               return tls;
          .           }
          .           
          .           /*
          .            * Return the bin for the given size.
          .            */
          .           inline Bin* TLSData::getAllocationBin(size_t size)
          .           {
112,089,953 ( 0.29%)      return bin + getIndex(size);
          .           }
          .           
          .           /* Return an empty uninitialized block in a non-blocking fashion. */
          .           Block *MemoryPool::getEmptyBlock(size_t size)
      2,151 ( 0.00%)  {
          .               TLSData* tls = getTLS(/*create=*/false);
          .               // try to use per-thread cache, if TLS available
          .               FreeBlockPool::ResOfGet resOfGet = tls?
        716 ( 0.00%)          tls->freeSlabBlocks.getBlock() : FreeBlockPool::ResOfGet(nullptr, false);
          .               Block *result = resOfGet.block;
          .           
          .               if (!result) { // not found in local cache, asks backend for slabs
         92 ( 0.00%)          int num = resOfGet.lastAccMiss? Backend::numOfSlabAllocOnMiss : 1;
          .                   BackRefIdx backRefIdx[Backend::numOfSlabAllocOnMiss];
          .           
         91 ( 0.00%)          result = static_cast<Block*>(extMemPool.backend.getSlabBlock(num));
        182 ( 0.00%)          if (!result) return nullptr;
          .           
        182 ( 0.00%)          if (!extMemPool.userPool())
        182 ( 0.00%)              for (int i=0; i<num; i++) {
      1,267 ( 0.00%)                  backRefIdx[i] = BackRefIdx::newBackRef(/*largeObj=*/false);
     12,308 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (181x)
        362 ( 0.00%)                  if (backRefIdx[i].isInvalid()) {
          .                               // roll back resource allocation
          .                               for (int j=0; j<i; j++)
          .                                   removeBackRef(backRefIdx[j]);
          .                               Block *b = result;
          .                               for (int j=0; j<num; b=(Block*)((uintptr_t)b+slabSize), j++)
          .                                   extMemPool.backend.putSlabBlock(b);
          .                               return nullptr;
          .                           }
          .                       }
          .                   // resources were allocated, register blocks
          .                   Block *b = result;
        182 ( 0.00%)          for (int i=0; i<num; b=(Block*)((uintptr_t)b+slabSize), i++) {
          .                       // slab block in user's pool must have invalid backRefIdx
        362 ( 0.00%)              if (extMemPool.userPool()) {
          .                           new (&b->backRefIdx) BackRefIdx();
          .                       } else {
        181 ( 0.00%)                  setBackRef(backRefIdx[i], b);
        905 ( 0.00%)                  b->backRefIdx = backRefIdx[i];
          .                       }
          .                       b->tlsPtr.store(tls, std::memory_order_relaxed);
        181 ( 0.00%)              b->poolPtr = this;
          .                       // all but first one go to per-thread pool
        362 ( 0.00%)              if (i > 0) {
          .                           MALLOC_ASSERT(tls, ASSERT_TEXT);
        180 ( 0.00%)                  tls->freeSlabBlocks.returnBlock(b);
      1,890 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (90x)
          .                       }
          .                   }
          .               }
          .               MALLOC_ASSERT(result, ASSERT_TEXT);
          .               result->initEmptyBlock(tls, size);
          .               STAT_increment(getThreadId(), getIndex(result->objectSize), allocBlockNew);
          .               return result;
      2,151 ( 0.00%)  }
          .           
          .           void MemoryPool::returnEmptyBlock(Block *block, bool poolTheBlock)
      1,175 ( 0.00%)  {
          .               block->reset();
        470 ( 0.00%)      if (poolTheBlock) {
        651 ( 0.00%)          getTLS(/*create=*/false)->freeSlabBlocks.returnBlock(block);
     46,667 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (217x)
          .               } else {
          .                   // slab blocks in user's pools do not have valid backRefIdx
         36 ( 0.00%)          if (!extMemPool.userPool())
         54 ( 0.00%)              removeBackRef(*(block->getBackRefIdx()));
        738 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (18x)
         54 ( 0.00%)          extMemPool.backend.putSlabBlock(block);
      6,118 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (18x)
          .               }
        705 ( 0.00%)  }
          .           
          .           bool ExtMemoryPool::init(intptr_t poolId, rawAllocType rawAlloc,
          .                                    rawFreeType rawFree, size_t granularity,
          .                                    bool keepAllMemory, bool fixedPool)
          .           {
          1 ( 0.00%)      this->poolId = poolId;
          1 ( 0.00%)      this->rawAlloc = rawAlloc;
          1 ( 0.00%)      this->rawFree = rawFree;
          1 ( 0.00%)      this->granularity = granularity;
          2 ( 0.00%)      this->keepAllMemory = keepAllMemory;
          1 ( 0.00%)      this->fixedPool = fixedPool;
          .               this->delayRegsReleasing = false;
          .               if (!initTLS())
          .                   return false;
          3 ( 0.00%)      loc.init(this);
      1,296 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/large_objects.cpp:rml::internal::LargeObjectCache::init(rml::internal::ExtMemoryPool*) [clone .part.0] (1x)
          .               backend.init(this);
          .               MALLOC_ASSERT(isPoolValid(), nullptr);
          .               return true;
          .           }
          .           
          .           bool ExtMemoryPool::initTLS() { return tlsPointerKey.init(); }
          .           
          .           bool MemoryPool::init(intptr_t poolId, const MemPoolPolicy *policy)
-- line 1089 ----------------------------------------
-- line 1148 ----------------------------------------
          .                   // for user pool, because it's just about to be released. But for system
          .                   // pool restoring, we do not want to do zeroing of it on subsequent reload.
          .                   bootStrapBlocks.reset();
          .                   extMemPool.orphanedBlocks.reset();
          .               }
          .               return extMemPool.destroy();
          .           }
          .           
          9 ( 0.00%)  void MemoryPool::onThreadShutdown(TLSData *tlsData)
          .           {
          5 ( 0.00%)      if (tlsData) { // might be called for "empty" TLS
     14,529 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::onThreadShutdown(rml::internal::TLSData*) [clone .part.0] (1x)
          .                   tlsData->release();
          2 ( 0.00%)          bootStrapBlocks.free(tlsData);
          .                   clearTLS();
          .               }
          8 ( 0.00%)  }
          .           
          .           #if MALLOC_DEBUG
          .           void Bin::verifyTLSBin (size_t size) const
          .           {
          .           /* The debug version verifies the TLSBin as needed */
          .               uint32_t objSize = getObjectSize(size);
          .           
          .               if (activeBlk) {
-- line 1171 ----------------------------------------
-- line 1209 ----------------------------------------
          .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .               MALLOC_ASSERT( block->objectSize != 0, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->next == nullptr, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->previous == nullptr, ASSERT_TEXT );
          .           
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               verifyTLSBin(size);
          .           
     16,773 ( 0.00%)      block->next = activeBlk;
     33,070 ( 0.00%)      if( activeBlk ) {
     33,028 ( 0.00%)          block->previous = activeBlk->previous;
     16,514 ( 0.00%)          activeBlk->previous = block;
     49,542 ( 0.00%)          if( block->previous )
     30,784 ( 0.00%)              block->previous->next = block;
          .               } else {
          .                   activeBlk = block;
          .               }
          .           
          .               verifyTLSBin(size);
          .           }
          .           
          .           /*
-- line 1230 ----------------------------------------
-- line 1239 ----------------------------------------
          .           
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               verifyTLSBin(size);
          .           
          .               if (block == activeBlk) {
          .                   activeBlk = block->previous? block->previous : block->next;
          .               }
          .               /* Unlink the block */
     49,542 ( 0.00%)      if (block->previous) {
          .                   MALLOC_ASSERT( block->previous->next == block, ASSERT_TEXT );
     32,964 ( 0.00%)          block->previous->next = block->next;
          .               }
     33,028 ( 0.00%)      if (block->next) {
          .                   MALLOC_ASSERT( block->next->previous == block, ASSERT_TEXT );
     16,488 ( 0.00%)          block->next->previous = block->previous;
          .               }
        217 ( 0.00%)      block->next = nullptr;
     16,514 ( 0.00%)      block->previous = nullptr;
          .           
          .               verifyTLSBin(size);
          .           }
          .           
          .           Block* Bin::getPrivatizedFreeListBlock()
          .           {
          .               Block* block;
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               // if this method is called, active block usage must be unsuccessful
          .               MALLOC_ASSERT( !activeBlk && !mailbox.load(std::memory_order_relaxed) || activeBlk && activeBlk->isFull, ASSERT_TEXT );
          .           
          .           // the counter should be changed    STAT_increment(getThreadId(), ThreadCommonCounters, lockPublicFreeList);
        476 ( 0.00%)      if (!mailbox.load(std::memory_order_acquire)) // hotpath is empty mailbox
          .                   return nullptr;
          .               else { // mailbox is not empty, take lock and inspect it
          .                   MallocMutex::scoped_lock scoped_cs(mailLock);
          .                   block = mailbox.load(std::memory_order_relaxed);
          .                   if( block ) {
          .                       MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .                       MALLOC_ASSERT( !isNotForUse(block->nextPrivatizable.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .                       mailbox.store(block->nextPrivatizable.load(std::memory_order_relaxed), std::memory_order_relaxed);
-- line 1277 ----------------------------------------
-- line 1319 ----------------------------------------
          .                       block->adjustPositionInBin(this);
          .                   block = tmp;
          .               }
          .               return released;
          .           }
          .           
          .           bool Block::adjustFullness()
          .           {
  2,715,192 ( 0.01%)      if (bumpPtr) {
          .                   /* If we are still using a bump ptr for this block it is empty enough to use. */
          .                   STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
          .                   isFull = false;
          .               } else {
          .                   const float threshold = (slabSize - sizeof(Block)) * (1 - emptyEnoughRatio);
          .                   /* allocatedCount shows how many objects in the block are in use; however it still counts
          .                    * blocks freed by other threads; so prior call to privatizePublicFreeList() is recommended */
  9,503,172 ( 0.02%)          isFull = (allocatedCount*objectSize > threshold) ? true : false;
          .           #if COLLECT_STATISTICS
          .                   if (isFull)
          .                       STAT_increment(getThreadId(), getIndex(objectSize), examineNotEmpty);
          .                   else
          .                       STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
          .           #endif
          .               }
          .               return isFull;
          .           }
          .           
          .           // This method resides in class Block, and not in class Bin, in order to avoid
          .           // calling getAllocationBin on a reasonably hot path in Block::freeOwnObject
          .           void Block::adjustPositionInBin(Bin* bin/*=nullptr*/)
 14,119,389 ( 0.04%)  {
          .               // If the block were full, but became empty enough to use,
          .               // move it to the front of the list
 29,596,374 ( 0.08%)      if (isFull && !adjustFullness()) {
     32,594 ( 0.00%)          if (!bin)
          .                       bin = tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize);
          .                   bin->moveBlockToFront(this);
          .               }
 14,103,997 ( 0.04%)  }
          .           
          .           /* Restore the bump pointer for an empty block that is planned to use */
          .           void Block::restoreBumpPtr()
          .           {
          .               MALLOC_ASSERT( allocatedCount == 0, ASSERT_TEXT );
          .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .               STAT_increment(getThreadId(), getIndex(objectSize), freeRestoreBumpPtr);
 33,317,889 ( 0.09%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
 11,105,963 ( 0.03%)      freeList = nullptr;
 11,105,963 ( 0.03%)      isFull = false;
 11,105,963 ( 0.03%)  }
          .           
          .           void Block::freeOwnObject(void *object)
          .           {
          .               tlsPtr.load(std::memory_order_relaxed)->markUsed();
          .               allocatedCount--;
          .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
          .           #if COLLECT_STATISTICS
          .               // Note that getAllocationBin is not called on the hottest path with statistics off.
-- line 1376 ----------------------------------------
-- line 1377 ----------------------------------------
          .               if (tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->getActiveBlock() != this)
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToInactiveBlock);
          .               else
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToActiveBlock);
          .           #endif
          .               if (empty()) {
          .                   // If the last object of a slab is freed, the slab cannot be marked full
          .                   MALLOC_ASSERT(!isFull, ASSERT_TEXT);
 22,212,360 ( 0.06%)          tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->processEmptyBlock(this, /*poolTheBlock=*/true);
          .               } else { // hot path
          .                   FreeObject *objectToFree = findObjectToFree(object);
 28,238,778 ( 0.07%)          objectToFree->next = freeList;
 14,119,389 ( 0.04%)          freeList = objectToFree;
 42,358,167 ( 0.11%)          adjustPositionInBin();
 70,704,630 ( 0.18%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) (14,119,389x)
          .               }
          .           }
          .           
          .           void Block::freePublicObject (FreeObject *objectToFree)
          .           {
          .               FreeObject* localPublicFreeList{};
          .           
          .               MALLOC_ITT_SYNC_RELEASING(&publicFreeList);
-- line 1398 ----------------------------------------
-- line 1524 ----------------------------------------
          .           
          .           void Block::shareOrphaned(intptr_t binTag, unsigned index)
          .           {
          .               MALLOC_ASSERT( binTag, ASSERT_TEXT );
          .               // unreferenced formal parameter warning
          .               tbb::detail::suppress_unused_warning(index);
          .               STAT_increment(getThreadId(), index, freeBlockPublic);
          .               markOrphaned();
          6 ( 0.00%)      if ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
          .                   // First check passed: the block is not in mailbox yet.
          .                   // Need to set publicFreeList to non-zero, so other threads
          .                   // will not change nextPrivatizable and it can be zeroed.
          6 ( 0.00%)          if ( !readyToShare() ) {
          .                       // another thread freed an object; we need to wait until it finishes.
          .                       // There is no need for exponential backoff, as the wait here is not for a lock;
          .                       // but need to yield, so the thread we wait has a chance to run.
          .                       // TODO: add a pause to also be friendly to hyperthreads
          .                       int count = 256;
          .                       while ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
          .                           if (--count==0) {
          .                               do_yield();
          .                               count = 256;
          .                           }
          .                       }
          .                   }
          .               }
          .               MALLOC_ASSERT( publicFreeList.load(std::memory_order_relaxed) !=nullptr, ASSERT_TEXT );
          .               // now it is safe to change our data
          3 ( 0.00%)      previous = nullptr;
          .               // it is caller responsibility to ensure that the list of blocks
          .               // formed by nextPrivatizable pointers is kept consistent if required.
          .               // if only called from thread shutdown code, it does not matter.
          .               nextPrivatizable.store((Block*)UNUSABLE, std::memory_order_relaxed);
          .           }
          .           
          .           void Block::cleanBlockHeader()
          .           {
        475 ( 0.00%)      next = nullptr;
        475 ( 0.00%)      previous = nullptr;
        476 ( 0.00%)      freeList = nullptr;
        951 ( 0.00%)      allocatedCount = 0;
        475 ( 0.00%)      isFull = false;
          .               tlsPtr.store(nullptr, std::memory_order_relaxed);
          .           
          .               publicFreeList.store(nullptr, std::memory_order_relaxed);
          .           }
          .           
          .           void Block::initEmptyBlock(TLSData *tls, size_t size)
          .           {
          .               // Having getIndex and getObjectSize called next to each other
          .               // allows better compiler optimization as they basically share the code.
          .               unsigned int index = getIndex(size);
          .               unsigned int objSz = getObjectSize(size);
          .           
          .               cleanBlockHeader();
        540 ( 0.00%)      objectSize = objSz;
          .               markOwned(tls);
          .               // bump pointer should be prepared for first allocation - thus mode it down to objectSize
      1,286 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
          .           
          .               // each block should have the address where the head of the list of "privatizable" blocks is kept
          .               // the only exception is a block for boot strap which is initialized when TLS is yet nullptr
      1,192 ( 0.00%)      nextPrivatizable.store( tls? (Block*)(tls->bin + index) : nullptr, std::memory_order_relaxed);
          .               TRACEF(( "[ScalableMalloc trace] Empty block %p is initialized, owner is %ld, objectSize is %d, bumpPtr is %p\n",
          .                        this, tlsPtr.load(std::memory_order_relaxed) ? getThreadId() : -1, objectSize, bumpPtr ));
          .           }
          .           
          .           Block *OrphanedBlocks::get(TLSData *tls, unsigned int size)
      2,142 ( 0.00%)  {
          .               // TODO: try to use index from getAllocationBin
          .               unsigned int index = getIndex(size);
        952 ( 0.00%)      Block *block = bins[index].pop();
          .               if (block) {
          .                   MALLOC_ITT_SYNC_ACQUIRED(bins+index);
          .                   block->privatizeOrphaned(tls, index);
          .               }
          .               return block;
      2,142 ( 0.00%)  }
          .           
          .           void OrphanedBlocks::put(intptr_t binTag, Block *block)
         21 ( 0.00%)  {
          .               unsigned int index = getIndex(block->getSize());
          .               block->shareOrphaned(binTag, index);
         15 ( 0.00%)      MALLOC_ITT_SYNC_RELEASING(bins+index);
          .               bins[index].push(block);
         24 ( 0.00%)  }
          .           
          .           void OrphanedBlocks::reset()
          .           {
          .               for (uint32_t i=0; i<numBlockBinLimit; i++)
          .                   new (bins+i) LifoList();
          .           }
          .           
          .           bool OrphanedBlocks::cleanup(Backend* backend)
-- line 1617 ----------------------------------------
-- line 1639 ----------------------------------------
          .               }
          .               return released;
          .           }
          .           
          .           FreeBlockPool::ResOfGet FreeBlockPool::getBlock()
          .           {
          .               Block *b = head.exchange(nullptr);
          .           
        476 ( 0.00%)      if (b) {
        148 ( 0.00%)          size--;
        148 ( 0.00%)          Block *newHead = b->next;
        148 ( 0.00%)          lastAccessMiss = false;
          .                   head.store(newHead, std::memory_order_release);
          .               } else {
         90 ( 0.00%)          lastAccessMiss = true;
          .               }
          .               return ResOfGet(b, lastAccessMiss);
          .           }
          .           
          .           void FreeBlockPool::returnBlock(Block *block)
      2,149 ( 0.00%)  {
          .               MALLOC_ASSERT( size <= POOL_HIGH_MARK, ASSERT_TEXT );
          .               Block *localHead = head.exchange(nullptr);
          .           
        921 ( 0.00%)      if (!localHead) {
          .                   size = 0; // head was stolen by externalClean, correct size accordingly
        639 ( 0.00%)      } else if (size == POOL_HIGH_MARK) {
          .                   // release cold blocks and add hot one,
          .                   // so keep POOL_LOW_MARK-1 blocks and add new block to head
          .                   Block *headToFree = localHead, *helper;
          .                   for (int i=0; i<POOL_LOW_MARK-2; i++)
         36 ( 0.00%)              headToFree = headToFree->next;
          .                   Block *last = headToFree;
          6 ( 0.00%)          headToFree = headToFree->next;
          6 ( 0.00%)          last->next = nullptr;
          6 ( 0.00%)          size = POOL_LOW_MARK-1;
        318 ( 0.00%)          for (Block *currBl = headToFree; currBl; currBl = helper) {
        150 ( 0.00%)              helper = currBl->next;
          .                       // slab blocks in user's pools do not have valid backRefIdx
        600 ( 0.00%)              if (!backend->inUserPool())
        450 ( 0.00%)                  removeBackRef(currBl->backRefIdx);
      6,150 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (150x)
        450 ( 0.00%)              backend->putSlabBlock(currBl);
     32,918 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (150x)
          .                   }
          .               }
        538 ( 0.00%)      size++;
        307 ( 0.00%)      block->next = localHead;
          .               head.store(block, std::memory_order_release);
      1,842 ( 0.00%)  }
          .           
          .           bool FreeBlockPool::externalCleanup()
          5 ( 0.00%)  {
          .               Block *helper;
          .               bool released = false;
          .           
         22 ( 0.00%)      for (Block *currBl=head.exchange(nullptr); currBl; currBl=helper) {
          9 ( 0.00%)          helper = currBl->next;
          .                   // slab blocks in user's pools do not have valid backRefIdx
         36 ( 0.00%)          if (!backend->inUserPool())
         27 ( 0.00%)              removeBackRef(currBl->backRefIdx);
        369 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (9x)
         27 ( 0.00%)          backend->putSlabBlock(currBl);
          1 ( 0.00%)          released = true;
          .               }
          .               return released;
          6 ( 0.00%)  }
          .           
          .           /* Prepare the block for returning to FreeBlockPool */
          .           void Block::reset()
          .           {
          .               // it is caller's responsibility to ensure no data is lost before calling this
          .               MALLOC_ASSERT( allocatedCount==0, ASSERT_TEXT );
          .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .               if (!isStartupAllocObject())
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeBlockBack);
          .           
          .               cleanBlockHeader();
          .           
          .               nextPrivatizable.store(nullptr, std::memory_order_relaxed);
          .           
        470 ( 0.00%)      objectSize = 0;
          .               // for an empty block, bump pointer should point right after the end of the block
        470 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize);
          .           }
          .           
          .           inline void Bin::setActiveBlock (Block *block)
          .           {
          .           //    MALLOC_ASSERT( bin, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .               // it is the caller responsibility to keep bin consistence (i.e. ensure this block is in the bin list)
        238 ( 0.00%)      activeBlk = block;
          .           }
          .           
          .           inline Block* Bin::setPreviousBlockActive()
          .           {
          .               MALLOC_ASSERT( activeBlk, ASSERT_TEXT );
     32,594 ( 0.00%)      Block* temp = activeBlk->previous;
     32,594 ( 0.00%)      if( temp ) {
          .                   MALLOC_ASSERT( !(temp->isFull), ASSERT_TEXT );
     16,080 ( 0.00%)          activeBlk = temp;
          .               }
          .               return temp;
          .           }
          .           
          .           inline bool Block::isOwnedByCurrentThread() const {
100,902,276 ( 0.26%)      return tlsPtr.load(std::memory_order_relaxed) && ownerTid.isCurrentThreadId();
          .           }
          .           
          .           FreeObject *Block::findObjectToFree(const void *object) const
          .           {
          .               FreeObject *objectToFree;
          .               // Due to aligned allocations, a pointer passed to scalable_free
          .               // might differ from the address of internally allocated object.
          .               // Small objects however should always be fine.
 42,358,167 ( 0.11%)      if (objectSize <= maxSegregatedObjectSize)
          .                   objectToFree = (FreeObject*)object;
          .               // "Fitting size" allocations are suspicious if aligned higher than naturally
          .               else {
        848 ( 0.00%)          if ( ! isAligned(object,2*fittingAlignment) )
          .                       // TODO: the above check is questionable - it gives false negatives in ~50% cases,
          .                       //       so might even be slower in average than unconditional use of findAllocatedObject.
          .                       // here it should be a "real" object
          .                       objectToFree = (FreeObject*)object;
          .                   else
          .                       // here object can be an aligned address, so applying additional checks
          .                       objectToFree = findAllocatedObject(object);
          .                   MALLOC_ASSERT( isAligned(objectToFree,fittingAlignment), ASSERT_TEXT );
-- line 1763 ----------------------------------------
-- line 1764 ----------------------------------------
          .               }
          .               MALLOC_ASSERT( isProperlyPlaced(objectToFree), ASSERT_TEXT );
          .           
          .               return objectToFree;
          .           }
          .           
          .           void TLSData::release()
          .           {
          1 ( 0.00%)      memPool->extMemPool.allLocalCaches.unregisterThread(this);
          .               externalCleanup(/*cleanOnlyUnused=*/false, /*cleanBins=*/false);
          .           
         90 ( 0.00%)      for (unsigned index = 0; index < numBlockBins; index++) {
          .                   Block *activeBlk = bin[index].getActiveBlock();
         58 ( 0.00%)          if (!activeBlk)
          .                       continue;
         21 ( 0.00%)          Block *threadlessBlock = activeBlk->previous;
         21 ( 0.00%)          bool syncOnMailbox = false;
         63 ( 0.00%)          while (threadlessBlock) {
          .                       Block *threadBlock = threadlessBlock->previous;
          .                       if (threadlessBlock->empty()) {
          .                           /* we destroy the thread, so not use its block pool */
          .                           memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
          .                       } else {
          .                           memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
          .                           syncOnMailbox = true;
          .                       }
          .                       threadlessBlock = threadBlock;
          .                   }
          .                   threadlessBlock = activeBlk;
         42 ( 0.00%)          while (threadlessBlock) {
         21 ( 0.00%)              Block *threadBlock = threadlessBlock->next;
          .                       if (threadlessBlock->empty()) {
          .                           /* we destroy the thread, so not use its block pool */
         54 ( 0.00%)                  memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
      7,414 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (18x)
          .                       } else {
         33 ( 0.00%)                  memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
        176 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::put(long, rml::internal::Block*) (3x)
          4 ( 0.00%)                  syncOnMailbox = true;
          .                       }
          .                       threadlessBlock = threadBlock;
          .                   }
          .                   bin[index].resetActiveBlock();
          .           
         42 ( 0.00%)          if (syncOnMailbox) {
          .                       // Although, we synchronized on nextPrivatizable inside a block, we still need to
          .                       // synchronize on the bin lifetime because the thread releasing an object into the public 
          .                       // free list is touching the bin (mailbox and mailLock)
          3 ( 0.00%)              MallocMutex::scoped_lock scoped_cs(bin[index].mailLock);
          .                   }
          .               }
          .           }
          .           
          .           
          .           #if MALLOC_CHECK_RECURSION
          .           // TODO: Use dedicated heap for this
          .           
-- line 1818 ----------------------------------------
-- line 1824 ----------------------------------------
          .            * allocations are performed by moving bump pointer and increasing of object counter,
          .            * releasing is done via counter of objects allocated in the block
          .            * or moving bump pointer if releasing object is on a bound.
          .            * TODO: make bump pointer to grow to the same backward direction as all the others.
          .            */
          .           
          .           class StartupBlock : public Block {
          .               size_t availableSize() const {
         24 ( 0.00%)          return slabSize - ((uintptr_t)bumpPtr - (uintptr_t)this);
          .               }
          .               static StartupBlock *getBlock();
          .           public:
          .               static FreeObject *allocate(size_t size);
          .               static size_t msize(void *ptr) { return *((size_t*)ptr - 1); }
          .               void free(void *ptr);
          .           };
          .           
          .           static MallocMutex startupMallocLock;
          .           static StartupBlock *firstStartupBlock;
          .           
          .           StartupBlock *StartupBlock::getBlock()
          .           {
          3 ( 0.00%)      BackRefIdx backRefIdx = BackRefIdx::newBackRef(/*largeObj=*/false);
         72 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (1x)
          2 ( 0.00%)      if (backRefIdx.isInvalid()) return nullptr;
          .           
          .               StartupBlock *block = static_cast<StartupBlock*>(
          2 ( 0.00%)          defaultMemPool->extMemPool.backend.getSlabBlock(1));
          2 ( 0.00%)      if (!block) return nullptr;
          .           
          .               block->cleanBlockHeader();
          .               setBackRef(backRefIdx, block);
          4 ( 0.00%)      block->backRefIdx = backRefIdx;
          .               // use startupAllocObjSizeMark to mark objects from startup block marker
          2 ( 0.00%)      block->objectSize = startupAllocObjSizeMark;
          3 ( 0.00%)      block->bumpPtr = (FreeObject *)((uintptr_t)block + sizeof(StartupBlock));
          .               return block;
          .           }
          .           
          .           FreeObject *StartupBlock::allocate(size_t size)
          .           {
          .               FreeObject *result;
          .               StartupBlock *newBlock = nullptr;
          .               bool newBlockUnused = false;
          .           
          .               /* Objects must be aligned on their natural bounds,
          .                  and objects bigger than word on word's bound. */
          .               size = alignUp(size, sizeof(size_t));
          .               // We need size of an object to implement msize.
          7 ( 0.00%)      size_t reqSize = size + sizeof(size_t);
          .               {
          .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
          .                   // Re-check whether we need a new block (conditions might have changed)
         33 ( 0.00%)          if (!firstStartupBlock || firstStartupBlock->availableSize() < reqSize) {
          .                       if (!newBlock) {
          .                           newBlock = StartupBlock::getBlock();
          .                           if (!newBlock) return nullptr;
          .                       }
          2 ( 0.00%)              newBlock->next = (Block*)firstStartupBlock;
          2 ( 0.00%)              if (firstStartupBlock)
          .                           firstStartupBlock->previous = (Block*)newBlock;
          1 ( 0.00%)              firstStartupBlock = newBlock;
          .                   }
          .                   result = firstStartupBlock->bumpPtr;
          7 ( 0.00%)          firstStartupBlock->allocatedCount++;
          .                   firstStartupBlock->bumpPtr =
         14 ( 0.00%)              (FreeObject *)((uintptr_t)firstStartupBlock->bumpPtr + reqSize);
          .               }
          .           
          .               // keep object size at the negative offset
          7 ( 0.00%)      *((size_t*)result) = size;
          7 ( 0.00%)      return (FreeObject*)((size_t*)result+1);
          .           }
          .           
          .           void StartupBlock::free(void *ptr)
          .           {
          .               Block* blockToRelease = nullptr;
          .               {
          .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
          .           
          .                   MALLOC_ASSERT(firstStartupBlock, ASSERT_TEXT);
          .                   MALLOC_ASSERT(startupAllocObjSizeMark==objectSize
          .                                 && allocatedCount>0, ASSERT_TEXT);
          .                   MALLOC_ASSERT((uintptr_t)ptr>=(uintptr_t)this+sizeof(StartupBlock)
          .                                 && (uintptr_t)ptr+StartupBlock::msize(ptr)<=(uintptr_t)this+slabSize,
          .                                 ASSERT_TEXT);
         12 ( 0.00%)          if (0 == --allocatedCount) {
          .                       if (this == firstStartupBlock)
          .                           firstStartupBlock = (StartupBlock*)firstStartupBlock->next;
          .                       if (previous)
          .                           previous->next = next;
          .                       if (next)
          .                           next->previous = previous;
          .                       blockToRelease = this;
         24 ( 0.00%)          } else if ((uintptr_t)ptr + StartupBlock::msize(ptr) == (uintptr_t)bumpPtr) {
          .                       // last object in the block released
          8 ( 0.00%)              FreeObject *newBump = (FreeObject*)((size_t*)ptr - 1);
          .                       MALLOC_ASSERT((uintptr_t)newBump>(uintptr_t)this+sizeof(StartupBlock),
          .                                     ASSERT_TEXT);
          .                       bumpPtr = newBump;
          .                   }
          .               }
          .               if (blockToRelease) {
          .                   blockToRelease->previous = blockToRelease->next = nullptr;
        217 ( 0.00%)          defaultMemPool->returnEmptyBlock(blockToRelease, /*poolTheBlock=*/false);
     56,649 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (217x)
          .               }
          .           }
          .           
          .           #endif /* MALLOC_CHECK_RECURSION */
          .           
          .           /********* End thread related code  *************/
          .           
          .           /********* Library initialization *************/
-- line 1935 ----------------------------------------
-- line 1997 ----------------------------------------
          .           
          .           inline bool isMallocInitialized() {
          .               // Load must have acquire fence; otherwise thread taking "initialized" path
          .               // might perform textually later loads *before* mallocInitialized becomes 2.
          .               return 2 == mallocInitialized.load(std::memory_order_acquire);
          .           }
          .           
          .           /* Caller is responsible for ensuring this routine is called exactly once. */
          6 ( 0.00%)  extern "C" void MallocInitializeITT() {
          .           #if __TBB_USE_ITT_NOTIFY
          .               if (!usedBySrcIncluded)
          .                   tbb::detail::r1::__TBB_load_ittnotify();
          .           #endif
          6 ( 0.00%)  }
          .           
          .           void MemoryPool::initDefaultPool() {
          .               hugePages.init();
          .           }
          .           
          .           /*
          .            * Allocator initialization routine;
          .            * it is called lazily on the very first scalable_malloc call.
-- line 2018 ----------------------------------------
-- line 2025 ----------------------------------------
          .               MALLOC_ASSERT( sizeof(FreeObject) == sizeof(void*), ASSERT_TEXT );
          .               MALLOC_ASSERT( isAligned(defaultMemPool, sizeof(intptr_t)),
          .                              "Memory pool must be void*-aligned for atomic to work over aligned arguments.");
          .           
          .           #if USE_WINTHREAD
          .               const size_t granularity = 64*1024; // granulatity of VirtualAlloc
          .           #else
          .               // POSIX.1-2001-compliant way to get page size
          7 ( 0.00%)      const size_t granularity = sysconf(_SC_PAGESIZE);
        829 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .           #endif
          3 ( 0.00%)      if (!defaultMemPool) {
          .                   // Do not rely on static constructors and do the assignment in case
          .                   // of library static section not initialized at this call yet.
          .                   defaultMemPool = (MemoryPool*)defaultMemPool_space;
          .               }
          .               bool initOk = defaultMemPool->
          1 ( 0.00%)          extMemPool.init(0, nullptr, nullptr, granularity,
          .                                   /*keepAllMemory=*/false, /*fixedPool=*/false);
          .           // TODO: extMemPool.init() to not allocate memory
          2 ( 0.00%)      if (!initOk || !initBackRefMain(&defaultMemPool->extMemPool.backend) || !ThreadId::init())
          .                   return false;
          .               MemoryPool::initDefaultPool();
          .               // init() is required iff initMemoryManager() is called
          .               // after mallocProcessShutdownNotification()
          .               shutdownSync.init();
          .           #if COLLECT_STATISTICS
          .               initStatisticsCollection();
          .           #endif
-- line 2052 ----------------------------------------
-- line 2056 ----------------------------------------
          .           static bool GetBoolEnvironmentVariable(const char* name) {
          .               return tbb::detail::r1::GetBoolEnvironmentVariable(name);
          .           }
          .           
          .           //! Ensures that initMemoryManager() is called once and only once.
          .           /** Does not return until initMemoryManager() has been completed by a thread.
          .               There is no need to call this routine if mallocInitialized==2 . */
          .           static bool doInitialization()
          8 ( 0.00%)  {
          .               MallocMutex::scoped_lock lock( initMutex );
          2 ( 0.00%)      if (mallocInitialized.load(std::memory_order_relaxed)!=2) {
          .                   MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==0, ASSERT_TEXT );
          .                   mallocInitialized.store(1, std::memory_order_relaxed);
          .                   RecursiveMallocCallProtector scoped;
          .                   if (!initMemoryManager()) {
          .                       mallocInitialized.store(0, std::memory_order_relaxed); // restore and out
          .                       return false;
          .                   }
          .           #ifdef  MALLOC_EXTRA_INITIALIZATION
-- line 2074 ----------------------------------------
-- line 2084 ----------------------------------------
          .                   mallocInitialized.store(2, std::memory_order_release);
          .                   if( GetBoolEnvironmentVariable("TBB_VERSION") ) {
          .                       fputs(VersionString+1,stderr);
          .                       hugePages.printStatus();
          .                   }
          .               }
          .               /* It can't be 0 or I would have initialized it */
          .               MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==2, ASSERT_TEXT );
          3 ( 0.00%)      return true;
          9 ( 0.00%)  }
          .           
          .           /********* End library initialization *************/
          .           
          .           /********* The malloc show begins     *************/
          .           
          .           
          .           FreeObject *Block::allocateFromFreeList()
          .           {
          .               FreeObject *result;
          .           
 75,725,607 ( 0.20%)      if (!freeList) return nullptr;
          .           
          .               result = freeList;
          .               MALLOC_ASSERT( result, ASSERT_TEXT );
          .           
 28,179,826 ( 0.07%)      freeList = result->next;
          .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
 14,089,913 ( 0.04%)      allocatedCount++;
          .               STAT_increment(getThreadId(), getIndex(objectSize), allocFreeListUsed);
          .           
          .               return result;
          .           }
          .           
          .           FreeObject *Block::allocateFromBumpPtr()
          .           {
 11,151,956 ( 0.03%)      FreeObject *result = bumpPtr;
 22,303,912 ( 0.06%)      if (result) {
 77,949,613 ( 0.20%)          bumpPtr = (FreeObject *) ((uintptr_t) bumpPtr - objectSize);
 11,135,659 ( 0.03%)          if ( (uintptr_t)bumpPtr < (uintptr_t)this+sizeof(Block) ) {
          .                       bumpPtr = nullptr;
          .                   }
          .                   MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
 11,135,659 ( 0.03%)          allocatedCount++;
          .                   STAT_increment(getThreadId(), getIndex(objectSize), allocBumpPtrUsed);
          .               }
 11,135,659 ( 0.03%)      return result;
          .           }
          .           
          .           inline FreeObject* Block::allocate()
          .           {
          .               MALLOC_ASSERT( isOwnedByCurrentThread(), ASSERT_TEXT );
          .           
          .               /* for better cache locality, first looking in the free list. */
          .               if ( FreeObject *result = allocateFromFreeList() ) {
-- line 2137 ----------------------------------------
-- line 2141 ----------------------------------------
          .           
          .               /* if free list is empty, try thread local bump pointer allocation. */
          .               if ( FreeObject *result = allocateFromBumpPtr() ) {
          .                   return result;
          .               }
          .               MALLOC_ASSERT( !bumpPtr, ASSERT_TEXT );
          .           
          .               /* the block is considered full. */
     16,297 ( 0.00%)      isFull = true;
          .               return nullptr;
          .           }
          .           
          .           size_t Block::findObjectSize(void *object) const
          .           {
          .               size_t blSize = getSize();
          .           #if MALLOC_CHECK_RECURSION
          .               // Currently, there is no aligned allocations from startup blocks,
-- line 2157 ----------------------------------------
-- line 2165 ----------------------------------------
          .                   blSize - ((uintptr_t)object - (uintptr_t)findObjectToFree(object));
          .               MALLOC_ASSERT(size>0 && size<minLargeObjectSize, ASSERT_TEXT);
          .               return size;
          .           }
          .           
          .           void Bin::moveBlockToFront(Block *block)
          .           {
          .               /* move the block to the front of the bin */
     48,891 ( 0.00%)      if (block == activeBlk) return;
          .               outofTLSBin(block);
          .               pushTLSBin(block);
          .           }
          .           
          .           void Bin::processEmptyBlock(Block *block, bool poolTheBlock)
          .           {
 33,318,540 ( 0.09%)      if (block != activeBlk) {
          .                   /* We are not using this block; return it to the pool */
          .                   outofTLSBin(block);
        651 ( 0.00%)          block->getMemPool()->returnEmptyBlock(block, poolTheBlock);
          .               } else {
          .                   /* all objects are free - let's restore the bump pointer */
          .                   block->restoreBumpPtr();
          .               }
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::put(LargeMemoryBlock *object, ExtMemoryPool *extMemPool)
          .           {
      2,912 ( 0.00%)      const size_t size = object->unalignedSize;
          .               // not spoil cache with too large object, that can cause its total cleanup
      5,824 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
          .                   return false;
          .               LargeMemoryBlock *localHead = head.exchange(nullptr);
          .           
      2,912 ( 0.00%)      object->prev = nullptr;
      2,912 ( 0.00%)      object->next = localHead;
      5,824 ( 0.00%)      if (localHead)
      2,911 ( 0.00%)          localHead->prev = object;
          .               else {
          .                   // those might not be cleaned during local cache stealing, correct them
          .                   totalSize = 0;
          .                   numOfBlocks = 0;
          1 ( 0.00%)          tail = object;
          .               }
          .               localHead = object;
      5,823 ( 0.00%)      totalSize += size;
      8,735 ( 0.00%)      numOfBlocks++;
          .               // must meet both size and number of cached objects constrains
     14,554 ( 0.00%)      if (totalSize > MAX_TOTAL_SIZE || numOfBlocks >= HIGH_MARK) {
          .                   // scanning from tail until meet conditions
        100 ( 0.00%)          while (totalSize > MAX_TOTAL_SIZE || numOfBlocks > LOW_MARK) {
         48 ( 0.00%)              totalSize -= tail->unalignedSize;
         48 ( 0.00%)              numOfBlocks--;
         48 ( 0.00%)              tail = tail->prev;
          .                   }
          3 ( 0.00%)          LargeMemoryBlock *headToRelease = tail->next;
          1 ( 0.00%)          tail->next = nullptr;
          .           
          .                   extMemPool->freeLargeObjectList(headToRelease);
          .               }
          .           
          .               head.store(localHead, std::memory_order_release);
          .               return true;
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           LargeMemoryBlock *LocalLOCImpl<LOW_MARK, HIGH_MARK>::get(size_t size)
          .           {
          .               LargeMemoryBlock *localHead, *res = nullptr;
          .           
      5,826 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
          .                   return nullptr;
          .           
          .               // TBB_REVAMP_TODO: review this line
     17,462 ( 0.00%)      if (!head.load(std::memory_order_acquire) || (localHead = head.exchange(nullptr)) == nullptr) {
          .                   // do not restore totalSize, numOfBlocks and tail at this point,
          .                   // as they are used only in put(), where they must be restored
          .                   return nullptr;
          .               }
          .           
      3,100 ( 0.00%)      for (LargeMemoryBlock *curr = localHead; curr; curr=curr->next) {
     17,688 ( 0.00%)          if (curr->unalignedSize == size) {
          .                       res = curr;
     10,166 ( 0.00%)              if (curr->next)
      5,737 ( 0.00%)                  curr->next->prev = curr->prev;
          .                       else
         14 ( 0.00%)                  tail = curr->prev;
      5,744 ( 0.00%)              if (curr != localHead)
      2,162 ( 0.00%)                  curr->prev->next = curr->next;
          .                       else
          .                           localHead = curr->next;
      2,872 ( 0.00%)              totalSize -= size;
      2,872 ( 0.00%)              numOfBlocks--;
          .                       break;
          .                   }
          .               }
          .           
          .               head.store(localHead, std::memory_order_release);
          .               return res;
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::externalCleanup(ExtMemoryPool *extMemPool)
          .           {
          2 ( 0.00%)      if (LargeMemoryBlock *localHead = head.exchange(nullptr)) {
          .                   extMemPool->freeLargeObjectList(localHead);
          .                   return true;
          .               }
          .               return false;
          .           }
          .           
          .           void *MemoryPool::getFromLLOCache(TLSData* tls, size_t size, size_t alignment)
     32,043 ( 0.00%)  {
          .               LargeMemoryBlock *lmb = nullptr;
          .           
          .               size_t headersSize = sizeof(LargeMemoryBlock)+sizeof(LargeObjectHdr);
      2,913 ( 0.00%)      size_t allocationSize = LargeObjectCache::alignToBin(size+headersSize+alignment);
      5,826 ( 0.00%)      if (allocationSize < size) // allocationSize is wrapped around after alignToBin
          .                   return nullptr;
          .               MALLOC_ASSERT(allocationSize >= alignment, "Overflow must be checked before.");
          .           
      5,826 ( 0.00%)      if (tls) {
          .                   tls->markUsed();
          .                   lmb = tls->lloc.get(allocationSize);
          .               }
          .               if (!lmb)
          .                   lmb = extMemPool.mallocLargeObject(this, allocationSize);
          .           
          .               if (lmb) {
          .                   // doing shuffle we suppose that alignment offset guarantees
          .                   // that different cache lines are in use
          .                   MALLOC_ASSERT(alignment >= estimatedCacheLineSize, ASSERT_TEXT);
          .           
          .                   void *alignedArea = (void*)alignUp((uintptr_t)lmb+headersSize, alignment);
          .                   uintptr_t alignedRight =
     11,570 ( 0.00%)              alignDown((uintptr_t)lmb+lmb->unalignedSize - size, alignment);
          .                   // Has some room to shuffle object between cache lines?
          .                   // Note that alignedRight and alignedArea are aligned at alignment.
          .                   unsigned ptrDelta = alignedRight - (uintptr_t)alignedArea;
     11,652 ( 0.00%)          if (ptrDelta && tls) { // !tls is cold path
          .                       // for the hot path of alignment==estimatedCacheLineSize,
          .                       // allow compilers to use shift for division
          .                       // (since estimatedCacheLineSize is a power-of-2 constant)
     11,652 ( 0.00%)              unsigned numOfPossibleOffsets = alignment == estimatedCacheLineSize?
          .                             ptrDelta / estimatedCacheLineSize :
          .                             ptrDelta / alignment;
      8,739 ( 0.00%)              unsigned myCacheIdx = ++tls->currCacheIdx;
          .                       unsigned offset = myCacheIdx % numOfPossibleOffsets;
          .           
          .                       // Move object to a cache line with an offset that is different from
          .                       // previous allocation. This supposedly allows us to use cache
          .                       // associativity more efficiently.
     11,652 ( 0.00%)              alignedArea = (void*)((uintptr_t)alignedArea + offset*alignment);
          .                   }
          .                   MALLOC_ASSERT((uintptr_t)lmb+lmb->unalignedSize >=
          .                                 (uintptr_t)alignedArea+size, "Object doesn't fit the block.");
      2,913 ( 0.00%)          LargeObjectHdr *header = (LargeObjectHdr*)alignedArea-1;
      2,913 ( 0.00%)          header->memoryBlock = lmb;
     11,652 ( 0.00%)          header->backRefIdx = lmb->backRefIdx;
          .                   setBackRef(header->backRefIdx, header);
          .           
      2,913 ( 0.00%)          lmb->objectSize = size;
          .           
          .                   MALLOC_ASSERT( isLargeObject<unknownMem>(alignedArea), ASSERT_TEXT );
          .                   MALLOC_ASSERT( isAligned(alignedArea, alignment), ASSERT_TEXT );
          .           
          .                   return alignedArea;
          .               }
          .               return nullptr;
     23,304 ( 0.00%)  }
          .           
          .           void MemoryPool::putToLLOCache(TLSData *tls, void *object)
      8,736 ( 0.00%)  {
          .               LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
          .               // overwrite backRefIdx to simplify double free detection
      8,736 ( 0.00%)      header->backRefIdx = BackRefIdx();
          .           
      5,824 ( 0.00%)      if (tls) {
          .                   tls->markUsed();
      2,912 ( 0.00%)          if (tls->lloc.put(header->memoryBlock, &extMemPool))
          .                       return;
          .               }
          .               extMemPool.freeLargeObject(header->memoryBlock);
     11,648 ( 0.00%)  }
          .           
          .           /*
          .            * All aligned allocations fall into one of the following categories:
          .            *  1. if both request size and alignment are <= maxSegregatedObjectSize,
          .            *       we just align the size up, and request this amount, because for every size
          .            *       aligned to some power of 2, the allocated object is at least that aligned.
          .            * 2. for size<minLargeObjectSize, check if already guaranteed fittingAlignment is enough.
          .            * 3. if size+alignment<minLargeObjectSize, we take an object of fittingSizeN and align
-- line 2356 ----------------------------------------
-- line 2453 ----------------------------------------
          .               return 0 == ((uintptr_t)this + slabSize - (uintptr_t)object) % objectSize;
          .           }
          .           #endif
          .           
          .           /* Finds the real object inside the block */
          .           FreeObject *Block::findAllocatedObject(const void *address) const
          .           {
          .               // calculate offset from the end of the block space
      1,272 ( 0.00%)      uint16_t offset = (uintptr_t)this + slabSize - (uintptr_t)address;
          .               MALLOC_ASSERT( offset<=slabSize-sizeof(Block), ASSERT_TEXT );
          .               // find offset difference from a multiple of allocation size
        848 ( 0.00%)      offset %= objectSize;
          .               // and move the address down to where the real object starts.
        848 ( 0.00%)      return (FreeObject*)((uintptr_t)address - (offset? objectSize-offset: 0));
          .           }
          .           
          .           /*
          .            * Bad dereference caused by a foreign pointer is possible only here, not earlier in call chain.
          .            * Separate function isolates SEH code, as it has bad influence on compiler optimization.
          .            */
          .           static inline BackRefIdx safer_dereference (const BackRefIdx *ptr)
          .           {
-- line 2474 ----------------------------------------
-- line 2478 ----------------------------------------
          .           #endif
          .                   id = dereference(ptr);
          .           #if _MSC_VER
          .               } __except( GetExceptionCode() == EXCEPTION_ACCESS_VIOLATION?
          .                           EXCEPTION_EXECUTE_HANDLER : EXCEPTION_CONTINUE_SEARCH ) {
          .                   id = BackRefIdx();
          .               }
          .           #endif
126,127,875 ( 0.33%)      return id;
          .           }
          .           
          .           template<MemoryOrigin memOrigin>
          .           bool isLargeObject(void *object)
          .           {
 54,424,419 ( 0.14%)      if (!isAligned(object, largeObjectAlignment))
 47,700,268 ( 0.12%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] (3,967,445x)
          .                   return false;
      3,056 ( 0.00%)      LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
          .               BackRefIdx idx = (memOrigin == unknownMem) ?
          .                   safer_dereference(&header->backRefIdx) : dereference(&header->backRefIdx);
          .           
          .               return idx.isLargeObject()
          .                   // in valid LargeObjectHdr memoryBlock is not nullptr
      9,168 ( 0.00%)          && header->memoryBlock
          .                   // in valid LargeObjectHdr memoryBlock points somewhere before header
          .                   // TODO: more strict check
      6,112 ( 0.00%)          && (uintptr_t)header->memoryBlock < (uintptr_t)header
  7,953,146 ( 0.02%)          && getBackRef(idx) == header;
     54,336 ( 0.00%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (3,040x)
  3,967,445 ( 0.01%)  }
          .           
          .           static inline bool isSmallObject (void *ptr)
          .           {
          .               Block* expectedBlock = (Block*)alignDown(ptr, slabSize);
          .               const BackRefIdx* idx = expectedBlock->getBackRefIdx();
          .           
 25,225,575 ( 0.07%)      bool isSmall = expectedBlock == getBackRef(safer_dereference(idx));
454,060,350 ( 1.19%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (25,225,575x)
          .               if (isSmall)
          .                   expectedBlock->checkFreePrecond(ptr);
          .               return isSmall;
          .           }
          .           
          .           /**** Check if an object was allocated by scalable_malloc ****/
          .           static inline bool isRecognized (void* ptr)
          .           {
          .               return defaultMemPool->extMemPool.backend.ptrCanBeValid(ptr) &&
          .                   (isLargeObject<unknownMem>(ptr) || isSmallObject(ptr));
          .           }
          .           
          .           static inline void freeSmallObject(void *object)
151,353,450 ( 0.40%)  {
          .               /* mask low bits to get the block */
          .               Block *block = (Block *)alignDown(object, slabSize);
          .               block->checkFreePrecond(object);
          .           
          .           #if MALLOC_CHECK_RECURSION
 50,451,150 ( 0.13%)      if (block->isStartupAllocObject()) {
          .                   ((StartupBlock *)block)->free(object);
          .                   return;
          .               }
          .           #endif
          .               if (block->isOwnedByCurrentThread()) {
          .                   block->freeOwnObject(object);
          .               } else { /* Slower path to add to the shared list, the allocatedCount is updated by the owner thread in malloc. */
          .                   FreeObject *objectToFree = block->findObjectToFree(object);
          .                   block->freePublicObject(objectToFree);
          .               }
137,233,844 ( 0.36%)  }
          .           
          .           static void *internalPoolMalloc(MemoryPool* memPool, size_t size)
227,056,365 ( 0.59%)  {
          .               Bin* bin;
          .               Block * mallocBlock;
          .           
 50,456,970 ( 0.13%)      if (!memPool) return nullptr;
          .           
 50,456,970 ( 0.13%)      if (!size) size = sizeof(size_t);
          .           
          .               TLSData *tls = memPool->getTLS(/*create=*/true);
          .           
          .               /* Allocate a large object */
 50,456,970 ( 0.13%)      if (size >= minLargeObjectSize)
     14,565 ( 0.00%)          return memPool->getFromLLOCache(tls, size, largeObjectAlignment);
    312,975 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getFromLLOCache(rml::internal::TLSData*, unsigned long, unsigned long) (2,913x)
          .           
          2 ( 0.00%)      if (!tls) return nullptr;
          .           
          .               tls->markUsed();
          .               /*
          .                * Get an element in thread-local array corresponding to the given size;
          .                * It keeps ptr to the active block for allocations of this size
          .                */
          .               bin = tls->getAllocationBin(size);
          .               if ( !bin ) return nullptr;
          .           
          .               /* Get a block to try to allocate in. */
 75,692,775 ( 0.20%)      for( mallocBlock = bin->getActiveBlock(); mallocBlock;
          .                    mallocBlock = bin->setPreviousBlockActive() ) // the previous block should be empty enough
          .               {
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .               }
          .           
          .               /*
          .                * else privatize publicly freed objects in some block and allocate from it
-- line 2579 ----------------------------------------
-- line 2586 ----------------------------------------
          .                   /* Else something strange happened, need to retry from the beginning; */
          .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in public free list; reentering.\n" ));
          .                   return internalPoolMalloc(memPool, size);
          .               }
          .           
          .               /*
          .                * no suitable own blocks, try to get a partial block that some other thread has discarded.
          .                */
      1,428 ( 0.00%)      mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
      8,760 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::get(rml::internal::TLSData*, unsigned int) (238x)
        476 ( 0.00%)      while (mallocBlock) {
          .                   bin->pushTLSBin(mallocBlock);
          .                   bin->setActiveBlock(mallocBlock); // TODO: move under the below condition?
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .                   mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
          .               }
          .           
          .               /*
          .                * else try to get a new empty block
          .                */
        714 ( 0.00%)      mallocBlock = memPool->getEmptyBlock(size);
     82,503 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (238x)
        476 ( 0.00%)      if (mallocBlock) {
          .                   bin->pushTLSBin(mallocBlock);
          .                   bin->setActiveBlock(mallocBlock);
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .                   /* Else something strange happened, need to retry from the beginning; */
          .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in empty block; reentering.\n" ));
          .                   return internalPoolMalloc(memPool, size);
          .               }
          .               /*
          .                * else nothing works so return nullptr
          .                */
          .               TRACEF(( "[ScalableMalloc trace] No memory found, returning nullptr.\n" ));
          .               return nullptr;
227,050,539 ( 0.59%)  }
          .           
          .           // When size==0 (i.e. unknown), detect here whether the object is large.
          .           // For size is known and < minLargeObjectSize, we still need to check
          .           // if the actual object is large, because large objects might be used
          .           // for aligned small allocations.
          .           static bool internalPoolFree(MemoryPool *memPool, void *object, size_t size)
          .           {
          .               if (!memPool || !object) return false;
-- line 2629 ----------------------------------------
-- line 2637 ----------------------------------------
          .               if (size >= minLargeObjectSize || isLargeObject<ourMem>(object))
          .                   memPool->putToLLOCache(memPool->getTLS(/*create=*/false), object);
          .               else
          .                   freeSmallObject(object);
          .               return true;
          .           }
          .           
          .           static void *internalMalloc(size_t size)
151,370,952 ( 0.40%)  {
 75,685,476 ( 0.20%)      if (!size) size = sizeof(size_t);
          .           
          .           #if MALLOC_CHECK_RECURSION
          .               if (RecursiveMallocCallProtector::sameThreadActive())
         14 ( 0.00%)          return size<minLargeObjectSize? StartupBlock::allocate(size) :
          .                       // nested allocation, so skip tls
          .                       (FreeObject*)defaultMemPool->getFromLLOCache(nullptr, size, slabSize);
          .           #endif
          .           
 50,456,970 ( 0.13%)      if (!isMallocInitialized())
          3 ( 0.00%)          if (!doInitialization())
    142,905 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::doInitialization() (1x)
          .                       return nullptr;
 75,685,455 ( 0.20%)      return internalPoolMalloc(defaultMemPool, size);
2,004,092,481 ( 5.24%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) (25,228,485x)
126,142,467 ( 0.33%)  }
          .           
          .           static void internalFree(void *object)
          .           {
          .               internalPoolFree(defaultMemPool, object, 0);
          .           }
          .           
          .           static size_t internalMsize(void* ptr)
          .           {
-- line 2667 ----------------------------------------
-- line 2840 ----------------------------------------
          .            * unless that value is nullptr.
          .            * For Windows, it is called from DllMain( DLL_THREAD_DETACH ).
          .            *
          .            * However neither of the above is called for the main process thread, so the routine
          .            * also needs to be called during the process shutdown.
          .            *
          .           */
          .           // TODO: Consider making this function part of class MemoryPool.
          4 ( 0.00%)  void doThreadShutdownNotification(TLSData* tls, bool main_thread)
          .           {
          .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return start %d\n",
          .                        getThreadId(),  threadGoingDownCount++ ));
          .           
          .           #if USE_PTHREAD
          .               if (tls) {
          .                   if (!shutdownSync.threadDtorStart()) return;
          .                   tls->getMemPool()->onThreadShutdown(tls);
-- line 2856 ----------------------------------------
-- line 2858 ----------------------------------------
          .               } else
          .           #endif
          .               {
          .                   suppress_unused_warning(tls); // not used on Windows
          .                   // The default pool is safe to use at this point:
          .                   //   on Linux, only the main thread can go here before destroying defaultMemPool;
          .                   //   on Windows, shutdown is synchronized via loader lock and isMallocInitialized().
          .                   // See also __TBB_mallocProcessShutdownNotification()
          1 ( 0.00%)          defaultMemPool->onThreadShutdown(defaultMemPool->getTLS(/*create=*/false));
          .                   // Take lock to walk through other pools; but waiting might be dangerous at this point
          .                   // (e.g. on Windows the main thread might deadlock)
          .                   bool locked;
          .                   MallocMutex::scoped_lock lock(MemoryPool::memPoolListLock, /*wait=*/!main_thread, &locked);
          3 ( 0.00%)          if (locked) { // the list is safe to process
          4 ( 0.00%)              for (MemoryPool *memPool = defaultMemPool->next; memPool; memPool = memPool->next)
          .                           memPool->onThreadShutdown(memPool->getTLS(/*create=*/false));
          .                   }
          .               }
          .           
          .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return end\n", getThreadId() ));
          4 ( 0.00%)  }
          .           
          .           #if USE_PTHREAD
          .           void mallocThreadShutdownNotification(void* arg)
          .           {
          .               // The routine is called for each pool (as TLS dtor) on each thread, except for the main thread
          .               if (!isMallocInitialized()) return;
          .               doThreadShutdownNotification((TLSData*)arg, false);
          .           }
-- line 2886 ----------------------------------------
-- line 2890 ----------------------------------------
          .               // The routine is called once per thread on Windows
          .               if (!isMallocInitialized()) return;
          .               doThreadShutdownNotification(nullptr, false);
          .           }
          .           #endif
          .           
          .           extern "C" void __TBB_mallocProcessShutdownNotification(bool windows_process_dying)
          .           {
          2 ( 0.00%)      if (!isMallocInitialized()) return;
          .           
          .               // Don't clean allocator internals if the entire process is exiting
          .               if (!windows_process_dying) {
          .                   doThreadShutdownNotification(nullptr, /*main_thread=*/true);
          .               }
          .           #if  __TBB_MALLOC_LOCACHE_STAT
          .               printf("cache hit ratio %f, size hit %f\n",
          .                      1.*cacheHits/mallocCalls, 1.*memHitKB/memAllocKB);
-- line 2906 ----------------------------------------
-- line 2925 ----------------------------------------
          .               for( int i=1; i<=nThreads && i<MAX_THREADS; ++i )
          .                   STAT_print(i);
          .           #endif
          .               if (!usedBySrcIncluded)
          .                   MALLOC_ITT_FINI_ITTLIB();
          .           }
          .           
          .           extern "C" void * scalable_malloc(size_t size)
 25,228,490 ( 0.07%)  {
 50,456,980 ( 0.13%)      void *ptr = internalMalloc(size);
2,584,343,439 ( 6.76%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (25,228,484x)
      1,107 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long)'2 (6x)
 50,456,980 ( 0.13%)      if (!ptr) errno = ENOMEM;
          .               return ptr;
 75,685,470 ( 0.20%)  }
          .           
          .           extern "C" void scalable_free(void *object)
          .           {
          .               internalFree(object);
          .           }
          .           
          .           #if MALLOC_ZONE_OVERLOAD_ENABLED
          .           extern "C" void __TBB_malloc_free_definite_size(void *object, size_t size)
-- line 2945 ----------------------------------------
-- line 2948 ----------------------------------------
          .           }
          .           #endif
          .           
          .           /*
          .            * A variant that provides additional memory safety, by checking whether the given address
          .            * was obtained with this allocator, and if not redirecting to the provided alternative call.
          .            */
          .           extern "C" TBBMALLOC_EXPORT void __TBB_malloc_safer_free(void *object, void (*original_free)(void*))
 25,228,487 ( 0.07%)  {
 50,456,974 ( 0.13%)      if (!object)
          .                   return;
          .           
          .               // tbbmalloc can allocate object only when tbbmalloc has been initialized
 75,685,461 ( 0.20%)      if (mallocInitialized.load(std::memory_order_acquire) && defaultMemPool->extMemPool.backend.ptrCanBeValid(object)) {
  7,934,890 ( 0.02%)          if (isLargeObject<unknownMem>(object)) {
          .                       // must check 1st for large object, because small object check touches 4 pages on left,
          .                       // and it can be inaccessible
          .                       TLSData *tls = defaultMemPool->getTLS(/*create=*/false);
          .           
      8,736 ( 0.00%)              defaultMemPool->putToLLOCache(tls, object);
    111,695 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::putToLLOCache(rml::internal::TLSData*, void*) (2,912x)
          .                       return;
 50,451,150 ( 0.13%)          } else if (isSmallObject(object)) {
 50,451,150 ( 0.13%)              freeSmallObject(object);
1,221,177,169 ( 3.19%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] (25,225,575x)
          .                       return;
          .                   }
          .               }
          .               if (original_free)
          .                   original_free(object);
 25,228,487 ( 0.07%)  }
          .           
          .           /********* End the free code        *************/
          .           
          .           /********* Code for scalable_realloc       ***********/
          .           
          .           /*
          .            * From K&R
          .            * "realloc changes the size of the object pointed to by p to size. The contents will
-- line 2984 ----------------------------------------
-- line 3058 ----------------------------------------
          .            * From K&R
          .            * calloc returns a pointer to space for an array of nobj objects,
          .            * each of size size, or nullptr if the request cannot be satisfied.
          .            * The space is initialized to zero bytes.
          .            *
          .            */
          .           
          .           extern "C" void * scalable_calloc(size_t nobj, size_t size)
          6 ( 0.00%)  {
          .               // it's square root of maximal size_t value
          .               const size_t mult_not_overflow = size_t(1) << (sizeof(size_t)*CHAR_BIT/2);
          4 ( 0.00%)      const size_t arraySize = nobj * size;
          .           
          .               // check for overflow during multiplication:
         10 ( 0.00%)      if (nobj>=mult_not_overflow || size>=mult_not_overflow) // 1) heuristic check
          4 ( 0.00%)          if (nobj && arraySize / nobj != size) {             // 2) exact check
          .                       errno = ENOMEM;
          .                       return nullptr;
          .                   }
          6 ( 0.00%)      void* result = internalMalloc(arraySize);
    147,105 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (1x)
          4 ( 0.00%)      if (result)
         10 ( 0.00%)          memset(result, 0, arraySize);
         11 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (1x)
          .               else
          .                   errno = ENOMEM;
          .               return result;
         10 ( 0.00%)  }
          .           
          .           /********* End code for scalable_calloc   ***********/
          .           
          .           /********* Code for aligned allocation API **********/
          .           
          .           extern "C" int scalable_posix_memalign(void **memptr, size_t alignment, size_t size)
          .           {
          .               if ( !isPowerOfTwoAtLeast(alignment, sizeof(void*)) )
-- line 3091 ----------------------------------------

--------------------------------------------------------------------------------
The following files chosen for auto-annotation could not be found:
--------------------------------------------------------------------------------
  ./nptl/pthread_getspecific.c
  ./nptl/pthread_self.c
  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S

--------------------------------------------------------------------------------
Ir                      
--------------------------------------------------------------------------------
37,242,267,228 (97.40%)  events annotated

