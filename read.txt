--------------------------------------------------------------------------------
Profile data file 'callgrind.out.102726' (creator: callgrind-3.16.1)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 5541854860
Trigger: Program termination
Profiled target:  ./build/nptest -r -m 4 ../real-time-task-generators-main/proper testsets/c4_slow.csv --por=priority (PID 102726, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
Ir                      
--------------------------------------------------------------------------------
20,052,699,742 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
Ir                      file:function
--------------------------------------------------------------------------------
9,073,391,549 (45.25%)  ???:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
2,036,130,982 (10.15%)  ???:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
1,546,399,797 ( 7.71%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
  954,914,642 ( 4.76%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] [/usr/local/lib/libtbbmalloc.so.2.9]
  749,206,900 ( 3.74%)  ???:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  589,205,691 ( 2.94%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free [/usr/local/lib/libtbbmalloc.so.2.9]
  576,312,924 ( 2.87%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
  515,657,678 ( 2.57%)  ./nptl/pthread_getspecific.c:pthread_getspecific [/usr/lib/x86_64-linux-gnu/libpthread-2.31.so]
  354,064,962 ( 1.77%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) [/usr/local/lib/libtbbmalloc.so.2.9]
  303,322,500 ( 1.51%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator delete(void*) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
  286,068,688 ( 1.43%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
  273,687,159 ( 1.36%)  ???:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  272,996,478 ( 1.36%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::getBackRef(rml::internal::BackRefIdx)
  272,994,435 ( 1.36%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) [/usr/local/lib/libtbbmalloc.so.2.9]
  242,658,112 ( 1.21%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc [/usr/local/lib/libtbbmalloc.so.2.9]
  242,658,000 ( 1.21%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
  187,943,590 ( 0.94%)  ???:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  178,105,534 ( 0.89%)  ???:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  170,101,497 ( 0.85%)  ???:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, long long>, true>*, unsigned long) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  160,650,437 ( 0.80%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::freeSmallObject(void*) [clone .lto_priv.0]
  151,661,305 ( 0.76%)  /usr/include/c++/10/bits/atomic_base.h:__TBB_malloc_safer_free
  121,329,044 ( 0.61%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.h:__TBB_malloc_safer_free
  100,005,862 ( 0.50%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) [/usr/local/lib/libtbbmalloc.so.2.9]
   81,403,793 ( 0.41%)  ???:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   60,664,868 ( 0.30%)  ./nptl/pthread_self.c:pthread_self [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
   60,664,518 ( 0.30%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:rml::internal::internalMalloc(unsigned long)
   60,664,518 ( 0.30%)  /usr/include/c++/10/bits/atomic_base.h:rml::internal::internalMalloc(unsigned long)
   60,664,482 ( 0.30%)  ???:operator delete(void*, unsigned long) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
   60,664,370 ( 0.30%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:__TBB_malloc_safer_free
   60,664,370 ( 0.30%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:rml::internal::freeSmallObject(void*) [clone .lto_priv.0]
   44,153,054 ( 0.22%)  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_erms [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
   38,678,652 ( 0.19%)  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] [/usr/local/lib/libtbbmalloc.so.2.9]

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 34 ----------------------------------------
          .               std::atomic<int> allocatedCount; // the number of objects allocated
          .               BackRefIdx::main_t myNum;   // the index in the main
          .               MallocMutex   blockMutex;
          .               // true if this block has been added to the listForUse chain,
          .               // modifications protected by mainMutex
          .               std::atomic<bool> addedToForUse;
          .           
          .               BackRefBlock(const BackRefBlock *blockToUse, intptr_t num) :
          8 ( 0.00%)          nextForUse(nullptr), bumpPtr((FreeObject*)((uintptr_t)blockToUse + slabSize - sizeof(void*))),
          .                   freeList(nullptr), nextRawMemBlock(nullptr), allocatedCount(0), myNum(num),
         16 ( 0.00%)          addedToForUse(false) {
          4 ( 0.00%)          memset(&blockMutex, 0, sizeof(MallocMutex));
          .           
          .                   MALLOC_ASSERT(!(num >> CHAR_BIT*sizeof(BackRefIdx::main_t)),
          .                                 "index in BackRefMain must fit to BackRefIdx::main");
          .               }
          .               // clean all but header
         24 ( 0.00%)      void zeroSet() { memset(this+1, 0, BackRefBlock::bytes-sizeof(BackRefBlock)); }
     49,008 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (3x)
     17,195 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .               static const int bytes = slabSize;
          .           };
          .           
          .           // max number of backreference pointers in slab block
          .           static const int BR_MAX_CNT = (BackRefBlock::bytes-sizeof(BackRefBlock))/sizeof(void*);
          .           
          .           struct BackRefMain {
          .           /* On 64-bit systems a slab block can hold up to ~2K back pointers to slab blocks
-- line 59 ----------------------------------------
-- line 92 ----------------------------------------
          .           
          .           static MallocMutex mainMutex;
          .           static std::atomic<BackRefMain*> backRefMain;
          .           
          .           bool initBackRefMain(Backend *backend)
          .           {
          .               bool rawMemUsed;
          .               BackRefMain *main =
          6 ( 0.00%)          (BackRefMain*)backend->getBackRefSpace(BackRefMain::mainSize,
      1,817 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::getBackRefSpace(unsigned long, bool*) (1x)
          .                                                            &rawMemUsed);
          2 ( 0.00%)      if (! main)
          .                   return false;
          1 ( 0.00%)      main->backend = backend;
          .               main->listForUse.store(nullptr, std::memory_order_relaxed);
          1 ( 0.00%)      main->allRawMemBlocks = nullptr;
          2 ( 0.00%)      main->rawMemUsed = rawMemUsed;
          .               main->lastUsed = -1;
          1 ( 0.00%)      memset(&main->requestNewSpaceMutex, 0, sizeof(MallocMutex));
         19 ( 0.00%)      for (int i=0; i<BackRefMain::leaves; i++) {
          .                   BackRefBlock *bl = (BackRefBlock*)((uintptr_t)main + BackRefMain::bytes + i*BackRefBlock::bytes);
          .                   bl->zeroSet();
         12 ( 0.00%)          main->initEmptyBackRefBlock(bl);
         56 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefMain::initEmptyBackRefBlock(rml::internal::BackRefBlock*) (4x)
          8 ( 0.00%)          if (i)
          .                       main->addToForUseList(bl);
          .                   else // active leaf is not needed in listForUse
          .                       main->active.store(bl, std::memory_order_relaxed);
          .               }
          .               // backRefMain is read in getBackRef, so publish it in consistent state
          .               backRefMain.store(main, std::memory_order_release);
          .               return true;
          .           }
-- line 122 ----------------------------------------
-- line 135 ----------------------------------------
          .                   backend->putBackRefSpace(backRefMain.load(std::memory_order_relaxed), BackRefMain::mainSize,
          .                                            backRefMain.load(std::memory_order_relaxed)->rawMemUsed);
          .               }
          .           }
          .           #endif
          .           
          .           void BackRefMain::addToForUseList(BackRefBlock *bl)
          .           {
          3 ( 0.00%)      bl->nextForUse = listForUse.load(std::memory_order_relaxed);
          .               listForUse.store(bl, std::memory_order_relaxed);
          .               bl->addedToForUse.store(true, std::memory_order_relaxed);
          .           }
          .           
          .           void BackRefMain::initEmptyBackRefBlock(BackRefBlock *newBl)
          .           {
          4 ( 0.00%)      intptr_t nextLU = lastUsed+1;
          .               new (newBl) BackRefBlock(newBl, nextLU);
          .               MALLOC_ASSERT(nextLU < dataSz, nullptr);
          4 ( 0.00%)      backRefBl[nextLU] = newBl;
          .               // lastUsed is read in getBackRef, and access to backRefBl[lastUsed]
          .               // is possible only after checking backref against current lastUsed
          .               lastUsed.store(nextLU, std::memory_order_release);
          4 ( 0.00%)  }
          .           
          .           bool BackRefMain::requestNewSpace()
          .           {
          .               bool isRawMemUsed;
          .               static_assert(!(blockSpaceSize % BackRefBlock::bytes),
          .                                    "Must request space for whole number of blocks.");
          .           
          .               if (BackRefMain::dataSz <= lastUsed + 1) // no space in main
-- line 165 ----------------------------------------
-- line 207 ----------------------------------------
          .               return true;
          .           }
          .           
          .           BackRefBlock *BackRefMain::findFreeBlock()
          .           {
          .               BackRefBlock* active_block = active.load(std::memory_order_acquire);
          .               MALLOC_ASSERT(active_block, ASSERT_TEXT);
          .           
        446 ( 0.00%)      if (active_block->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT)
          .                   return active_block;
          .           
          .               if (listForUse.load(std::memory_order_relaxed)) { // use released list
          .                   MallocMutex::scoped_lock lock(mainMutex);
          .           
          .                   if (active_block->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT) {
          .                       active_block = listForUse.load(std::memory_order_relaxed);
          .                       if (active_block) {
-- line 223 ----------------------------------------
-- line 233 ----------------------------------------
          .               return active.load(std::memory_order_acquire); // reread because of requestNewSpace
          .           }
          .           
          .           void *getBackRef(BackRefIdx backRefIdx)
          .           {
          .               // !backRefMain means no initialization done, so it can't be valid memory
          .               // see addEmptyBackRefBlock for fences around lastUsed
          .               if (!(backRefMain.load(std::memory_order_acquire))
 30,332,942 ( 0.15%)          || backRefIdx.getMain() > (backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_acquire))
181,997,652 ( 0.91%)          || backRefIdx.getOffset() >= BR_MAX_CNT)
          .               {
      1,362 ( 0.00%)          return nullptr;
          .               }
          .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)(
          .                       (uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
 30,332,261 ( 0.15%)              + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(std::atomic<void*>)
          .                   );
          .               return backRefEntry.load(std::memory_order_relaxed);
 30,332,261 ( 0.15%)  }
          .           
          .           void setBackRef(BackRefIdx backRefIdx, void *newPtr)
          .           {
          .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
          .                                            && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
        367 ( 0.00%)      ((std::atomic<void*>*)((uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
        261 ( 0.00%)          + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(void*)))->store(newPtr, std::memory_order_relaxed);
          .           }
          .           
          .           BackRefIdx BackRefIdx::newBackRef(bool largeObj)
      2,007 ( 0.00%)  {
          .               BackRefBlock *blockToUse;
          .               void **toUse;
          .               BackRefIdx res;
          .               bool lastBlockFirstUsed = false;
          .           
          .               do {
          .                   MALLOC_ASSERT(backRefMain.load(std::memory_order_relaxed), ASSERT_TEXT);
          .                   blockToUse = backRefMain.load(std::memory_order_relaxed)->findFreeBlock();
        446 ( 0.00%)          if (!blockToUse)
          .                       return BackRefIdx();
          .                   toUse = nullptr;
          .                   { // the block is locked to find a reference
        223 ( 0.00%)              MallocMutex::scoped_lock lock(blockToUse->blockMutex);
          .           
        669 ( 0.00%)              if (blockToUse->freeList) {
          .                           toUse = (void**)blockToUse->freeList;
          .                           blockToUse->freeList = blockToUse->freeList->next;
          .                           MALLOC_ASSERT(!blockToUse->freeList ||
          .                                         ((uintptr_t)blockToUse->freeList>=(uintptr_t)blockToUse
          .                                          && (uintptr_t)blockToUse->freeList <
          .                                          (uintptr_t)blockToUse + slabSize), ASSERT_TEXT);
        446 ( 0.00%)              } else if (blockToUse->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT) {
        223 ( 0.00%)                  toUse = (void**)blockToUse->bumpPtr;
          .                           blockToUse->bumpPtr =
        446 ( 0.00%)                      (FreeObject*)((uintptr_t)blockToUse->bumpPtr - sizeof(void*));
          .                           if (blockToUse->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT-1) {
          .                               MALLOC_ASSERT((uintptr_t)blockToUse->bumpPtr
          .                                             < (uintptr_t)blockToUse+sizeof(BackRefBlock),
          .                                             ASSERT_TEXT);
        892 ( 0.00%)                      blockToUse->bumpPtr = nullptr;
          .                           }
          .                       }
        669 ( 0.00%)              if (toUse) {
        446 ( 0.00%)                  if (!blockToUse->allocatedCount.load(std::memory_order_relaxed) &&
          .                               !backRefMain.load(std::memory_order_relaxed)->listForUse.load(std::memory_order_relaxed)) {
          .                               lastBlockFirstUsed = true;
          .                           }
        223 ( 0.00%)                  blockToUse->allocatedCount.store(blockToUse->allocatedCount.load(std::memory_order_relaxed) + 1, std::memory_order_relaxed);
          .                       }
          .                   } // end of lock scope
          .               } while (!toUse);
          .               // The first thread that uses the last block requests new space in advance;
          .               // possible failures are ignored.
          2 ( 0.00%)      if (lastBlockFirstUsed)
          .                   backRefMain.load(std::memory_order_relaxed)->requestNewSpace();
          .           
          .               res.main = blockToUse->myNum;
        223 ( 0.00%)      uintptr_t offset =
        446 ( 0.00%)          ((uintptr_t)toUse - ((uintptr_t)blockToUse + sizeof(BackRefBlock)))/sizeof(void*);
          .               // Is offset too big?
          .               MALLOC_ASSERT(!(offset >> 15), ASSERT_TEXT);
          .               res.offset = offset;
          .               if (largeObj) res.largeObj = largeObj;
          .           
      1,561 ( 0.00%)      return res;
      1,784 ( 0.00%)  }
          .           
          .           void removeBackRef(BackRefIdx backRefIdx)
        945 ( 0.00%)  {
          .               MALLOC_ASSERT(!backRefIdx.isInvalid(), ASSERT_TEXT);
          .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
          .                             && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
        378 ( 0.00%)      BackRefBlock *currBlock = backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()];
          .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)((uintptr_t)currBlock + sizeof(BackRefBlock)
        567 ( 0.00%)                                          + backRefIdx.getOffset()*sizeof(std::atomic<void*>));
          .               MALLOC_ASSERT(((uintptr_t)&backRefEntry >(uintptr_t)currBlock &&
          .                              (uintptr_t)&backRefEntry <(uintptr_t)currBlock + slabSize), ASSERT_TEXT);
          .               {
        189 ( 0.00%)          MallocMutex::scoped_lock lock(currBlock->blockMutex);
          .           
          .                   backRefEntry.store(currBlock->freeList, std::memory_order_relaxed);
          .           #if MALLOC_DEBUG
          .                   uintptr_t backRefEntryValue = (uintptr_t)backRefEntry.load(std::memory_order_relaxed);
          .                   MALLOC_ASSERT(!backRefEntryValue ||
          .                                 (backRefEntryValue > (uintptr_t)currBlock
          .                                  && backRefEntryValue < (uintptr_t)currBlock + slabSize), ASSERT_TEXT);
          .           #endif
        189 ( 0.00%)          currBlock->freeList = (FreeObject*)&backRefEntry;
        189 ( 0.00%)          currBlock->allocatedCount.store(currBlock->allocatedCount.load(std::memory_order_relaxed)-1, std::memory_order_relaxed);
          .               }
          .               // TODO: do we need double-check here?
        756 ( 0.00%)      if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
          .                   currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed)) {
          .                   MallocMutex::scoped_lock lock(mainMutex);
          .           
          .                   if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
          .                       currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed))
          .                       backRefMain.load(std::memory_order_relaxed)->addToForUseList(currBlock);
          .               }
      1,134 ( 0.00%)  }
          .           
          .           /********* End of backreferences ***********************/
          .           
          .           } // namespace internal
          .           } // namespace rml
          .           

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h
--------------------------------------------------------------------------------
Ir                   

-- line 31 ----------------------------------------
          .           #endif
          .           
          .           /*
          .            * Functions to align an integer down or up to the given power of two,
          .            * and test for such an alignment, and for power of two.
          .            */
          .           template<typename T>
          .           static inline T alignDown(T arg, uintptr_t alignment) {
121,328,822 ( 0.61%)      return T( (uintptr_t)arg                & ~(alignment-1));
          .           }
          .           template<typename T>
          .           static inline T alignUp  (T arg, uintptr_t alignment) {
      1,064 ( 0.00%)      return T(((uintptr_t)arg+(alignment-1)) & ~(alignment-1));
          .               // /*is this better?*/ return (((uintptr_t)arg-1) | (alignment-1)) + 1;
          .           }
          .           template<typename T> // works for not power-of-2 alignments
          .           static inline T alignUpGeneric(T arg, uintptr_t alignment) {
         63 ( 0.00%)      if (size_t rem = arg % alignment) {
          8 ( 0.00%)          arg += alignment - rem;
          .               }
          .               return arg;
          .           }
          .           
          .           /*
          .            * Compile time Log2 calculation
          .            */
          .           template <size_t NUM>
-- line 57 ----------------------------------------
-- line 97 ----------------------------------------
          .               #pragma warning(pop)
          .           #endif
          .           
          .           #if __SUNPRO_CC
          .               #pragma error_messages (on, refmemnoconstr)
          .           #endif
          .           
          .           template <int BUF_LINE_SIZE, int N>
         10 ( 0.00%)  void parseFile(const char* file, const parseFileItem (&items)[N]) {
          .               // Tries to find all items in each line
          1 ( 0.00%)      int found[N] = { 0 };
          .               // If all items found, stop forward file reading
          .               int numFound = 0;
          .               // Line storage
          .               char buf[BUF_LINE_SIZE];
          .           
         27 ( 0.00%)      if (FILE *f = fopen(file, "r")) {
      2,889 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
        435 ( 0.00%)          while (numFound < N && fgets(buf, BUF_LINE_SIZE, f)) {
      1,307 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
      7,863 ( 0.00%)  => ./libio/iofgets.c:fgets (46x)
          .                       for (int i = 0; i < N; ++i) {
      1,025 ( 0.00%)                  if (!found[i] && 1 == sscanf(buf, items[i].format, &items[i].value)) {
     34,776 ( 0.00%)  => ./stdio-common/isoc99_sscanf.c:__isoc99_sscanf (89x)
      1,222 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          2 ( 0.00%)                      ++numFound;
          7 ( 0.00%)                      found[i] = 1;
          .                           }
          .                       }
          .                   }
         13 ( 0.00%)          fclose(f);
     10,900 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .               }
         10 ( 0.00%)  }
          .           
          .           namespace rml {
          .           namespace internal {
          .           
          .           /*
          .            * Best estimate of cache line size, for the purpose of avoiding false sharing.
          .            * Too high causes memory overhead, too low causes false-sharing overhead.
          .            * Because, e.g., 32-bit code might run on a 64-bit system with a larger cache line size,
-- line 132 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 180 ----------------------------------------
          .           
          .           class ThreadId {
          .           #if USE_PTHREAD
          .               std::atomic<pthread_t> tid;
          .           #else
          .               std::atomic<DWORD>     tid;
          .           #endif
          .           public:
        486 ( 0.00%)      ThreadId() : tid(GetMyTID()) {}
        486 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (243x)
          .           #if USE_PTHREAD
 60,664,358 ( 0.30%)      bool isCurrentThreadId() const { return pthread_equal(pthread_self(), tid.load(std::memory_order_relaxed)); }
 60,664,358 ( 0.30%)  => ./nptl/pthread_self.c:pthread_self (30,332,179x)
          .           #else
          .               bool isCurrentThreadId() const { return GetCurrentThreadId() == tid.load(std::memory_order_relaxed); }
          .           #endif
          .               ThreadId& operator=(const ThreadId& other) {
          .                   tid.store(other.tid.load(std::memory_order_relaxed), std::memory_order_relaxed);
          .                   return *this;
          .               }
          .               static bool init() { return true; }
-- line 198 ----------------------------------------
-- line 207 ----------------------------------------
          .           
          .           bool TLSKey::init()
          .           {
          .           #if USE_WINTHREAD
          .               TLS_pointer_key = TlsAlloc();
          .               if (TLS_pointer_key == TLS_ALLOC_FAILURE)
          .                   return false;
          .           #else
          9 ( 0.00%)      int status = pthread_key_create( &TLS_pointer_key, mallocThreadShutdownNotification );
      1,152 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          2 ( 0.00%)      if ( status )
          .                   return false;
          .           #endif /* USE_WINTHREAD */
          .               return true;
          .           }
          .           
          .           bool TLSKey::destroy()
          .           {
          .           #if USE_WINTHREAD
-- line 224 ----------------------------------------
-- line 227 ----------------------------------------
          .               int status1 = pthread_key_delete(TLS_pointer_key);
          .           #endif /* USE_WINTHREAD */
          .               MALLOC_ASSERT(!status1, "The memory manager cannot delete tls key.");
          .               return status1==0;
          .           }
          .           
          .           inline TLSData* TLSKey::getThreadMallocTLS() const
          .           {
121,331,058 ( 0.61%)      return (TLSData *)TlsGetValue_func( TLS_pointer_key );
      3,757 ( 0.00%)  => ./nptl/pthread_getspecific.c:pthread_getspecific (221x)
          .           }
          .           
          .           inline void TLSKey::setThreadMallocTLS( TLSData * newvalue ) {
          .               RecursiveMallocCallProtector scoped;
         13 ( 0.00%)      TlsSetValue_func( TLS_pointer_key, newvalue );
         32 ( 0.00%)  => ./nptl/pthread_setspecific.c:pthread_setspecific (1x)
          .           }
          .           
          .           /* The 'next' field in the block header has to maintain some invariants:
          .            *   it needs to be on a 16K boundary and the first field in the block.
          .            *   Any value stored there needs to have the lower 14 bits set to 0
          .            *   so that various assert work. This means that if you want to smash this memory
          .            *   for debugging purposes you will need to obey this invariant.
          .            * The total size of the header needs to be a power of 2 to simplify
-- line 248 ----------------------------------------
-- line 346 ----------------------------------------
          .           
          .           // Use inheritance to guarantee that a user data start on next cache line.
          .           // Can't use member for it, because when LocalBlockFields already on cache line,
          .           // we must have no additional memory consumption for all compilers.
          .           class Block : public LocalBlockFields,
          .                         Padding<2*blockHeaderAlignment - sizeof(LocalBlockFields)> {
          .           public:
          .               bool empty() const {
 60,664,400 ( 0.30%)          if (allocatedCount > 0) return false;
          .                   MALLOC_ASSERT(!isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT);
          .                   return true;
          .               }
          .               inline FreeObject* allocate();
          .               inline FreeObject *allocateFromFreeList();
          .           
          .               inline bool adjustFullness();
          .               void adjustPositionInBin(Bin* bin = nullptr);
-- line 362 ----------------------------------------
-- line 369 ----------------------------------------
          .               void privatizePublicFreeList( bool reset = true );
          .               void restoreBumpPtr();
          .               void privatizeOrphaned(TLSData *tls, unsigned index);
          .               bool readyToShare();
          .               void shareOrphaned(intptr_t binTag, unsigned index);
          .               unsigned int getSize() const {
          .                   MALLOC_ASSERT(isStartupAllocObject() || objectSize<minLargeObjectSize,
          .                                 "Invalid object size");
          9 ( 0.00%)          return isStartupAllocObject()? 0 : objectSize;
          .               }
          .               const BackRefIdx *getBackRefIdx() const { return &backRefIdx; }
          .               inline bool isOwnedByCurrentThread() const;
          3 ( 0.00%)      bool isStartupAllocObject() const { return objectSize == startupAllocObjSizeMark; }
          .               inline FreeObject *findObjectToFree(const void *object) const;
          .               void checkFreePrecond(const void *object) const {
          .           #if MALLOC_DEBUG
          .                   const char *msg = "Possible double free or heap corruption.";
          .                   // small objects are always at least sizeof(size_t) Byte aligned,
          .                   // try to check this before this dereference as for invalid objects
          .                   // this may be unreadable
          .                   MALLOC_ASSERT(isAligned(object, sizeof(size_t)), "Try to free invalid small object");
-- line 389 ----------------------------------------
-- line 453 ----------------------------------------
          .           class Bin {
          .           private:
          .           public:
          .               Block *activeBlk;
          .               std::atomic<Block*> mailbox;
          .               MallocMutex mailLock;
          .           
          .           public:
 30,332,211 ( 0.15%)      inline Block* getActiveBlock() const { return activeBlk; }
         21 ( 0.00%)      void resetActiveBlock() { activeBlk = nullptr; }
          .               inline void setActiveBlock(Block *block);
          .               inline Block* setPreviousBlockActive();
          .               Block* getPrivatizedFreeListBlock();
          .               void moveBlockToFront(Block *block);
          .               bool cleanPublicFreeLists();
          .               void processEmptyBlock(Block *block, bool poolTheBlock);
          .               void addPublicFreeListBlock(Block* block);
          .           
-- line 470 ----------------------------------------
-- line 546 ----------------------------------------
          .                   ResOfGet() = delete;
          .               public:
          .                   Block* block;
          .                   bool   lastAccMiss;
          .                   ResOfGet(Block *b, bool lastMiss) : block(b), lastAccMiss(lastMiss) {}
          .               };
          .           
          .               // allocated in zero-initialized memory
          1 ( 0.00%)      FreeBlockPool(Backend *bknd) : backend(bknd) {}
          .               ResOfGet getBlock();
          .               void returnBlock(Block *block);
          .               bool externalCleanup(); // can be called by another thread
          .           };
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           class LocalLOCImpl {
          .           private:
-- line 562 ----------------------------------------
-- line 586 ----------------------------------------
          .           public:
          .               Bin           bin[numBlockBinLimit];
          .               FreeBlockPool freeSlabBlocks;
          .               LocalLOC      lloc;
          .               unsigned      currCacheIdx;
          .           private:
          .               std::atomic<bool> unused;
          .           public:
         95 ( 0.00%)      TLSData(MemoryPool *mPool, Backend *bknd) : memPool(mPool), freeSlabBlocks(bknd) {}
          .               MemoryPool *getMemPool() const { return memPool; }
          .               Bin* getAllocationBin(size_t size);
          .               void release();
          .               bool externalCleanup(bool cleanOnlyUnused, bool cleanBins) {
          .                   if (!unused.load(std::memory_order_relaxed) && cleanOnlyUnused) return false;
          .                   // Heavy operation in terms of synchronization complexity,
          .                   // should be called only for the current thread
          .                   bool released = cleanBins ? cleanupBlockBins() : false;
          .                   // both cleanups to be called, and the order is not important
          4 ( 0.00%)          return released | lloc.externalCleanup(&memPool->extMemPool) | freeSlabBlocks.externalCleanup();
      2,854 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::externalCleanup() (1x)
          .               }
          .               bool cleanupBlockBins();
          .               void markUsed() { unused.store(false, std::memory_order_relaxed); } // called by owner when TLS touched
          .               void markUnused() { unused.store(true, std::memory_order_relaxed); } // can be called by not owner thread
          .           };
          .           
          .           TLSData *TLSKey::createTLS(MemoryPool *memPool, Backend *backend)
         11 ( 0.00%)  {
          .               MALLOC_ASSERT( sizeof(TLSData) >= sizeof(Bin) * numBlockBins + sizeof(FreeBlockPool), ASSERT_TEXT );
          1 ( 0.00%)      TLSData* tls = (TLSData*) memPool->bootStrapBlocks.allocate(memPool, sizeof(TLSData));
          .               if ( !tls )
          .                   return nullptr;
          .               new(tls) TLSData(memPool, backend);
          .               /* the block contains zeroes after bootStrapMalloc, so bins are initialized */
          .           #if MALLOC_DEBUG
          .               for (uint32_t i = 0; i < numBlockBinLimit; i++)
          .                   tls->bin[i].verifyInitState();
          .           #endif
          .               setThreadMallocTLS(tls);
          .               memPool->extMemPool.allLocalCaches.registerThread(tls);
          .               return tls;
          9 ( 0.00%)  }
          .           
          .           bool TLSData::cleanupBlockBins()
          .           {
          .               bool released = false;
          .               for (uint32_t i = 0; i < numBlockBinLimit; i++) {
          .                   released |= bin[i].cleanPublicFreeLists();
          .                   // After cleaning public free lists, only the active block might be empty.
          .                   // Do not use processEmptyBlock because it will just restore bumpPtr.
-- line 634 ----------------------------------------
-- line 651 ----------------------------------------
          .               if (TLSData *tlsData = tlsPointerKey.getThreadMallocTLS())
          .                   released |= tlsData->cleanupBlockBins();
          .           
          .               return released;
          .           }
          .           
          .           void AllLocalCaches::registerThread(TLSRemote *tls)
          .           {
          1 ( 0.00%)      tls->prev = nullptr;
          1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
          .               MALLOC_ASSERT(head!=tls, ASSERT_TEXT);
          2 ( 0.00%)      tls->next = head;
          2 ( 0.00%)      if (head)
          .                   head->prev = tls;
          1 ( 0.00%)      head = tls;
          .               MALLOC_ASSERT(head->next!=head, ASSERT_TEXT);
          .           }
          .           
          .           void AllLocalCaches::unregisterThread(TLSRemote *tls)
          .           {
          1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
          .               MALLOC_ASSERT(head, "Can't unregister thread: no threads are registered.");
          2 ( 0.00%)      if (head == tls)
          3 ( 0.00%)          head = tls->next;
          2 ( 0.00%)      if (tls->next)
          1 ( 0.00%)          tls->next->prev = tls->prev;
          2 ( 0.00%)      if (tls->prev)
          .                   tls->prev->next = tls->next;
          .               MALLOC_ASSERT(!tls->next || tls->next->next!=tls->next, ASSERT_TEXT);
          .           }
          .           
          .           bool AllLocalCaches::cleanup(bool cleanOnlyUnused)
          .           {
          .               bool released = false;
          .               {
-- line 685 ----------------------------------------
-- line 688 ----------------------------------------
          .                       released |= static_cast<TLSData*>(curr)->externalCleanup(cleanOnlyUnused, /*cleanBins=*/false);
          .               }
          .               return released;
          .           }
          .           
          .           void AllLocalCaches::markUnused()
          .           {
          .               bool locked;
          2 ( 0.00%)      MallocMutex::scoped_lock lock(listLock, /*block=*/false, &locked);
          4 ( 0.00%)      if (!locked) // not wait for marking if someone doing something with it
          .                   return;
          .           
         14 ( 0.00%)      for (TLSRemote *curr=head; curr; curr=curr->next)
          .                   static_cast<TLSData*>(curr)->markUnused();
          .           }
          .           
          .           #if MALLOC_CHECK_RECURSION
          .           MallocMutex RecursiveMallocCallProtector::rmc_mutex;
          .           std::atomic<pthread_t> RecursiveMallocCallProtector::owner_thread;
          .           std::atomic<void*> RecursiveMallocCallProtector::autoObjPtr;
          .           bool        RecursiveMallocCallProtector::mallocRecursionDetected;
-- line 708 ----------------------------------------
-- line 768 ----------------------------------------
          .           #endif
          .           static inline unsigned int highestBitPos(unsigned int n)
          .           {
          .               MALLOC_ASSERT( n>=64 && n<1024, ASSERT_TEXT ); // only needed for bsr array lookup, but always true
          .               unsigned int pos;
          .           #if __ARCH_x86_32||__ARCH_x86_64
          .           
          .           # if __unix__||__APPLE__||__MINGW32__
     12,520 ( 0.00%)      __asm__ ("bsr %1,%0" : "=r"(pos) : "r"(n));
          .           # elif (_WIN32 && (!_WIN64 || __INTEL_COMPILER))
          .               __asm
          .               {
          .                   bsr eax, n
          .                   mov pos, eax
          .               }
          .           # elif _WIN64 && _MSC_VER>=1400
          .               _BitScanReverse((unsigned long*)&pos, (unsigned long)n);
-- line 784 ----------------------------------------
-- line 796 ----------------------------------------
          .               static unsigned int bsr[16] = {0/*N/A*/,6,7,7,8,8,8,8,9,9,9,9,9,9,9,9};
          .               pos = bsr[ n>>6 ];
          .           #endif /* __ARCH_* */
          .               return pos;
          .           }
          .           
          .           unsigned int getSmallObjectIndex(unsigned int size)
          .           {
 78,653,474 ( 0.39%)      unsigned int result = (size-1)>>3;
          .               if (sizeof(void*)==8) {
          .                   // For 64-bit malloc, 16 byte alignment is needed except for bin 0.
196,633,795 ( 0.98%)          if (result) result |= 1; // 0,1,3,5,7; bins 2,4,6 are not aligned to 16 bytes
          .               }
          .               return result;
          .           }
          .           
          .           /*
          .            * Depending on indexRequest, for a given size return either the index into the bin
          .            * for objects of this size, or the actual size of objects in this bin.
          .            */
          .           template<bool indexRequest>
          .           static unsigned int getIndexOrObjectSize (unsigned int size)
          .           {
 78,678,756 ( 0.39%)      if (size <= maxSmallObjectSize) { // selection from 8/16/24/32/40/48/56/64
          .                   unsigned int index = getSmallObjectIndex( size );
          .                    /* Bin 0 is for 8 bytes, bin 1 is for 16, and so forth */
          .                   return indexRequest ? index : (index+1)<<3;
          .               }
     25,208 ( 0.00%)      else if (size <= maxSegregatedObjectSize ) { // 80/96/112/128 / 160/192/224/256 / 320/384/448/512 / 640/768/896/1024
     12,520 ( 0.00%)          unsigned int order = highestBitPos(size-1); // which group of bin sizes?
          .                   MALLOC_ASSERT( 6<=order && order<=9, ASSERT_TEXT );
          .                   if (indexRequest)
     49,428 ( 0.00%)              return minSegregatedObjectIndex - (4*6) - 4 + (4*order) + ((size-1)>>(order-2));
          .                   else {
        815 ( 0.00%)              unsigned int alignment = 128 >> (9-order); // alignment in the group
          .                       MALLOC_ASSERT( alignment==16 || alignment==32 || alignment==64 || alignment==128, ASSERT_TEXT );
          .                       return alignUp(size,alignment);
          .                   }
          .               }
          .               else {
        171 ( 0.00%)          if( size <= fittingSize3 ) {
         88 ( 0.00%)              if( size <= fittingSize2 ) {
         18 ( 0.00%)                  if( size <= fittingSize1 )
          .                               return indexRequest ? minFittingIndex : fittingSize1;
          .                           else
        205 ( 0.00%)                      return indexRequest ? minFittingIndex+1 : fittingSize2;
          .                       } else
         41 ( 0.00%)                  return indexRequest ? minFittingIndex+2 : fittingSize3;
          .                   } else {
         80 ( 0.00%)              if( size <= fittingSize5 ) {
         15 ( 0.00%)                  if( size <= fittingSize4 )
          .                               return indexRequest ? minFittingIndex+3 : fittingSize4;
          .                           else
        148 ( 0.00%)                      return indexRequest ? minFittingIndex+4 : fittingSize5;
          .                       } else {
          .                           MALLOC_ASSERT( 0,ASSERT_TEXT ); // this should not happen
         37 ( 0.00%)                  return ~0U;
          .                       }
          .                   }
          .               }
         37 ( 0.00%)  }
          .           
          .           static unsigned int getIndex (unsigned int size)
          .           {
 69,671,562 ( 0.35%)      return getIndexOrObjectSize</*indexRequest=*/true>(size);
 80,905,918 ( 0.40%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) (8,989,512x)
          .           }
          .           
          .           static unsigned int getObjectSize (unsigned int size)
          .           {
          .               return getIndexOrObjectSize</*indexRequest=*/false>(size);
          .           }
          .           
          .           
-- line 868 ----------------------------------------
-- line 870 ----------------------------------------
          .           {
          .               FreeObject *result;
          .           
          .               MALLOC_ASSERT( size == sizeof(TLSData), ASSERT_TEXT );
          .           
          .               { // Lock with acquire
          .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
          .           
          3 ( 0.00%)          if( bootStrapObjectList) {
          .                       result = bootStrapObjectList;
          .                       bootStrapObjectList = bootStrapObjectList->next;
          .                   } else {
          3 ( 0.00%)              if (!bootStrapBlock) {
          4 ( 0.00%)                  bootStrapBlock = memPool->getEmptyBlock(size);
        584 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (1x)
          2 ( 0.00%)                  if (!bootStrapBlock) return nullptr;
          .                       }
          1 ( 0.00%)              result = bootStrapBlock->bumpPtr;
          5 ( 0.00%)              bootStrapBlock->bumpPtr = (FreeObject *)((uintptr_t)bootStrapBlock->bumpPtr - bootStrapBlock->objectSize);
          3 ( 0.00%)              if ((uintptr_t)bootStrapBlock->bumpPtr < (uintptr_t)bootStrapBlock+sizeof(Block)) {
          .                           bootStrapBlock->bumpPtr = nullptr;
          .                           bootStrapBlock->next = bootStrapBlockUsed;
          .                           bootStrapBlockUsed = bootStrapBlock;
          .                           bootStrapBlock = nullptr;
          .                       }
          .                   }
          .               } // Unlock with release
        115 ( 0.00%)      memset (result, 0, size);
          .               return (void*)result;
          .           }
          .           
          .           void BootStrapBlocks::free(void* ptr)
          .           {
          .               MALLOC_ASSERT( ptr, ASSERT_TEXT );
          .               { // Lock with acquire
          .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
          3 ( 0.00%)          ((FreeObject*)ptr)->next = bootStrapObjectList;
          1 ( 0.00%)          bootStrapObjectList = (FreeObject*)ptr;
          .               } // Unlock with release
          .           }
          .           
          .           void BootStrapBlocks::reset()
          .           {
          .               bootStrapBlock = bootStrapBlockUsed = nullptr;
          .               bootStrapObjectList = nullptr;
          .           }
-- line 914 ----------------------------------------
-- line 927 ----------------------------------------
          .           LifoList::LifoList( ) : top(nullptr)
          .           {
          .               // MallocMutex assumes zero initialization
          .               memset(&lock, 0, sizeof(MallocMutex));
          .           }
          .           
          .           void LifoList::push(Block *block)
          .           {
          3 ( 0.00%)      MallocMutex::scoped_lock scoped_cs(lock);
          3 ( 0.00%)      block->next = top.load(std::memory_order_relaxed);
          .               top.store(block, std::memory_order_relaxed);
          .           }
          .           
          .           Block *LifoList::pop()
          .           {
        242 ( 0.00%)      Block* block = nullptr;
        484 ( 0.00%)      if (top.load(std::memory_order_relaxed)) {
          .                   MallocMutex::scoped_lock scoped_cs(lock);
          .                   block = top.load(std::memory_order_relaxed);
          .                   if (block) {
          .                       top.store(block->next, std::memory_order_relaxed);
          .                   }
          .               }
          .               return block;
          .           }
-- line 951 ----------------------------------------
-- line 980 ----------------------------------------
          .                        backend->returnLargeObject(lmb);
          .                    }
          .                }
          .           }
          .           
          .           TLSData* MemoryPool::getTLS(bool create)
          .           {
          .               TLSData* tls = extMemPool.tlsPointerKey.getThreadMallocTLS();
 60,664,518 ( 0.30%)      if (create && !tls)
 60,664,523 ( 0.30%)          tls = extMemPool.tlsPointerKey.createTLS(this, &extMemPool.backend);
      2,123 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::TLSKey::createTLS(rml::internal::MemoryPool*, rml::internal::Backend*) (1x)
          .               return tls;
          .           }
          .           
          .           /*
          .            * Return the bin for the given size.
          .            */
          .           inline Bin* TLSData::getAllocationBin(size_t size)
          .           {
130,403,010 ( 0.65%)      return bin + getIndex(size);
          .           }
          .           
          .           /* Return an empty uninitialized block in a non-blocking fashion. */
          .           Block *MemoryPool::getEmptyBlock(size_t size)
      2,187 ( 0.00%)  {
          .               TLSData* tls = getTLS(/*create=*/false);
          .               // try to use per-thread cache, if TLS available
          .               FreeBlockPool::ResOfGet resOfGet = tls?
        728 ( 0.00%)          tls->freeSlabBlocks.getBlock() : FreeBlockPool::ResOfGet(nullptr, false);
          .               Block *result = resOfGet.block;
          .           
          .               if (!result) { // not found in local cache, asks backend for slabs
         93 ( 0.00%)          int num = resOfGet.lastAccMiss? Backend::numOfSlabAllocOnMiss : 1;
          .                   BackRefIdx backRefIdx[Backend::numOfSlabAllocOnMiss];
          .           
         92 ( 0.00%)          result = static_cast<Block*>(extMemPool.backend.getSlabBlock(num));
        184 ( 0.00%)          if (!result) return nullptr;
          .           
        184 ( 0.00%)          if (!extMemPool.userPool())
        184 ( 0.00%)              for (int i=0; i<num; i++) {
      1,281 ( 0.00%)                  backRefIdx[i] = BackRefIdx::newBackRef(/*largeObj=*/false);
     12,444 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (183x)
        366 ( 0.00%)                  if (backRefIdx[i].isInvalid()) {
          .                               // roll back resource allocation
          .                               for (int j=0; j<i; j++)
          .                                   removeBackRef(backRefIdx[j]);
          .                               Block *b = result;
          .                               for (int j=0; j<num; b=(Block*)((uintptr_t)b+slabSize), j++)
          .                                   extMemPool.backend.putSlabBlock(b);
          .                               return nullptr;
          .                           }
          .                       }
          .                   // resources were allocated, register blocks
          .                   Block *b = result;
        184 ( 0.00%)          for (int i=0; i<num; b=(Block*)((uintptr_t)b+slabSize), i++) {
          .                       // slab block in user's pool must have invalid backRefIdx
        366 ( 0.00%)              if (extMemPool.userPool()) {
          .                           new (&b->backRefIdx) BackRefIdx();
          .                       } else {
        183 ( 0.00%)                  setBackRef(backRefIdx[i], b);
        915 ( 0.00%)                  b->backRefIdx = backRefIdx[i];
          .                       }
          .                       b->tlsPtr.store(tls, std::memory_order_relaxed);
        183 ( 0.00%)              b->poolPtr = this;
          .                       // all but first one go to per-thread pool
        366 ( 0.00%)              if (i > 0) {
          .                           MALLOC_ASSERT(tls, ASSERT_TEXT);
        182 ( 0.00%)                  tls->freeSlabBlocks.returnBlock(b);
      1,911 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (91x)
          .                       }
          .                   }
          .               }
          .               MALLOC_ASSERT(result, ASSERT_TEXT);
          .               result->initEmptyBlock(tls, size);
          .               STAT_increment(getThreadId(), getIndex(result->objectSize), allocBlockNew);
          .               return result;
      2,187 ( 0.00%)  }
          .           
          .           void MemoryPool::returnEmptyBlock(Block *block, bool poolTheBlock)
      1,195 ( 0.00%)  {
          .               block->reset();
        478 ( 0.00%)      if (poolTheBlock) {
        663 ( 0.00%)          getTLS(/*create=*/false)->freeSlabBlocks.returnBlock(block);
     46,854 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (221x)
          .               } else {
          .                   // slab blocks in user's pools do not have valid backRefIdx
         36 ( 0.00%)          if (!extMemPool.userPool())
         54 ( 0.00%)              removeBackRef(*(block->getBackRefIdx()));
        738 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (18x)
         54 ( 0.00%)          extMemPool.backend.putSlabBlock(block);
      5,241 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (18x)
          .               }
        717 ( 0.00%)  }
          .           
          .           bool ExtMemoryPool::init(intptr_t poolId, rawAllocType rawAlloc,
          .                                    rawFreeType rawFree, size_t granularity,
          .                                    bool keepAllMemory, bool fixedPool)
          .           {
          1 ( 0.00%)      this->poolId = poolId;
          1 ( 0.00%)      this->rawAlloc = rawAlloc;
          1 ( 0.00%)      this->rawFree = rawFree;
          1 ( 0.00%)      this->granularity = granularity;
          2 ( 0.00%)      this->keepAllMemory = keepAllMemory;
          1 ( 0.00%)      this->fixedPool = fixedPool;
          .               this->delayRegsReleasing = false;
          .               if (!initTLS())
          .                   return false;
          3 ( 0.00%)      loc.init(this);
      1,296 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/large_objects.cpp:rml::internal::LargeObjectCache::init(rml::internal::ExtMemoryPool*) [clone .part.0] (1x)
          .               backend.init(this);
          .               MALLOC_ASSERT(isPoolValid(), nullptr);
          .               return true;
          .           }
          .           
          .           bool ExtMemoryPool::initTLS() { return tlsPointerKey.init(); }
          .           
          .           bool MemoryPool::init(intptr_t poolId, const MemPoolPolicy *policy)
-- line 1089 ----------------------------------------
-- line 1148 ----------------------------------------
          .                   // for user pool, because it's just about to be released. But for system
          .                   // pool restoring, we do not want to do zeroing of it on subsequent reload.
          .                   bootStrapBlocks.reset();
          .                   extMemPool.orphanedBlocks.reset();
          .               }
          .               return extMemPool.destroy();
          .           }
          .           
          9 ( 0.00%)  void MemoryPool::onThreadShutdown(TLSData *tlsData)
          .           {
          5 ( 0.00%)      if (tlsData) { // might be called for "empty" TLS
     14,992 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::onThreadShutdown(rml::internal::TLSData*) [clone .part.0] (1x)
          .                   tlsData->release();
          2 ( 0.00%)          bootStrapBlocks.free(tlsData);
          .                   clearTLS();
          .               }
          8 ( 0.00%)  }
          .           
          .           #if MALLOC_DEBUG
          .           void Bin::verifyTLSBin (size_t size) const
          .           {
          .           /* The debug version verifies the TLSBin as needed */
          .               uint32_t objSize = getObjectSize(size);
          .           
          .               if (activeBlk) {
-- line 1171 ----------------------------------------
-- line 1209 ----------------------------------------
          .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .               MALLOC_ASSERT( block->objectSize != 0, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->next == nullptr, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->previous == nullptr, ASSERT_TEXT );
          .           
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               verifyTLSBin(size);
          .           
     17,438 ( 0.00%)      block->next = activeBlk;
     34,392 ( 0.00%)      if( activeBlk ) {
     34,350 ( 0.00%)          block->previous = activeBlk->previous;
     17,175 ( 0.00%)          activeBlk->previous = block;
     51,525 ( 0.00%)          if( block->previous )
     31,424 ( 0.00%)              block->previous->next = block;
          .               } else {
          .                   activeBlk = block;
          .               }
          .           
          .               verifyTLSBin(size);
          .           }
          .           
          .           /*
-- line 1230 ----------------------------------------
-- line 1239 ----------------------------------------
          .           
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               verifyTLSBin(size);
          .           
          .               if (block == activeBlk) {
          .                   activeBlk = block->previous? block->previous : block->next;
          .               }
          .               /* Unlink the block */
     51,525 ( 0.00%)      if (block->previous) {
          .                   MALLOC_ASSERT( block->previous->next == block, ASSERT_TEXT );
     34,288 ( 0.00%)          block->previous->next = block->next;
          .               }
     34,350 ( 0.00%)      if (block->next) {
          .                   MALLOC_ASSERT( block->next->previous == block, ASSERT_TEXT );
     17,151 ( 0.00%)          block->next->previous = block->previous;
          .               }
        221 ( 0.00%)      block->next = nullptr;
     17,175 ( 0.00%)      block->previous = nullptr;
          .           
          .               verifyTLSBin(size);
          .           }
          .           
          .           Block* Bin::getPrivatizedFreeListBlock()
          .           {
          .               Block* block;
          .               MALLOC_ASSERT( this, ASSERT_TEXT );
          .               // if this method is called, active block usage must be unsuccessful
          .               MALLOC_ASSERT( !activeBlk && !mailbox.load(std::memory_order_relaxed) || activeBlk && activeBlk->isFull, ASSERT_TEXT );
          .           
          .           // the counter should be changed    STAT_increment(getThreadId(), ThreadCommonCounters, lockPublicFreeList);
        484 ( 0.00%)      if (!mailbox.load(std::memory_order_acquire)) // hotpath is empty mailbox
          .                   return nullptr;
          .               else { // mailbox is not empty, take lock and inspect it
          .                   MallocMutex::scoped_lock scoped_cs(mailLock);
          .                   block = mailbox.load(std::memory_order_relaxed);
          .                   if( block ) {
          .                       MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .                       MALLOC_ASSERT( !isNotForUse(block->nextPrivatizable.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .                       mailbox.store(block->nextPrivatizable.load(std::memory_order_relaxed), std::memory_order_relaxed);
-- line 1277 ----------------------------------------
-- line 1319 ----------------------------------------
          .                       block->adjustPositionInBin(this);
          .                   block = tmp;
          .               }
          .               return released;
          .           }
          .           
          .           bool Block::adjustFullness()
          .           {
  2,825,568 ( 0.01%)      if (bumpPtr) {
          .                   /* If we are still using a bump ptr for this block it is empty enough to use. */
          .                   STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
          .                   isFull = false;
          .               } else {
          .                   const float threshold = (slabSize - sizeof(Block)) * (1 - emptyEnoughRatio);
          .                   /* allocatedCount shows how many objects in the block are in use; however it still counts
          .                    * blocks freed by other threads; so prior call to privatizePublicFreeList() is recommended */
  9,889,488 ( 0.05%)          isFull = (allocatedCount*objectSize > threshold) ? true : false;
          .           #if COLLECT_STATISTICS
          .                   if (isFull)
          .                       STAT_increment(getThreadId(), getIndex(objectSize), examineNotEmpty);
          .                   else
          .                       STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
          .           #endif
          .               }
          .               return isFull;
          .           }
          .           
          .           // This method resides in class Block, and not in class Bin, in order to avoid
          .           // calling getAllocationBin on a reasonably hot path in Block::freeOwnObject
          .           void Block::adjustPositionInBin(Bin* bin/*=nullptr*/)
 21,342,667 ( 0.11%)  {
          .               // If the block were full, but became empty enough to use,
          .               // move it to the front of the list
 44,098,118 ( 0.22%)      if (isFull && !adjustFullness()) {
     33,908 ( 0.00%)          if (!bin)
          .                       bin = tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize);
          .                   bin->moveBlockToFront(this);
          .               }
 21,326,955 ( 0.11%)  }
          .           
          .           /* Restore the bump pointer for an empty block that is planned to use */
          .           void Block::restoreBumpPtr()
          .           {
          .               MALLOC_ASSERT( allocatedCount == 0, ASSERT_TEXT );
          .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .               STAT_increment(getThreadId(), getIndex(objectSize), freeRestoreBumpPtr);
 26,967,873 ( 0.13%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
  8,989,291 ( 0.04%)      freeList = nullptr;
  8,989,291 ( 0.04%)      isFull = false;
  8,989,291 ( 0.04%)  }
          .           
          .           void Block::freeOwnObject(void *object)
          .           {
          .               tlsPtr.load(std::memory_order_relaxed)->markUsed();
          .               allocatedCount--;
          .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
          .           #if COLLECT_STATISTICS
          .               // Note that getAllocationBin is not called on the hottest path with statistics off.
-- line 1376 ----------------------------------------
-- line 1377 ----------------------------------------
          .               if (tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->getActiveBlock() != this)
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToInactiveBlock);
          .               else
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToActiveBlock);
          .           #endif
          .               if (empty()) {
          .                   // If the last object of a slab is freed, the slab cannot be marked full
          .                   MALLOC_ASSERT(!isFull, ASSERT_TEXT);
 17,979,024 ( 0.09%)          tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->processEmptyBlock(this, /*poolTheBlock=*/true);
          .               } else { // hot path
          .                   FreeObject *objectToFree = findObjectToFree(object);
 42,685,334 ( 0.21%)          objectToFree->next = freeList;
 21,342,667 ( 0.11%)          freeList = objectToFree;
 64,028,001 ( 0.32%)          adjustPositionInBin();
100,175,567 ( 0.50%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) (21,342,667x)
          .               }
          .           }
          .           
          .           void Block::freePublicObject (FreeObject *objectToFree)
          .           {
          .               FreeObject* localPublicFreeList{};
          .           
          .               MALLOC_ITT_SYNC_RELEASING(&publicFreeList);
-- line 1398 ----------------------------------------
-- line 1524 ----------------------------------------
          .           
          .           void Block::shareOrphaned(intptr_t binTag, unsigned index)
          .           {
          .               MALLOC_ASSERT( binTag, ASSERT_TEXT );
          .               // unreferenced formal parameter warning
          .               tbb::detail::suppress_unused_warning(index);
          .               STAT_increment(getThreadId(), index, freeBlockPublic);
          .               markOrphaned();
          6 ( 0.00%)      if ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
          .                   // First check passed: the block is not in mailbox yet.
          .                   // Need to set publicFreeList to non-zero, so other threads
          .                   // will not change nextPrivatizable and it can be zeroed.
          6 ( 0.00%)          if ( !readyToShare() ) {
          .                       // another thread freed an object; we need to wait until it finishes.
          .                       // There is no need for exponential backoff, as the wait here is not for a lock;
          .                       // but need to yield, so the thread we wait has a chance to run.
          .                       // TODO: add a pause to also be friendly to hyperthreads
          .                       int count = 256;
          .                       while ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
          .                           if (--count==0) {
          .                               do_yield();
          .                               count = 256;
          .                           }
          .                       }
          .                   }
          .               }
          .               MALLOC_ASSERT( publicFreeList.load(std::memory_order_relaxed) !=nullptr, ASSERT_TEXT );
          .               // now it is safe to change our data
          3 ( 0.00%)      previous = nullptr;
          .               // it is caller responsibility to ensure that the list of blocks
          .               // formed by nextPrivatizable pointers is kept consistent if required.
          .               // if only called from thread shutdown code, it does not matter.
          .               nextPrivatizable.store((Block*)UNUSABLE, std::memory_order_relaxed);
          .           }
          .           
          .           void Block::cleanBlockHeader()
          .           {
        483 ( 0.00%)      next = nullptr;
        483 ( 0.00%)      previous = nullptr;
        484 ( 0.00%)      freeList = nullptr;
        967 ( 0.00%)      allocatedCount = 0;
        483 ( 0.00%)      isFull = false;
          .               tlsPtr.store(nullptr, std::memory_order_relaxed);
          .           
          .               publicFreeList.store(nullptr, std::memory_order_relaxed);
          .           }
          .           
          .           void Block::initEmptyBlock(TLSData *tls, size_t size)
          .           {
          .               // Having getIndex and getObjectSize called next to each other
          .               // allows better compiler optimization as they basically share the code.
          .               unsigned int index = getIndex(size);
          .               unsigned int objSz = getObjectSize(size);
          .           
          .               cleanBlockHeader();
        552 ( 0.00%)      objectSize = objSz;
          .               markOwned(tls);
          .               // bump pointer should be prepared for first allocation - thus mode it down to objectSize
      1,307 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
          .           
          .               // each block should have the address where the head of the list of "privatizable" blocks is kept
          .               // the only exception is a block for boot strap which is initialized when TLS is yet nullptr
      1,212 ( 0.00%)      nextPrivatizable.store( tls? (Block*)(tls->bin + index) : nullptr, std::memory_order_relaxed);
          .               TRACEF(( "[ScalableMalloc trace] Empty block %p is initialized, owner is %ld, objectSize is %d, bumpPtr is %p\n",
          .                        this, tlsPtr.load(std::memory_order_relaxed) ? getThreadId() : -1, objectSize, bumpPtr ));
          .           }
          .           
          .           Block *OrphanedBlocks::get(TLSData *tls, unsigned int size)
      2,178 ( 0.00%)  {
          .               // TODO: try to use index from getAllocationBin
          .               unsigned int index = getIndex(size);
        968 ( 0.00%)      Block *block = bins[index].pop();
          .               if (block) {
          .                   MALLOC_ITT_SYNC_ACQUIRED(bins+index);
          .                   block->privatizeOrphaned(tls, index);
          .               }
          .               return block;
      2,178 ( 0.00%)  }
          .           
          .           void OrphanedBlocks::put(intptr_t binTag, Block *block)
         21 ( 0.00%)  {
          .               unsigned int index = getIndex(block->getSize());
          .               block->shareOrphaned(binTag, index);
         15 ( 0.00%)      MALLOC_ITT_SYNC_RELEASING(bins+index);
          .               bins[index].push(block);
         24 ( 0.00%)  }
          .           
          .           void OrphanedBlocks::reset()
          .           {
          .               for (uint32_t i=0; i<numBlockBinLimit; i++)
          .                   new (bins+i) LifoList();
          .           }
          .           
          .           bool OrphanedBlocks::cleanup(Backend* backend)
-- line 1617 ----------------------------------------
-- line 1639 ----------------------------------------
          .               }
          .               return released;
          .           }
          .           
          .           FreeBlockPool::ResOfGet FreeBlockPool::getBlock()
          .           {
          .               Block *b = head.exchange(nullptr);
          .           
        484 ( 0.00%)      if (b) {
        151 ( 0.00%)          size--;
        151 ( 0.00%)          Block *newHead = b->next;
        151 ( 0.00%)          lastAccessMiss = false;
          .                   head.store(newHead, std::memory_order_release);
          .               } else {
         91 ( 0.00%)          lastAccessMiss = true;
          .               }
          .               return ResOfGet(b, lastAccessMiss);
          .           }
          .           
          .           void FreeBlockPool::returnBlock(Block *block)
      2,184 ( 0.00%)  {
          .               MALLOC_ASSERT( size <= POOL_HIGH_MARK, ASSERT_TEXT );
          .               Block *localHead = head.exchange(nullptr);
          .           
        936 ( 0.00%)      if (!localHead) {
          .                   size = 0; // head was stolen by externalClean, correct size accordingly
        651 ( 0.00%)      } else if (size == POOL_HIGH_MARK) {
          .                   // release cold blocks and add hot one,
          .                   // so keep POOL_LOW_MARK-1 blocks and add new block to head
          .                   Block *headToFree = localHead, *helper;
          .                   for (int i=0; i<POOL_LOW_MARK-2; i++)
         36 ( 0.00%)              headToFree = headToFree->next;
          .                   Block *last = headToFree;
          6 ( 0.00%)          headToFree = headToFree->next;
          6 ( 0.00%)          last->next = nullptr;
          6 ( 0.00%)          size = POOL_LOW_MARK-1;
        318 ( 0.00%)          for (Block *currBl = headToFree; currBl; currBl = helper) {
        150 ( 0.00%)              helper = currBl->next;
          .                       // slab blocks in user's pools do not have valid backRefIdx
        600 ( 0.00%)              if (!backend->inUserPool())
        450 ( 0.00%)                  removeBackRef(currBl->backRefIdx);
      6,150 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (150x)
        450 ( 0.00%)              backend->putSlabBlock(currBl);
     33,005 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (150x)
          .                   }
          .               }
        547 ( 0.00%)      size++;
        312 ( 0.00%)      block->next = localHead;
          .               head.store(block, std::memory_order_release);
      1,872 ( 0.00%)  }
          .           
          .           bool FreeBlockPool::externalCleanup()
          5 ( 0.00%)  {
          .               Block *helper;
          .               bool released = false;
          .           
         26 ( 0.00%)      for (Block *currBl=head.exchange(nullptr); currBl; currBl=helper) {
         11 ( 0.00%)          helper = currBl->next;
          .                   // slab blocks in user's pools do not have valid backRefIdx
         44 ( 0.00%)          if (!backend->inUserPool())
         33 ( 0.00%)              removeBackRef(currBl->backRefIdx);
        451 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (11x)
         33 ( 0.00%)          backend->putSlabBlock(currBl);
          1 ( 0.00%)          released = true;
          .               }
          .               return released;
          6 ( 0.00%)  }
          .           
          .           /* Prepare the block for returning to FreeBlockPool */
          .           void Block::reset()
          .           {
          .               // it is caller's responsibility to ensure no data is lost before calling this
          .               MALLOC_ASSERT( allocatedCount==0, ASSERT_TEXT );
          .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
          .               if (!isStartupAllocObject())
          .                   STAT_increment(getThreadId(), getIndex(objectSize), freeBlockBack);
          .           
          .               cleanBlockHeader();
          .           
          .               nextPrivatizable.store(nullptr, std::memory_order_relaxed);
          .           
        478 ( 0.00%)      objectSize = 0;
          .               // for an empty block, bump pointer should point right after the end of the block
        478 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize);
          .           }
          .           
          .           inline void Bin::setActiveBlock (Block *block)
          .           {
          .           //    MALLOC_ASSERT( bin, ASSERT_TEXT );
          .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
          .               // it is the caller responsibility to keep bin consistence (i.e. ensure this block is in the bin list)
        242 ( 0.00%)      activeBlk = block;
          .           }
          .           
          .           inline Block* Bin::setPreviousBlockActive()
          .           {
          .               MALLOC_ASSERT( activeBlk, ASSERT_TEXT );
     33,908 ( 0.00%)      Block* temp = activeBlk->previous;
     33,908 ( 0.00%)      if( temp ) {
          .                   MALLOC_ASSERT( !(temp->isFull), ASSERT_TEXT );
     16,733 ( 0.00%)          activeBlk = temp;
          .               }
          .               return temp;
          .           }
          .           
          .           inline bool Block::isOwnedByCurrentThread() const {
121,328,716 ( 0.61%)      return tlsPtr.load(std::memory_order_relaxed) && ownerTid.isCurrentThreadId();
          .           }
          .           
          .           FreeObject *Block::findObjectToFree(const void *object) const
          .           {
          .               FreeObject *objectToFree;
          .               // Due to aligned allocations, a pointer passed to scalable_free
          .               // might differ from the address of internally allocated object.
          .               // Small objects however should always be fine.
 64,028,001 ( 0.32%)      if (objectSize <= maxSegregatedObjectSize)
          .                   objectToFree = (FreeObject*)object;
          .               // "Fitting size" allocations are suspicious if aligned higher than naturally
          .               else {
         22 ( 0.00%)          if ( ! isAligned(object,2*fittingAlignment) )
          .                       // TODO: the above check is questionable - it gives false negatives in ~50% cases,
          .                       //       so might even be slower in average than unconditional use of findAllocatedObject.
          .                       // here it should be a "real" object
          .                       objectToFree = (FreeObject*)object;
          .                   else
          .                       // here object can be an aligned address, so applying additional checks
          .                       objectToFree = findAllocatedObject(object);
          .                   MALLOC_ASSERT( isAligned(objectToFree,fittingAlignment), ASSERT_TEXT );
-- line 1763 ----------------------------------------
-- line 1764 ----------------------------------------
          .               }
          .               MALLOC_ASSERT( isProperlyPlaced(objectToFree), ASSERT_TEXT );
          .           
          .               return objectToFree;
          .           }
          .           
          .           void TLSData::release()
          .           {
          1 ( 0.00%)      memPool->extMemPool.allLocalCaches.unregisterThread(this);
          .               externalCleanup(/*cleanOnlyUnused=*/false, /*cleanBins=*/false);
          .           
         90 ( 0.00%)      for (unsigned index = 0; index < numBlockBins; index++) {
          .                   Block *activeBlk = bin[index].getActiveBlock();
         58 ( 0.00%)          if (!activeBlk)
          .                       continue;
         21 ( 0.00%)          Block *threadlessBlock = activeBlk->previous;
         21 ( 0.00%)          bool syncOnMailbox = false;
         63 ( 0.00%)          while (threadlessBlock) {
          .                       Block *threadBlock = threadlessBlock->previous;
          .                       if (threadlessBlock->empty()) {
          .                           /* we destroy the thread, so not use its block pool */
          .                           memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
          .                       } else {
          .                           memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
          .                           syncOnMailbox = true;
          .                       }
          .                       threadlessBlock = threadBlock;
          .                   }
          .                   threadlessBlock = activeBlk;
         42 ( 0.00%)          while (threadlessBlock) {
         21 ( 0.00%)              Block *threadBlock = threadlessBlock->next;
          .                       if (threadlessBlock->empty()) {
          .                           /* we destroy the thread, so not use its block pool */
         54 ( 0.00%)                  memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
      6,537 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (18x)
          .                       } else {
         33 ( 0.00%)                  memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
        176 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::put(long, rml::internal::Block*) (3x)
          4 ( 0.00%)                  syncOnMailbox = true;
          .                       }
          .                       threadlessBlock = threadBlock;
          .                   }
          .                   bin[index].resetActiveBlock();
          .           
         42 ( 0.00%)          if (syncOnMailbox) {
          .                       // Although, we synchronized on nextPrivatizable inside a block, we still need to
          .                       // synchronize on the bin lifetime because the thread releasing an object into the public 
          .                       // free list is touching the bin (mailbox and mailLock)
          3 ( 0.00%)              MallocMutex::scoped_lock scoped_cs(bin[index].mailLock);
          .                   }
          .               }
          .           }
          .           
          .           
          .           #if MALLOC_CHECK_RECURSION
          .           // TODO: Use dedicated heap for this
          .           
-- line 1818 ----------------------------------------
-- line 1824 ----------------------------------------
          .            * allocations are performed by moving bump pointer and increasing of object counter,
          .            * releasing is done via counter of objects allocated in the block
          .            * or moving bump pointer if releasing object is on a bound.
          .            * TODO: make bump pointer to grow to the same backward direction as all the others.
          .            */
          .           
          .           class StartupBlock : public Block {
          .               size_t availableSize() const {
         24 ( 0.00%)          return slabSize - ((uintptr_t)bumpPtr - (uintptr_t)this);
          .               }
          .               static StartupBlock *getBlock();
          .           public:
          .               static FreeObject *allocate(size_t size);
          .               static size_t msize(void *ptr) { return *((size_t*)ptr - 1); }
          .               void free(void *ptr);
          .           };
          .           
          .           static MallocMutex startupMallocLock;
          .           static StartupBlock *firstStartupBlock;
          .           
          .           StartupBlock *StartupBlock::getBlock()
          .           {
          3 ( 0.00%)      BackRefIdx backRefIdx = BackRefIdx::newBackRef(/*largeObj=*/false);
         72 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (1x)
          2 ( 0.00%)      if (backRefIdx.isInvalid()) return nullptr;
          .           
          .               StartupBlock *block = static_cast<StartupBlock*>(
          2 ( 0.00%)          defaultMemPool->extMemPool.backend.getSlabBlock(1));
          2 ( 0.00%)      if (!block) return nullptr;
          .           
          .               block->cleanBlockHeader();
          .               setBackRef(backRefIdx, block);
          4 ( 0.00%)      block->backRefIdx = backRefIdx;
          .               // use startupAllocObjSizeMark to mark objects from startup block marker
          2 ( 0.00%)      block->objectSize = startupAllocObjSizeMark;
          3 ( 0.00%)      block->bumpPtr = (FreeObject *)((uintptr_t)block + sizeof(StartupBlock));
          .               return block;
          .           }
          .           
          .           FreeObject *StartupBlock::allocate(size_t size)
          .           {
          .               FreeObject *result;
          .               StartupBlock *newBlock = nullptr;
          .               bool newBlockUnused = false;
          .           
          .               /* Objects must be aligned on their natural bounds,
          .                  and objects bigger than word on word's bound. */
          .               size = alignUp(size, sizeof(size_t));
          .               // We need size of an object to implement msize.
          7 ( 0.00%)      size_t reqSize = size + sizeof(size_t);
          .               {
          .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
          .                   // Re-check whether we need a new block (conditions might have changed)
         33 ( 0.00%)          if (!firstStartupBlock || firstStartupBlock->availableSize() < reqSize) {
          .                       if (!newBlock) {
          .                           newBlock = StartupBlock::getBlock();
          .                           if (!newBlock) return nullptr;
          .                       }
          2 ( 0.00%)              newBlock->next = (Block*)firstStartupBlock;
          2 ( 0.00%)              if (firstStartupBlock)
          .                           firstStartupBlock->previous = (Block*)newBlock;
          1 ( 0.00%)              firstStartupBlock = newBlock;
          .                   }
          .                   result = firstStartupBlock->bumpPtr;
          7 ( 0.00%)          firstStartupBlock->allocatedCount++;
          .                   firstStartupBlock->bumpPtr =
         14 ( 0.00%)              (FreeObject *)((uintptr_t)firstStartupBlock->bumpPtr + reqSize);
          .               }
          .           
          .               // keep object size at the negative offset
          7 ( 0.00%)      *((size_t*)result) = size;
          7 ( 0.00%)      return (FreeObject*)((size_t*)result+1);
          .           }
          .           
          .           void StartupBlock::free(void *ptr)
          .           {
          .               Block* blockToRelease = nullptr;
          .               {
          .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
          .           
          .                   MALLOC_ASSERT(firstStartupBlock, ASSERT_TEXT);
          .                   MALLOC_ASSERT(startupAllocObjSizeMark==objectSize
          .                                 && allocatedCount>0, ASSERT_TEXT);
          .                   MALLOC_ASSERT((uintptr_t)ptr>=(uintptr_t)this+sizeof(StartupBlock)
          .                                 && (uintptr_t)ptr+StartupBlock::msize(ptr)<=(uintptr_t)this+slabSize,
          .                                 ASSERT_TEXT);
         12 ( 0.00%)          if (0 == --allocatedCount) {
          .                       if (this == firstStartupBlock)
          .                           firstStartupBlock = (StartupBlock*)firstStartupBlock->next;
          .                       if (previous)
          .                           previous->next = next;
          .                       if (next)
          .                           next->previous = previous;
          .                       blockToRelease = this;
         24 ( 0.00%)          } else if ((uintptr_t)ptr + StartupBlock::msize(ptr) == (uintptr_t)bumpPtr) {
          .                       // last object in the block released
          8 ( 0.00%)              FreeObject *newBump = (FreeObject*)((size_t*)ptr - 1);
          .                       MALLOC_ASSERT((uintptr_t)newBump>(uintptr_t)this+sizeof(StartupBlock),
          .                                     ASSERT_TEXT);
          .                       bumpPtr = newBump;
          .                   }
          .               }
          .               if (blockToRelease) {
          .                   blockToRelease->previous = blockToRelease->next = nullptr;
        221 ( 0.00%)          defaultMemPool->returnEmptyBlock(blockToRelease, /*poolTheBlock=*/false);
     57,020 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (221x)
          .               }
          .           }
          .           
          .           #endif /* MALLOC_CHECK_RECURSION */
          .           
          .           /********* End thread related code  *************/
          .           
          .           /********* Library initialization *************/
-- line 1935 ----------------------------------------
-- line 1997 ----------------------------------------
          .           
          .           inline bool isMallocInitialized() {
          .               // Load must have acquire fence; otherwise thread taking "initialized" path
          .               // might perform textually later loads *before* mallocInitialized becomes 2.
          .               return 2 == mallocInitialized.load(std::memory_order_acquire);
          .           }
          .           
          .           /* Caller is responsible for ensuring this routine is called exactly once. */
          6 ( 0.00%)  extern "C" void MallocInitializeITT() {
          .           #if __TBB_USE_ITT_NOTIFY
          .               if (!usedBySrcIncluded)
          .                   tbb::detail::r1::__TBB_load_ittnotify();
          .           #endif
          6 ( 0.00%)  }
          .           
          .           void MemoryPool::initDefaultPool() {
          .               hugePages.init();
          .           }
          .           
          .           /*
          .            * Allocator initialization routine;
          .            * it is called lazily on the very first scalable_malloc call.
-- line 2018 ----------------------------------------
-- line 2025 ----------------------------------------
          .               MALLOC_ASSERT( sizeof(FreeObject) == sizeof(void*), ASSERT_TEXT );
          .               MALLOC_ASSERT( isAligned(defaultMemPool, sizeof(intptr_t)),
          .                              "Memory pool must be void*-aligned for atomic to work over aligned arguments.");
          .           
          .           #if USE_WINTHREAD
          .               const size_t granularity = 64*1024; // granulatity of VirtualAlloc
          .           #else
          .               // POSIX.1-2001-compliant way to get page size
          7 ( 0.00%)      const size_t granularity = sysconf(_SC_PAGESIZE);
        829 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          .           #endif
          3 ( 0.00%)      if (!defaultMemPool) {
          .                   // Do not rely on static constructors and do the assignment in case
          .                   // of library static section not initialized at this call yet.
          .                   defaultMemPool = (MemoryPool*)defaultMemPool_space;
          .               }
          .               bool initOk = defaultMemPool->
          1 ( 0.00%)          extMemPool.init(0, nullptr, nullptr, granularity,
          .                                   /*keepAllMemory=*/false, /*fixedPool=*/false);
          .           // TODO: extMemPool.init() to not allocate memory
          2 ( 0.00%)      if (!initOk || !initBackRefMain(&defaultMemPool->extMemPool.backend) || !ThreadId::init())
          .                   return false;
          .               MemoryPool::initDefaultPool();
          .               // init() is required iff initMemoryManager() is called
          .               // after mallocProcessShutdownNotification()
          .               shutdownSync.init();
          .           #if COLLECT_STATISTICS
          .               initStatisticsCollection();
          .           #endif
-- line 2052 ----------------------------------------
-- line 2056 ----------------------------------------
          .           static bool GetBoolEnvironmentVariable(const char* name) {
          .               return tbb::detail::r1::GetBoolEnvironmentVariable(name);
          .           }
          .           
          .           //! Ensures that initMemoryManager() is called once and only once.
          .           /** Does not return until initMemoryManager() has been completed by a thread.
          .               There is no need to call this routine if mallocInitialized==2 . */
          .           static bool doInitialization()
          8 ( 0.00%)  {
          .               MallocMutex::scoped_lock lock( initMutex );
          2 ( 0.00%)      if (mallocInitialized.load(std::memory_order_relaxed)!=2) {
          .                   MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==0, ASSERT_TEXT );
          .                   mallocInitialized.store(1, std::memory_order_relaxed);
          .                   RecursiveMallocCallProtector scoped;
          .                   if (!initMemoryManager()) {
          .                       mallocInitialized.store(0, std::memory_order_relaxed); // restore and out
          .                       return false;
          .                   }
          .           #ifdef  MALLOC_EXTRA_INITIALIZATION
-- line 2074 ----------------------------------------
-- line 2084 ----------------------------------------
          .                   mallocInitialized.store(2, std::memory_order_release);
          .                   if( GetBoolEnvironmentVariable("TBB_VERSION") ) {
          .                       fputs(VersionString+1,stderr);
          .                       hugePages.printStatus();
          .                   }
          .               }
          .               /* It can't be 0 or I would have initialized it */
          .               MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==2, ASSERT_TEXT );
          3 ( 0.00%)      return true;
          9 ( 0.00%)  }
          .           
          .           /********* End library initialization *************/
          .           
          .           /********* The malloc show begins     *************/
          .           
          .           
          .           FreeObject *Block::allocateFromFreeList()
          .           {
          .               FreeObject *result;
          .           
 91,047,408 ( 0.45%)      if (!freeList) return nullptr;
          .           
          .               result = freeList;
          .               MALLOC_ASSERT( result, ASSERT_TEXT );
          .           
 42,619,114 ( 0.21%)      freeList = result->next;
          .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
 21,309,557 ( 0.11%)      allocatedCount++;
          .               STAT_increment(getThreadId(), getIndex(objectSize), allocFreeListUsed);
          .           
          .               return result;
          .           }
          .           
          .           FreeObject *Block::allocateFromBumpPtr()
          .           {
  9,039,579 ( 0.05%)      FreeObject *result = bumpPtr;
 18,079,158 ( 0.09%)      if (result) {
 63,158,375 ( 0.31%)          bumpPtr = (FreeObject *) ((uintptr_t) bumpPtr - objectSize);
  9,022,625 ( 0.04%)          if ( (uintptr_t)bumpPtr < (uintptr_t)this+sizeof(Block) ) {
          .                       bumpPtr = nullptr;
          .                   }
          .                   MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
  9,022,625 ( 0.04%)          allocatedCount++;
          .                   STAT_increment(getThreadId(), getIndex(objectSize), allocBumpPtrUsed);
          .               }
  9,022,625 ( 0.04%)      return result;
          .           }
          .           
          .           inline FreeObject* Block::allocate()
          .           {
          .               MALLOC_ASSERT( isOwnedByCurrentThread(), ASSERT_TEXT );
          .           
          .               /* for better cache locality, first looking in the free list. */
          .               if ( FreeObject *result = allocateFromFreeList() ) {
-- line 2137 ----------------------------------------
-- line 2141 ----------------------------------------
          .           
          .               /* if free list is empty, try thread local bump pointer allocation. */
          .               if ( FreeObject *result = allocateFromBumpPtr() ) {
          .                   return result;
          .               }
          .               MALLOC_ASSERT( !bumpPtr, ASSERT_TEXT );
          .           
          .               /* the block is considered full. */
     16,954 ( 0.00%)      isFull = true;
          .               return nullptr;
          .           }
          .           
          .           size_t Block::findObjectSize(void *object) const
          .           {
          .               size_t blSize = getSize();
          .           #if MALLOC_CHECK_RECURSION
          .               // Currently, there is no aligned allocations from startup blocks,
-- line 2157 ----------------------------------------
-- line 2165 ----------------------------------------
          .                   blSize - ((uintptr_t)object - (uintptr_t)findObjectToFree(object));
          .               MALLOC_ASSERT(size>0 && size<minLargeObjectSize, ASSERT_TEXT);
          .               return size;
          .           }
          .           
          .           void Bin::moveBlockToFront(Block *block)
          .           {
          .               /* move the block to the front of the bin */
     50,862 ( 0.00%)      if (block == activeBlk) return;
          .               outofTLSBin(block);
          .               pushTLSBin(block);
          .           }
          .           
          .           void Bin::processEmptyBlock(Block *block, bool poolTheBlock)
          .           {
 26,968,536 ( 0.13%)      if (block != activeBlk) {
          .                   /* We are not using this block; return it to the pool */
          .                   outofTLSBin(block);
        663 ( 0.00%)          block->getMemPool()->returnEmptyBlock(block, poolTheBlock);
          .               } else {
          .                   /* all objects are free - let's restore the bump pointer */
          .                   block->restoreBumpPtr();
          .               }
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::put(LargeMemoryBlock *object, ExtMemoryPool *extMemPool)
          .           {
         76 ( 0.00%)      const size_t size = object->unalignedSize;
          .               // not spoil cache with too large object, that can cause its total cleanup
        152 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
          .                   return false;
          .               LargeMemoryBlock *localHead = head.exchange(nullptr);
          .           
         76 ( 0.00%)      object->prev = nullptr;
         76 ( 0.00%)      object->next = localHead;
        152 ( 0.00%)      if (localHead)
         75 ( 0.00%)          localHead->prev = object;
          .               else {
          .                   // those might not be cleaned during local cache stealing, correct them
          .                   totalSize = 0;
          .                   numOfBlocks = 0;
          1 ( 0.00%)          tail = object;
          .               }
          .               localHead = object;
        151 ( 0.00%)      totalSize += size;
        227 ( 0.00%)      numOfBlocks++;
          .               // must meet both size and number of cached objects constrains
        374 ( 0.00%)      if (totalSize > MAX_TOTAL_SIZE || numOfBlocks >= HIGH_MARK) {
          .                   // scanning from tail until meet conditions
        100 ( 0.00%)          while (totalSize > MAX_TOTAL_SIZE || numOfBlocks > LOW_MARK) {
         48 ( 0.00%)              totalSize -= tail->unalignedSize;
         48 ( 0.00%)              numOfBlocks--;
         48 ( 0.00%)              tail = tail->prev;
          .                   }
          3 ( 0.00%)          LargeMemoryBlock *headToRelease = tail->next;
          1 ( 0.00%)          tail->next = nullptr;
          .           
          .                   extMemPool->freeLargeObjectList(headToRelease);
          .               }
          .           
          .               head.store(localHead, std::memory_order_release);
          .               return true;
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           LargeMemoryBlock *LocalLOCImpl<LOW_MARK, HIGH_MARK>::get(size_t size)
          .           {
          .               LargeMemoryBlock *localHead, *res = nullptr;
          .           
        154 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
          .                   return nullptr;
          .           
          .               // TBB_REVAMP_TODO: review this line
        446 ( 0.00%)      if (!head.load(std::memory_order_acquire) || (localHead = head.exchange(nullptr)) == nullptr) {
          .                   // do not restore totalSize, numOfBlocks and tail at this point,
          .                   // as they are used only in put(), where they must be restored
          .                   return nullptr;
          .               }
          .           
        912 ( 0.00%)      for (LargeMemoryBlock *curr = localHead; curr; curr=curr->next) {
      1,976 ( 0.00%)          if (curr->unalignedSize == size) {
          .                       res = curr;
        570 ( 0.00%)              if (curr->next)
         69 ( 0.00%)                  curr->next->prev = curr->prev;
          .                       else
         14 ( 0.00%)                  tail = curr->prev;
         76 ( 0.00%)              if (curr != localHead)
         52 ( 0.00%)                  curr->prev->next = curr->next;
          .                       else
          .                           localHead = curr->next;
         38 ( 0.00%)              totalSize -= size;
         38 ( 0.00%)              numOfBlocks--;
          .                       break;
          .                   }
          .               }
          .           
          .               head.store(localHead, std::memory_order_release);
          .               return res;
          .           }
          .           
          .           template<int LOW_MARK, int HIGH_MARK>
          .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::externalCleanup(ExtMemoryPool *extMemPool)
          .           {
          2 ( 0.00%)      if (LargeMemoryBlock *localHead = head.exchange(nullptr)) {
          .                   extMemPool->freeLargeObjectList(localHead);
          .                   return true;
          .               }
          .               return false;
          .           }
          .           
          .           void *MemoryPool::getFromLLOCache(TLSData* tls, size_t size, size_t alignment)
        847 ( 0.00%)  {
          .               LargeMemoryBlock *lmb = nullptr;
          .           
          .               size_t headersSize = sizeof(LargeMemoryBlock)+sizeof(LargeObjectHdr);
         77 ( 0.00%)      size_t allocationSize = LargeObjectCache::alignToBin(size+headersSize+alignment);
        154 ( 0.00%)      if (allocationSize < size) // allocationSize is wrapped around after alignToBin
          .                   return nullptr;
          .               MALLOC_ASSERT(allocationSize >= alignment, "Overflow must be checked before.");
          .           
        154 ( 0.00%)      if (tls) {
          .                   tls->markUsed();
          .                   lmb = tls->lloc.get(allocationSize);
          .               }
          .               if (!lmb)
          .                   lmb = extMemPool.mallocLargeObject(this, allocationSize);
          .           
          .               if (lmb) {
          .                   // doing shuffle we suppose that alignment offset guarantees
          .                   // that different cache lines are in use
          .                   MALLOC_ASSERT(alignment >= estimatedCacheLineSize, ASSERT_TEXT);
          .           
          .                   void *alignedArea = (void*)alignUp((uintptr_t)lmb+headersSize, alignment);
          .                   uintptr_t alignedRight =
        230 ( 0.00%)              alignDown((uintptr_t)lmb+lmb->unalignedSize - size, alignment);
          .                   // Has some room to shuffle object between cache lines?
          .                   // Note that alignedRight and alignedArea are aligned at alignment.
          .                   unsigned ptrDelta = alignedRight - (uintptr_t)alignedArea;
        308 ( 0.00%)          if (ptrDelta && tls) { // !tls is cold path
          .                       // for the hot path of alignment==estimatedCacheLineSize,
          .                       // allow compilers to use shift for division
          .                       // (since estimatedCacheLineSize is a power-of-2 constant)
        308 ( 0.00%)              unsigned numOfPossibleOffsets = alignment == estimatedCacheLineSize?
          .                             ptrDelta / estimatedCacheLineSize :
          .                             ptrDelta / alignment;
        231 ( 0.00%)              unsigned myCacheIdx = ++tls->currCacheIdx;
          .                       unsigned offset = myCacheIdx % numOfPossibleOffsets;
          .           
          .                       // Move object to a cache line with an offset that is different from
          .                       // previous allocation. This supposedly allows us to use cache
          .                       // associativity more efficiently.
        308 ( 0.00%)              alignedArea = (void*)((uintptr_t)alignedArea + offset*alignment);
          .                   }
          .                   MALLOC_ASSERT((uintptr_t)lmb+lmb->unalignedSize >=
          .                                 (uintptr_t)alignedArea+size, "Object doesn't fit the block.");
         77 ( 0.00%)          LargeObjectHdr *header = (LargeObjectHdr*)alignedArea-1;
         77 ( 0.00%)          header->memoryBlock = lmb;
        308 ( 0.00%)          header->backRefIdx = lmb->backRefIdx;
          .                   setBackRef(header->backRefIdx, header);
          .           
         77 ( 0.00%)          lmb->objectSize = size;
          .           
          .                   MALLOC_ASSERT( isLargeObject<unknownMem>(alignedArea), ASSERT_TEXT );
          .                   MALLOC_ASSERT( isAligned(alignedArea, alignment), ASSERT_TEXT );
          .           
          .                   return alignedArea;
          .               }
          .               return nullptr;
        616 ( 0.00%)  }
          .           
          .           void MemoryPool::putToLLOCache(TLSData *tls, void *object)
        228 ( 0.00%)  {
          .               LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
          .               // overwrite backRefIdx to simplify double free detection
        228 ( 0.00%)      header->backRefIdx = BackRefIdx();
          .           
        152 ( 0.00%)      if (tls) {
          .                   tls->markUsed();
         76 ( 0.00%)          if (tls->lloc.put(header->memoryBlock, &extMemPool))
          .                       return;
          .               }
          .               extMemPool.freeLargeObject(header->memoryBlock);
        304 ( 0.00%)  }
          .           
          .           /*
          .            * All aligned allocations fall into one of the following categories:
          .            *  1. if both request size and alignment are <= maxSegregatedObjectSize,
          .            *       we just align the size up, and request this amount, because for every size
          .            *       aligned to some power of 2, the allocated object is at least that aligned.
          .            * 2. for size<minLargeObjectSize, check if already guaranteed fittingAlignment is enough.
          .            * 3. if size+alignment<minLargeObjectSize, we take an object of fittingSizeN and align
-- line 2356 ----------------------------------------
-- line 2453 ----------------------------------------
          .               return 0 == ((uintptr_t)this + slabSize - (uintptr_t)object) % objectSize;
          .           }
          .           #endif
          .           
          .           /* Finds the real object inside the block */
          .           FreeObject *Block::findAllocatedObject(const void *address) const
          .           {
          .               // calculate offset from the end of the block space
         33 ( 0.00%)      uint16_t offset = (uintptr_t)this + slabSize - (uintptr_t)address;
          .               MALLOC_ASSERT( offset<=slabSize-sizeof(Block), ASSERT_TEXT );
          .               // find offset difference from a multiple of allocation size
         22 ( 0.00%)      offset %= objectSize;
          .               // and move the address down to where the real object starts.
         22 ( 0.00%)      return (FreeObject*)((uintptr_t)address - (offset? objectSize-offset: 0));
          .           }
          .           
          .           /*
          .            * Bad dereference caused by a foreign pointer is possible only here, not earlier in call chain.
          .            * Separate function isolates SEH code, as it has bad influence on compiler optimization.
          .            */
          .           static inline BackRefIdx safer_dereference (const BackRefIdx *ptr)
          .           {
-- line 2474 ----------------------------------------
-- line 2478 ----------------------------------------
          .           #endif
          .                   id = dereference(ptr);
          .           #if _MSC_VER
          .               } __except( GetExceptionCode() == EXCEPTION_ACCESS_VIOLATION?
          .                           EXCEPTION_EXECUTE_HANDLER : EXCEPTION_CONTINUE_SEARCH ) {
          .                   id = BackRefIdx();
          .               }
          .           #endif
151,660,925 ( 0.76%)      return id;
          .           }
          .           
          .           template<MemoryOrigin memOrigin>
          .           bool isLargeObject(void *object)
          .           {
 64,962,150 ( 0.32%)      if (!isAligned(object, largeObjectAlignment))
 51,592,315 ( 0.26%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] (4,297,628x)
          .                   return false;
        773 ( 0.00%)      LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
          .               BackRefIdx idx = (memOrigin == unknownMem) ?
          .                   safer_dereference(&header->backRefIdx) : dereference(&header->backRefIdx);
          .           
          .               return idx.isLargeObject()
          .                   // in valid LargeObjectHdr memoryBlock is not nullptr
      2,319 ( 0.00%)          && header->memoryBlock
          .                   // in valid LargeObjectHdr memoryBlock points somewhere before header
          .                   // TODO: more strict check
      1,546 ( 0.00%)          && (uintptr_t)header->memoryBlock < (uintptr_t)header
  8,599,814 ( 0.04%)          && getBackRef(idx) == header;
     11,583 ( 0.00%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (757x)
  4,297,628 ( 0.02%)  }
          .           
          .           static inline bool isSmallObject (void *ptr)
          .           {
          .               Block* expectedBlock = (Block*)alignDown(ptr, slabSize);
          .               const BackRefIdx* idx = expectedBlock->getBackRefIdx();
          .           
 30,332,185 ( 0.15%)      bool isSmall = expectedBlock == getBackRef(safer_dereference(idx));
545,979,330 ( 2.72%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (30,332,185x)
          .               if (isSmall)
          .                   expectedBlock->checkFreePrecond(ptr);
          .               return isSmall;
          .           }
          .           
          .           /**** Check if an object was allocated by scalable_malloc ****/
          .           static inline bool isRecognized (void* ptr)
          .           {
          .               return defaultMemPool->extMemPool.backend.ptrCanBeValid(ptr) &&
          .                   (isLargeObject<unknownMem>(ptr) || isSmallObject(ptr));
          .           }
          .           
          .           static inline void freeSmallObject(void *object)
181,993,110 ( 0.91%)  {
          .               /* mask low bits to get the block */
          .               Block *block = (Block *)alignDown(object, slabSize);
          .               block->checkFreePrecond(object);
          .           
          .           #if MALLOC_CHECK_RECURSION
 60,664,370 ( 0.30%)      if (block->isStartupAllocObject()) {
          .                   ((StartupBlock *)block)->free(object);
          .                   return;
          .               }
          .           #endif
          .               if (block->isOwnedByCurrentThread()) {
          .                   block->freeOwnObject(object);
          .               } else { /* Slower path to add to the shared list, the allocatedCount is updated by the owner thread in malloc. */
          .                   FreeObject *objectToFree = block->findObjectToFree(object);
          .                   block->freePublicObject(objectToFree);
          .               }
160,650,222 ( 0.80%)  }
          .           
          .           static void *internalPoolMalloc(MemoryPool* memPool, size_t size)
272,990,331 ( 1.36%)  {
          .               Bin* bin;
          .               Block * mallocBlock;
          .           
 60,664,518 ( 0.30%)      if (!memPool) return nullptr;
          .           
 60,664,518 ( 0.30%)      if (!size) size = sizeof(size_t);
          .           
          .               TLSData *tls = memPool->getTLS(/*create=*/true);
          .           
          .               /* Allocate a large object */
 60,664,518 ( 0.30%)      if (size >= minLargeObjectSize)
        385 ( 0.00%)          return memPool->getFromLLOCache(tls, size, largeObjectAlignment);
     37,423 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getFromLLOCache(rml::internal::TLSData*, unsigned long, unsigned long) (77x)
          .           
          2 ( 0.00%)      if (!tls) return nullptr;
          .           
          .               tls->markUsed();
          .               /*
          .                * Get an element in thread-local array corresponding to the given size;
          .                * It keeps ptr to the active block for allocations of this size
          .                */
          .               bin = tls->getAllocationBin(size);
          .               if ( !bin ) return nullptr;
          .           
          .               /* Get a block to try to allocate in. */
 91,013,258 ( 0.45%)      for( mallocBlock = bin->getActiveBlock(); mallocBlock;
          .                    mallocBlock = bin->setPreviousBlockActive() ) // the previous block should be empty enough
          .               {
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .               }
          .           
          .               /*
          .                * else privatize publicly freed objects in some block and allocate from it
-- line 2579 ----------------------------------------
-- line 2586 ----------------------------------------
          .                   /* Else something strange happened, need to retry from the beginning; */
          .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in public free list; reentering.\n" ));
          .                   return internalPoolMalloc(memPool, size);
          .               }
          .           
          .               /*
          .                * no suitable own blocks, try to get a partial block that some other thread has discarded.
          .                */
      1,452 ( 0.00%)      mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
      8,904 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::get(rml::internal::TLSData*, unsigned int) (242x)
        484 ( 0.00%)      while (mallocBlock) {
          .                   bin->pushTLSBin(mallocBlock);
          .                   bin->setActiveBlock(mallocBlock); // TODO: move under the below condition?
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .                   mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
          .               }
          .           
          .               /*
          .                * else try to get a new empty block
          .                */
        726 ( 0.00%)      mallocBlock = memPool->getEmptyBlock(size);
     83,523 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (242x)
        484 ( 0.00%)      if (mallocBlock) {
          .                   bin->pushTLSBin(mallocBlock);
          .                   bin->setActiveBlock(mallocBlock);
          .                   if( FreeObject *result = mallocBlock->allocate() )
          .                       return result;
          .                   /* Else something strange happened, need to retry from the beginning; */
          .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in empty block; reentering.\n" ));
          .                   return internalPoolMalloc(memPool, size);
          .               }
          .               /*
          .                * else nothing works so return nullptr
          .                */
          .               TRACEF(( "[ScalableMalloc trace] No memory found, returning nullptr.\n" ));
          .               return nullptr;
272,990,177 ( 1.36%)  }
          .           
          .           // When size==0 (i.e. unknown), detect here whether the object is large.
          .           // For size is known and < minLargeObjectSize, we still need to check
          .           // if the actual object is large, because large objects might be used
          .           // for aligned small allocations.
          .           static bool internalPoolFree(MemoryPool *memPool, void *object, size_t size)
          .           {
          .               if (!memPool || !object) return false;
-- line 2629 ----------------------------------------
-- line 2637 ----------------------------------------
          .               if (size >= minLargeObjectSize || isLargeObject<ourMem>(object))
          .                   memPool->putToLLOCache(memPool->getTLS(/*create=*/false), object);
          .               else
          .                   freeSmallObject(object);
          .               return true;
          .           }
          .           
          .           static void *internalMalloc(size_t size)
181,993,596 ( 0.91%)  {
 90,996,798 ( 0.45%)      if (!size) size = sizeof(size_t);
          .           
          .           #if MALLOC_CHECK_RECURSION
          .               if (RecursiveMallocCallProtector::sameThreadActive())
         14 ( 0.00%)          return size<minLargeObjectSize? StartupBlock::allocate(size) :
          .                       // nested allocation, so skip tls
          .                       (FreeObject*)defaultMemPool->getFromLLOCache(nullptr, size, slabSize);
          .           #endif
          .           
 60,664,518 ( 0.30%)      if (!isMallocInitialized())
          3 ( 0.00%)          if (!doInitialization())
    142,905 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::doInitialization() (1x)
          .                       return nullptr;
 90,996,777 ( 0.45%)      return internalPoolMalloc(defaultMemPool, size);
2,365,515,282 (11.80%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) (30,332,259x)
151,661,337 ( 0.76%)  }
          .           
          .           static void internalFree(void *object)
          .           {
          .               internalPoolFree(defaultMemPool, object, 0);
          .           }
          .           
          .           static size_t internalMsize(void* ptr)
          .           {
-- line 2667 ----------------------------------------
-- line 2840 ----------------------------------------
          .            * unless that value is nullptr.
          .            * For Windows, it is called from DllMain( DLL_THREAD_DETACH ).
          .            *
          .            * However neither of the above is called for the main process thread, so the routine
          .            * also needs to be called during the process shutdown.
          .            *
          .           */
          .           // TODO: Consider making this function part of class MemoryPool.
          4 ( 0.00%)  void doThreadShutdownNotification(TLSData* tls, bool main_thread)
          .           {
          .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return start %d\n",
          .                        getThreadId(),  threadGoingDownCount++ ));
          .           
          .           #if USE_PTHREAD
          .               if (tls) {
          .                   if (!shutdownSync.threadDtorStart()) return;
          .                   tls->getMemPool()->onThreadShutdown(tls);
-- line 2856 ----------------------------------------
-- line 2858 ----------------------------------------
          .               } else
          .           #endif
          .               {
          .                   suppress_unused_warning(tls); // not used on Windows
          .                   // The default pool is safe to use at this point:
          .                   //   on Linux, only the main thread can go here before destroying defaultMemPool;
          .                   //   on Windows, shutdown is synchronized via loader lock and isMallocInitialized().
          .                   // See also __TBB_mallocProcessShutdownNotification()
          1 ( 0.00%)          defaultMemPool->onThreadShutdown(defaultMemPool->getTLS(/*create=*/false));
          .                   // Take lock to walk through other pools; but waiting might be dangerous at this point
          .                   // (e.g. on Windows the main thread might deadlock)
          .                   bool locked;
          .                   MallocMutex::scoped_lock lock(MemoryPool::memPoolListLock, /*wait=*/!main_thread, &locked);
          3 ( 0.00%)          if (locked) { // the list is safe to process
          4 ( 0.00%)              for (MemoryPool *memPool = defaultMemPool->next; memPool; memPool = memPool->next)
          .                           memPool->onThreadShutdown(memPool->getTLS(/*create=*/false));
          .                   }
          .               }
          .           
          .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return end\n", getThreadId() ));
          4 ( 0.00%)  }
          .           
          .           #if USE_PTHREAD
          .           void mallocThreadShutdownNotification(void* arg)
          .           {
          .               // The routine is called for each pool (as TLS dtor) on each thread, except for the main thread
          .               if (!isMallocInitialized()) return;
          .               doThreadShutdownNotification((TLSData*)arg, false);
          .           }
-- line 2886 ----------------------------------------
-- line 2890 ----------------------------------------
          .               // The routine is called once per thread on Windows
          .               if (!isMallocInitialized()) return;
          .               doThreadShutdownNotification(nullptr, false);
          .           }
          .           #endif
          .           
          .           extern "C" void __TBB_mallocProcessShutdownNotification(bool windows_process_dying)
          .           {
          2 ( 0.00%)      if (!isMallocInitialized()) return;
          .           
          .               // Don't clean allocator internals if the entire process is exiting
          .               if (!windows_process_dying) {
          .                   doThreadShutdownNotification(nullptr, /*main_thread=*/true);
          .               }
          .           #if  __TBB_MALLOC_LOCACHE_STAT
          .               printf("cache hit ratio %f, size hit %f\n",
          .                      1.*cacheHits/mallocCalls, 1.*memHitKB/memAllocKB);
-- line 2906 ----------------------------------------
-- line 2925 ----------------------------------------
          .               for( int i=1; i<=nThreads && i<MAX_THREADS; ++i )
          .                   STAT_print(i);
          .           #endif
          .               if (!usedBySrcIncluded)
          .                   MALLOC_ITT_FINI_ITTLIB();
          .           }
          .           
          .           extern "C" void * scalable_malloc(size_t size)
 30,332,264 ( 0.15%)  {
 60,664,528 ( 0.30%)      void *ptr = internalMalloc(size);
3,063,153,042 (15.28%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (30,332,258x)
      1,107 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long)'2 (6x)
 60,664,528 ( 0.30%)      if (!ptr) errno = ENOMEM;
          .               return ptr;
 90,996,792 ( 0.45%)  }
          .           
          .           extern "C" void scalable_free(void *object)
          .           {
          .               internalFree(object);
          .           }
          .           
          .           #if MALLOC_ZONE_OVERLOAD_ENABLED
          .           extern "C" void __TBB_malloc_free_definite_size(void *object, size_t size)
-- line 2945 ----------------------------------------
-- line 2948 ----------------------------------------
          .           }
          .           #endif
          .           
          .           /*
          .            * A variant that provides additional memory safety, by checking whether the given address
          .            * was obtained with this allocator, and if not redirecting to the provided alternative call.
          .            */
          .           extern "C" TBBMALLOC_EXPORT void __TBB_malloc_safer_free(void *object, void (*original_free)(void*))
 30,332,261 ( 0.15%)  {
 60,664,522 ( 0.30%)      if (!object)
          .                   return;
          .           
          .               // tbbmalloc can allocate object only when tbbmalloc has been initialized
 90,996,783 ( 0.45%)      if (mallocInitialized.load(std::memory_order_acquire) && defaultMemPool->extMemPool.backend.ptrCanBeValid(object)) {
  8,595,256 ( 0.04%)          if (isLargeObject<unknownMem>(object)) {
          .                       // must check 1st for large object, because small object check touches 4 pages on left,
          .                       // and it can be inaccessible
          .                       TLSData *tls = defaultMemPool->getTLS(/*create=*/false);
          .           
        228 ( 0.00%)              defaultMemPool->putToLLOCache(tls, object);
      6,816 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::putToLLOCache(rml::internal::TLSData*, void*) (76x)
          .                       return;
 60,664,370 ( 0.30%)          } else if (isSmallObject(object)) {
 60,664,370 ( 0.30%)              freeSmallObject(object);
1,418,032,330 ( 7.07%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] (30,332,185x)
          .                       return;
          .                   }
          .               }
          .               if (original_free)
          .                   original_free(object);
 30,332,261 ( 0.15%)  }
          .           
          .           /********* End the free code        *************/
          .           
          .           /********* Code for scalable_realloc       ***********/
          .           
          .           /*
          .            * From K&R
          .            * "realloc changes the size of the object pointed to by p to size. The contents will
-- line 2984 ----------------------------------------
-- line 3058 ----------------------------------------
          .            * From K&R
          .            * calloc returns a pointer to space for an array of nobj objects,
          .            * each of size size, or nullptr if the request cannot be satisfied.
          .            * The space is initialized to zero bytes.
          .            *
          .            */
          .           
          .           extern "C" void * scalable_calloc(size_t nobj, size_t size)
          6 ( 0.00%)  {
          .               // it's square root of maximal size_t value
          .               const size_t mult_not_overflow = size_t(1) << (sizeof(size_t)*CHAR_BIT/2);
          4 ( 0.00%)      const size_t arraySize = nobj * size;
          .           
          .               // check for overflow during multiplication:
         10 ( 0.00%)      if (nobj>=mult_not_overflow || size>=mult_not_overflow) // 1) heuristic check
          4 ( 0.00%)          if (nobj && arraySize / nobj != size) {             // 2) exact check
          .                       errno = ENOMEM;
          .                       return nullptr;
          .                   }
          6 ( 0.00%)      void* result = internalMalloc(arraySize);
    147,105 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (1x)
          4 ( 0.00%)      if (result)
         10 ( 0.00%)          memset(result, 0, arraySize);
         11 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (1x)
          .               else
          .                   errno = ENOMEM;
          .               return result;
         10 ( 0.00%)  }
          .           
          .           /********* End code for scalable_calloc   ***********/
          .           
          .           /********* Code for aligned allocation API **********/
          .           
          .           extern "C" int scalable_posix_memalign(void **memptr, size_t alignment, size_t size)
          .           {
          .               if ( !isPowerOfTwoAtLeast(alignment, sizeof(void*)) )
-- line 3091 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.h
--------------------------------------------------------------------------------
Ir                   

-- line 26 ----------------------------------------
          .           // global state of blocks currently in processing
          .           class BackendSync {
          .               // Class instances should reside in zero-initialized memory!
          .               // The number of blocks currently removed from a bin and not returned back
          .               std::atomic<intptr_t> inFlyBlocks;        // to another
          .               std::atomic<intptr_t> binsModifications;  // incremented on every bin modification
          .               Backend *backend;
          .           public:
          1 ( 0.00%)      void init(Backend *b) { backend = b; }
          .               void blockConsumed() { inFlyBlocks++; }
          .               void binsModified() { binsModifications++; }
          .               void blockReleased() {
          .           #if __TBB_MALLOC_BACKEND_STAT
          .                   MALLOC_ITT_SYNC_RELEASING(&inFlyBlocks);
          .           #endif
          .                   binsModifications++;
          .                   intptr_t prev = inFlyBlocks.fetch_sub(1);
-- line 42 ----------------------------------------
-- line 67 ----------------------------------------
          .               std::atomic<intptr_t>    active;
          .           public:
          .               bool wait() {
          .                   bool rescanBins = false;
          .                   // up to 3 threads can add more memory from OS simultaneously,
          .                   // rest of threads have to wait
          .                   intptr_t prevCnt = active.load(std::memory_order_acquire);
          .                   for (;;) {
          4 ( 0.00%)              if (prevCnt < 3) {
          4 ( 0.00%)                  if (active.compare_exchange_strong(prevCnt, prevCnt + 1)) {
          .                               break;
          .                           }
          .                       } else {
          .                           SpinWaitWhileEq(active, prevCnt);
          .                           rescanBins = true;
          .                           break;
          .                       }
          .                   }
-- line 84 ----------------------------------------
-- line 212 ----------------------------------------
          .                   void init() { leftBound.store(ADDRESS_UPPER_BOUND, std::memory_order_relaxed); }
          .                   void registerAlloc(uintptr_t left, uintptr_t right);
          .                   void registerFree(uintptr_t left, uintptr_t right);
          .                   // as only left and right bounds are kept, we can return true
          .                   // for pointer not allocated by us, if more than single region
          .                   // was requested from OS
          .                   bool inRange(void *ptr) const {
          .                       const uintptr_t p = (uintptr_t)ptr;
121,329,044 ( 0.61%)              return leftBound.load(std::memory_order_relaxed)<=p &&
          .                              p<=rightBound.load(std::memory_order_relaxed);
          .                   }
          .               };
          .           #else
          .               class UsedAddressRange {
          .               public:
          .                   void init() { }
          .                   void registerAlloc(uintptr_t, uintptr_t) {}
-- line 228 ----------------------------------------
-- line 315 ----------------------------------------
          .               // Clean all memory from all caches (extMemPool hard cleanup)
          .               FreeBlock *releaseMemInCaches(intptr_t startModifiedCnt, int *lockedBinsThreshold, int numOfLockedBins);
          .               // Soft heap limit (regular cleanup, then maybe hard cleanup)
          .               void releaseCachesToLimit();
          .           
          .               /*---------------------------------- Utility ----------------------------------*/
          .               // TODO: move inside IndexedBins class
          .               static int sizeToBin(size_t size) {
      1,235 ( 0.00%)          if (size >= maxBinned_HugePage)
        451 ( 0.00%)              return HUGE_BIN;
        916 ( 0.00%)          else if (size < minBinnedSize)
          .                       return NO_BIN;
          .           
      1,069 ( 0.00%)          int bin = (size - minBinnedSize)/freeBinsStep;
          .           
          .                   MALLOC_ASSERT(bin < HUGE_BIN, "Invalid size.");
          .                   return bin;
          .               }
          .               static bool toAlignedBin(FreeBlock *block, size_t size) {
          .                   return isAligned((char*)block + size, slabSize) && size >= slabSize;
          .               }
          .           
-- line 336 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp
--------------------------------------------------------------------------------
Ir                   

-- line 76 ----------------------------------------
          .           
          .           // In case there is no std::get_new_handler function
          .           // which provides synchronized access to std::new_handler
          .           #if !__TBB_CPP11_GET_NEW_HANDLER_PRESENT
          .           static ProxyMutex new_lock;
          .           #endif
          .           
          .           static inline void* InternalOperatorNew(size_t sz) {
 60,664,500 ( 0.30%)      void* res = scalable_malloc(sz);
3,305,806,095 (16.49%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (30,332,250x)
          .           #if TBB_USE_EXCEPTIONS
 60,664,500 ( 0.30%)      while (!res) {
          .                   std::new_handler handler;
          .           #if __TBB_CPP11_GET_NEW_HANDLER_PRESENT
          .                   handler = std::get_new_handler();
          .           #else
          .                   {
          .                       ProxyMutex::scoped_lock lock(new_lock);
          .                       handler = std::set_new_handler(0);
          .                       std::set_new_handler(handler);
-- line 94 ----------------------------------------
-- line 156 ----------------------------------------
          .               *orig_libc_realloc;
          .           
          .           // We already tried to find ptr to original functions.
          .           static std::atomic<bool> origFuncSearched{false};
          .           
          .           inline void InitOrigPointers()
          .           {
          .               // race is OK here, as different threads found same functions
 60,664,516 ( 0.30%)      if (!origFuncSearched.load(std::memory_order_acquire)) {
          9 ( 0.00%)          orig_free = dlsym(RTLD_NEXT, "free");
      4,794 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          5 ( 0.00%)          orig_realloc = dlsym(RTLD_NEXT, "realloc");
        873 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_msize = dlsym(RTLD_NEXT, "malloc_usable_size");
      1,027 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_libc_free = dlsym(RTLD_NEXT, "__libc_free");
        905 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          5 ( 0.00%)          orig_libc_realloc = dlsym(RTLD_NEXT, "__libc_realloc");
        972 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
          .           
          .                   origFuncSearched.store(true, std::memory_order_release);
          .               }
          .           }
          .           
          .           /*** replacements for malloc and the family ***/
          .           extern "C" {
          .           #elif MALLOC_ZONE_OVERLOAD_ENABLED
-- line 177 ----------------------------------------
-- line 180 ----------------------------------------
          .           #define ZONE_ARG struct _malloc_zone_t *,
          .           #define PREFIX(name) impl_##name
          .           // not interested in original functions for zone overload
          .           inline void InitOrigPointers() {}
          .           
          .           #endif // MALLOC_UNIXLIKE_OVERLOAD_ENABLED and MALLOC_ZONE_OVERLOAD_ENABLED
          .           
          .           void *PREFIX(malloc)(ZONE_ARG size_t size) __THROW
         11 ( 0.00%)  {
         26 ( 0.00%)      return scalable_malloc(size);
      3,357 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (10x)
      1,677 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         22 ( 0.00%)  }
          .           
          .           void *PREFIX(calloc)(ZONE_ARG size_t num, size_t size) __THROW
          2 ( 0.00%)  {
          8 ( 0.00%)      return scalable_calloc(num, size);
    147,962 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
          4 ( 0.00%)  }
          .           
          .           void PREFIX(free)(ZONE_ARG void *object) __THROW
         16 ( 0.00%)  {
          .               InitOrigPointers();
         36 ( 0.00%)      __TBB_malloc_safer_free(object, (void (*)(void*))orig_free);
      1,022 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
        711 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (7x)
         16 ( 0.00%)  }
          .           
          .           void *PREFIX(realloc)(ZONE_ARG void* ptr, size_t sz) __THROW
          .           {
          .               InitOrigPointers();
          .               return __TBB_malloc_safer_realloc(ptr, sz, orig_realloc);
          .           }
          .           
          .           /* The older *NIX interface for aligned allocations;
-- line 209 ----------------------------------------
-- line 303 ----------------------------------------
          .               return __TBB_malloc_safer_realloc(ptr, size, orig_libc_realloc);
          .           }
          .           #endif // !__ANDROID__
          .           
          .           } /* extern "C" */
          .           
          .           /*** replacements for global operators new and delete ***/
          .           
 60,664,500 ( 0.30%)  void* operator new(size_t sz) {
          .               return InternalOperatorNew(sz);
 60,664,524 ( 0.30%)  }
      1,959 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (3x)
          .           void* operator new[](size_t sz) {
          .               return InternalOperatorNew(sz);
          .           }
 60,664,500 ( 0.30%)  void operator delete(void* ptr) noexcept {
          .               InitOrigPointers();
121,329,000 ( 0.61%)      __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
2,938,471,329 (14.65%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (30,332,250x)
 60,664,500 ( 0.30%)  }
          .           void operator delete[](void* ptr) noexcept {
          .               InitOrigPointers();
          .               __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
          .           }
          .           void* operator new(size_t sz, const std::nothrow_t&) noexcept {
          .               return scalable_malloc(sz);
          .           }
          .           void* operator new[](std::size_t sz, const std::nothrow_t&) noexcept {
-- line 328 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h
--------------------------------------------------------------------------------
Ir                  

-- line 159 ----------------------------------------
         .           };
         .           
         .           template<typename Arg, typename Compare>
         .           inline void AtomicUpdate(std::atomic<Arg>& location, Arg newVal, const Compare &cmp)
         .           {
         .               static_assert(sizeof(Arg) == sizeof(intptr_t), "Type of argument must match AtomicCompareExchange type.");
         .               Arg old = location.load(std::memory_order_acquire);
         .               for (; cmp(old, newVal); ) {
         6 ( 0.00%)          if (location.compare_exchange_strong(old, newVal))
         .                       break;
         .                   // TODO: do we need backoff after unsuccessful CAS?
         .                   //old = val;
         .               }
         .           }
         .           
         .           // TODO: make BitMaskBasic more general
         .           // TODO: check that BitMaskBasic is not used for synchronization
-- line 175 ----------------------------------------
-- line 180 ----------------------------------------
         .               static const unsigned WORD_LEN = CHAR_BIT*sizeof(uintptr_t);
         .           
         .               std::atomic<uintptr_t> mask[SZ];
         .           
         .           protected:
         .               void set(size_t idx, bool val) {
         .                   MALLOC_ASSERT(idx<NUM, ASSERT_TEXT);
         .           
       583 ( 0.00%)          size_t i = idx / WORD_LEN;
     1,009 ( 0.00%)          int pos = WORD_LEN - idx % WORD_LEN - 1;
         .                   if (val) {
       751 ( 0.00%)              mask[i].fetch_or(1ULL << pos);
         .                   } else {
       678 ( 0.00%)              mask[i].fetch_and(~(1ULL << pos));
         .                   }
         .               }
         .               int getMinTrue(unsigned startIdx) const {
       296 ( 0.00%)          unsigned idx = startIdx / WORD_LEN;
         .                   int pos;
         .           
       524 ( 0.00%)          if (startIdx % WORD_LEN) {
         .                       // only interested in part of a word, clear bits before startIdx
       290 ( 0.00%)              pos = WORD_LEN - startIdx % WORD_LEN;
       450 ( 0.00%)              uintptr_t actualMask = mask[idx].load(std::memory_order_relaxed) & (((uintptr_t)1<<pos) - 1);
       142 ( 0.00%)              idx++;
         .                       if (-1 != (pos = BitScanRev(actualMask)))
       166 ( 0.00%)                  return idx*WORD_LEN - pos - 1;
         .                   }
         .           
       498 ( 0.00%)          while (idx<SZ)
       243 ( 0.00%)              if (-1 != (pos = BitScanRev(mask[idx++].load(std::memory_order_relaxed))))
       300 ( 0.00%)                  return idx*WORD_LEN - pos - 1;
         .                   return -1;
         .               }
         .           public:
         .               void reset() { for (unsigned i=0; i<SZ; i++) mask[i].store(0, std::memory_order_relaxed); }
         .           };
         .           
         .           template<unsigned NUM>
         .           class BitMaskMin : public BitMaskBasic<NUM> {
-- line 219 ----------------------------------------
-- line 223 ----------------------------------------
         .                   return BitMaskBasic<NUM>::getMinTrue(startIdx);
         .               }
         .           };
         .           
         .           template<unsigned NUM>
         .           class BitMaskMax : public BitMaskBasic<NUM> {
         .           public:
         .               void set(size_t idx, bool val) {
        86 ( 0.00%)          BitMaskBasic<NUM>::set(NUM - 1 - idx, val);
         .               }
         .               int getMaxTrue(unsigned startIdx) const {
        28 ( 0.00%)          int p = BitMaskBasic<NUM>::getMinTrue(NUM-startIdx-1);
        26 ( 0.00%)          return -1==p? -1 : (int)NUM - 1 - p;
         .               }
         .           };
         .           
         .           
         .           // The part of thread-specific data that can be modified by other threads.
         .           // Such modifications must be protected by AllLocalCaches::listLock.
         .           struct TLSRemote {
         .               TLSRemote *next,
-- line 243 ----------------------------------------
-- line 304 ----------------------------------------
         .           public:
         .               typedef MainIndexSelect<4 < sizeof(uintptr_t)>::main_type main_t;
         .           private:
         .               static const main_t invalid = ~main_t(0);
         .               main_t main;      // index in BackRefMain
         .               uint16_t largeObj:1;  // is this object "large"?
         .               uint16_t offset  :15; // offset from beginning of BackRefBlock
         .           public:
       552 ( 0.00%)      BackRefIdx() : main(invalid), largeObj(0), offset(0) {}
         .               bool isInvalid() const { return main == invalid; }
30,083,396 ( 0.15%)      bool isLargeObject() const { return largeObj; }
         .               main_t getMain() const { return main; }
         .               uint16_t getOffset() const { return offset; }
         .           
         .           #if __TBB_USE_THREAD_SANITIZER
         .               friend
         .               __attribute__((no_sanitize("thread")))
         .                BackRefIdx dereference(const BackRefIdx* ptr) {
         .                   BackRefIdx idx;
-- line 322 ----------------------------------------
-- line 323 ----------------------------------------
         .                   idx.main = ptr->main;
         .                   idx.largeObj = ptr->largeObj;
         .                   idx.offset = ptr->offset;
         .                   return idx;
         .               }
         .           #else
         .               friend
         .               BackRefIdx dereference(const BackRefIdx* ptr) {
 8,595,256 ( 0.04%)          return *ptr;
         .               }
         .           #endif
         .           
         .               // only newBackRef can modify BackRefIdx
         .               static BackRefIdx newBackRef(bool largeObj);
         .           };
         .           
         .           // Block header is used during block coalescing
-- line 339 ----------------------------------------
-- line 388 ----------------------------------------
         .               }
         .           
         .               bool ready() const {
         .                   return setDone;
         .               }
         .           
         .               // envName - environment variable to get controlled mode
         .               void initReadEnv(const char *envName, intptr_t defaultVal) {
         2 ( 0.00%)          if (!setDone) {
         .                       // unreferenced formal parameter warning
         .                       tbb::detail::suppress_unused_warning(envName);
         .           #if !__TBB_WIN8UI_SUPPORT
         .                   // TODO: use strtol to get the actual value of the envirable
         4 ( 0.00%)              const char *envVal = getenv(envName);
       402 ( 0.00%)  => ./stdlib/getenv.c:getenv (1x)
         3 ( 0.00%)              if (envVal && !strcmp(envVal, "1"))
         .                           val = 1;
         .                       else
         .           #endif
         1 ( 0.00%)                  val = defaultVal;
         1 ( 0.00%)              setDone = true;
         .                   }
         1 ( 0.00%)      }
         .           };
         .           
         .           // Page type to be used inside MapMemory.
         .           // Regular (4KB aligned), Huge and Transparent Huge Pages (2MB aligned).
         .           enum PageType {
         .               REGULAR = 0,
         .               PREALLOCATED_HUGE_PAGE,
         .               TRANSPARENT_HUGE_PAGE
-- line 417 ----------------------------------------
-- line 440 ----------------------------------------
         .                   fputs("TBBmalloc: huge pages\t", stderr);
         .                   if (!state)
         .                       fputs("not ", stderr);
         .                   fputs(stateName, stderr);
         .                   fputs("\n", stderr);
         .               }
         .           
         .               void parseSystemMemInfo() {
         1 ( 0.00%)          bool hpAvailable  = false;
         .                   bool thpAvailable = false;
         1 ( 0.00%)          long long hugePageSize = -1;
         .           
         .           #if __unix__
         .                   // Check huge pages existence
         1 ( 0.00%)          long long meminfoHugePagesTotal = 0;
         .           
         .                   parseFileItem meminfoItems[] = {
         .                       // Parse system huge page size
         .                       { "Hugepagesize: %lld kB", hugePageSize },
         .                       // Check if there are preallocated huge pages on the system
         .                       // https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
         .                       { "HugePages_Total: %lld", meminfoHugePagesTotal } };
         .           
         .                   parseFile</*BUFF_SIZE=*/100>("/proc/meminfo", meminfoItems);
         .           
         .                   // Double check another system information regarding preallocated
         .                   // huge pages if there are no information in /proc/meminfo
         1 ( 0.00%)          long long vmHugePagesTotal = 0;
         .           
         4 ( 0.00%)          parseFileItem vmItem[] = { { "%lld", vmHugePagesTotal } };
         .           
         .                   // We parse a counter number, it can't be huge
         3 ( 0.00%)          parseFile</*BUFF_SIZE=*/100>("/proc/sys/vm/nr_hugepages", vmItem);
     2,299 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:void parseFile<100, 1>(char const*, parseFileItem const (&) [1]) (1x)
         .           
         7 ( 0.00%)          if (hugePageSize > -1 && (meminfoHugePagesTotal > 0 || vmHugePagesTotal > 0)) {
         .                       MALLOC_ASSERT(hugePageSize != 0, "Huge Page size can't be zero if we found preallocated.");
         .           
         .                       // Any non zero value clearly states that there are preallocated
         .                       // huge pages on the system
         1 ( 0.00%)              hpAvailable = true;
         .                   }
         .           
         .                   // Check if there is transparent huge pages support on the system
         1 ( 0.00%)          long long thpPresent = 'n';
         4 ( 0.00%)          parseFileItem thpItem[] = { { "[alwa%cs] madvise never\n", thpPresent } };
         3 ( 0.00%)          parseFile</*BUFF_SIZE=*/100>("/sys/kernel/mm/transparent_hugepage/enabled", thpItem);
     2,848 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/shared_utils.h:void parseFile<100, 1>(char const*, parseFileItem const (&) [1]) (1x)
         .           
         5 ( 0.00%)          if (hugePageSize > -1 && thpPresent == 'y') {
         .                       MALLOC_ASSERT(hugePageSize != 0, "Huge Page size can't be zero if we found thp existence.");
         .                       thpAvailable = true;
         .                   }
         .           #endif
         .                   MALLOC_ASSERT(!pageSize, "Huge page size can't be set twice. Double initialization.");
         .           
         .                   // Initialize object variables
         2 ( 0.00%)          pageSize       = hugePageSize * 1024; // was read in KB from meminfo
         1 ( 0.00%)          isHPAvailable  = hpAvailable;
         1 ( 0.00%)          isTHPAvailable = thpAvailable;
         .               }
         .           
         .           public:
         .           
         .               // System information
         .               bool isHPAvailable;
         .               bool isTHPAvailable;
         .           
         .               // User defined value
         .               bool isEnabled;
         .           
         .               void init() {
         .                   parseSystemMemInfo();
         .                   MallocMutex::scoped_lock lock(setModeLock);
         .                   requestedMode.initReadEnv("TBB_MALLOC_USE_HUGE_PAGES", 0);
         9 ( 0.00%)          isEnabled = (isHPAvailable || isTHPAvailable) && requestedMode.get();
         .               }
         .           
         .               // Could be set from user code at any place.
         .               // If we didn't call init() at this place, isEnabled will be false
         .               void setMode(intptr_t newVal) {
         .                   MallocMutex::scoped_lock lock(setModeLock);
         .                   requestedMode.set(newVal);
         .                   isEnabled = (isHPAvailable || isTHPAvailable) && newVal;
-- line 521 ----------------------------------------
-- line 576 ----------------------------------------
         .                                 fixedPool;
         .               TLSKey            tlsPointerKey;  // per-pool TLS key
         .           
         .               bool init(intptr_t poolId, rawAllocType rawAlloc, rawFreeType rawFree,
         .                         size_t granularity, bool keepAllMemory, bool fixedPool);
         .               bool initTLS();
         .           
         .               // i.e., not system default pool for scalable_malloc/scalable_free
       728 ( 0.00%)      bool userPool() const { return rawAlloc; }
         .           
         .                // true if something has been released
         .               bool softCachesCleanup();
         .               bool releaseAllLocalCaches();
         .               bool hardCachesCleanup();
         .               void *remap(void *ptr, size_t oldSize, size_t newSize, size_t alignment);
         .               bool reset() {
         .                   loc.reset();
-- line 592 ----------------------------------------
-- line 665 ----------------------------------------
         .            */
         .               static bool mallocRecursionDetected;
         .           
         .               MallocMutex::scoped_lock* lock_acquired;
         .               char scoped_lock_space[sizeof(MallocMutex::scoped_lock)+1];
         .               
         .           public:
         .           
         3 ( 0.00%)      RecursiveMallocCallProtector() : lock_acquired(nullptr) {
         6 ( 0.00%)          lock_acquired = new (scoped_lock_space) MallocMutex::scoped_lock( rmc_mutex );
         .                   if (canUsePthread)
        10 ( 0.00%)              owner_thread.store(pthread_self(), std::memory_order_relaxed);
         2 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (1x)
         .                   autoObjPtr.store(&scoped_lock_space, std::memory_order_relaxed);
         .               }
         .               ~RecursiveMallocCallProtector() {
         6 ( 0.00%)          if (lock_acquired) {
         .                       autoObjPtr.store(nullptr, std::memory_order_relaxed);
         3 ( 0.00%)              lock_acquired->~scoped_lock();
         .                   }
         2 ( 0.00%)      }
         .               static bool sameThreadActive() {
60,664,532 ( 0.30%)          if (!autoObjPtr.load(std::memory_order_relaxed)) // fast path
         .                       return false;
         .                   // Some thread has an active recursive call protector; check if the current one.
         .                   // Exact pthread_self based test
         .                   if (canUsePthread) {
        35 ( 0.00%)              if (pthread_equal( owner_thread.load(std::memory_order_relaxed), pthread_self() )) {
        14 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (7x)
         .                           mallocRecursionDetected = true;
         .                           return true;
         .                       } else
         .                           return false;
         .                   }
         .                   // inexact stack size based test
         .                   const uintptr_t threadStackSz = 2*1024*1024;
         .                   int dummy;
-- line 699 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/atomic_base.h
--------------------------------------------------------------------------------
Ir                   

-- line 188 ----------------------------------------
          .               atomic_flag() noexcept = default;
          .               ~atomic_flag() noexcept = default;
          .               atomic_flag(const atomic_flag&) = delete;
          .               atomic_flag& operator=(const atomic_flag&) = delete;
          .               atomic_flag& operator=(const atomic_flag&) volatile = delete;
          .           
          .               // Conversion to ATOMIC_FLAG_INIT.
          .               constexpr atomic_flag(bool __i) noexcept
         31 ( 0.00%)        : __atomic_flag_base{ _S_init(__i) }
          .               { }
          .           
          .               _GLIBCXX_ALWAYS_INLINE bool
          .               test_and_set(memory_order __m = memory_order_seq_cst) noexcept
          .               {
      3,707 ( 0.00%)        return __atomic_test_and_set (&_M_i, int(__m));
          .               }
          .           
          .               _GLIBCXX_ALWAYS_INLINE bool
          .               test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept
          .               {
          .                 return __atomic_test_and_set (&_M_i, int(__m));
          .               }
          .           
-- line 210 ----------------------------------------
-- line 211 ----------------------------------------
          .               _GLIBCXX_ALWAYS_INLINE void
          .               clear(memory_order __m = memory_order_seq_cst) noexcept
          .               {
          .                 memory_order __b = __m & __memory_order_mask;
          .                 __glibcxx_assert(__b != memory_order_consume);
          .                 __glibcxx_assert(__b != memory_order_acquire);
          .                 __glibcxx_assert(__b != memory_order_acq_rel);
          .           
      1,112 ( 0.00%)        __atomic_clear (&_M_i, int(__m));
          .               }
          .           
          .               _GLIBCXX_ALWAYS_INLINE void
          .               clear(memory_order __m = memory_order_seq_cst) volatile noexcept
          .               {
          .                 memory_order __b = __m & __memory_order_mask;
          .                 __glibcxx_assert(__b != memory_order_consume);
          .                 __glibcxx_assert(__b != memory_order_acquire);
-- line 227 ----------------------------------------
-- line 278 ----------------------------------------
          .               public:
          .                 __atomic_base() noexcept = default;
          .                 ~__atomic_base() noexcept = default;
          .                 __atomic_base(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
          .           
          .                 // Requires __int_type convertible to _M_i.
        251 ( 0.00%)        constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }
          .           
          .                 operator __int_type() const noexcept
          .                 { return load(); }
          .           
          .                 operator __int_type() const volatile noexcept
          .                 { return load(); }
          .           
          .                 __int_type
-- line 294 ----------------------------------------
-- line 396 ----------------------------------------
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           	__glibcxx_assert(__b != memory_order_consume);
          .           
 60,667,447 ( 0.30%)  	__atomic_store_n(&_M_i, __i, int(__m));
         11 ( 0.00%)        }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__int_type __i,
          .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 413 ----------------------------------------
-- line 418 ----------------------------------------
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 load(memory_order __m = memory_order_seq_cst) const noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           
272,997,237 ( 1.36%)  	return __atomic_load_n(&_M_i, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 434 ----------------------------------------
-- line 501 ----------------------------------------
          .           			      memory_order __m1, memory_order __m2) noexcept
          .                 {
          .           	memory_order __b2 = __m2 & __memory_order_mask;
          .           	memory_order __b1 = __m1 & __memory_order_mask;
          .           	__glibcxx_assert(__b2 != memory_order_release);
          .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
          .           	__glibcxx_assert(__b2 <= __b1);
          .           
      2,053 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0,
          .           					   int(__m1), int(__m2));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE bool
          .                 compare_exchange_strong(__int_type& __i1, __int_type __i2,
          .           			      memory_order __m1,
          .           			      memory_order __m2) volatile noexcept
          .                 {
-- line 517 ----------------------------------------
-- line 540 ----------------------------------------
          .                 {
          .           	return compare_exchange_strong(__i1, __i2, __m,
          .           				       __cmpexch_failure_order(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_add(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
      1,535 ( 0.00%)        { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_add(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_sub(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
        325 ( 0.00%)        { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_sub(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_and(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) noexcept
        404 ( 0.00%)        { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_and(__int_type __i,
          .           		memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_or(__int_type __i,
          .           	       memory_order __m = memory_order_seq_cst) noexcept
        750 ( 0.00%)        { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_or(__int_type __i,
          .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __int_type
          .                 fetch_xor(__int_type __i,
-- line 586 ----------------------------------------
-- line 613 ----------------------------------------
          .               public:
          .                 __atomic_base() noexcept = default;
          .                 ~__atomic_base() noexcept = default;
          .                 __atomic_base(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) = delete;
          .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
          .           
          .                 // Requires __pointer_type convertible to _M_p.
         53 ( 0.00%)        constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }
          .           
          .                 operator __pointer_type() const noexcept
          .                 { return load(); }
          .           
          .                 operator __pointer_type() const volatile noexcept
          .                 { return load(); }
          .           
          .                 __pointer_type
-- line 629 ----------------------------------------
-- line 717 ----------------------------------------
          .           	    memory_order __m = memory_order_seq_cst) noexcept
          .                 {
          .                   memory_order __b = __m & __memory_order_mask;
          .           
          .           	__glibcxx_assert(__b != memory_order_acquire);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           	__glibcxx_assert(__b != memory_order_consume);
          .           
      4,553 ( 0.00%)  	__atomic_store_n(&_M_p, __p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE void
          .                 store(__pointer_type __p,
          .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_acquire);
-- line 733 ----------------------------------------
-- line 739 ----------------------------------------
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 load(memory_order __m = memory_order_seq_cst) const noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
          .           
373,001,162 ( 1.86%)  	return __atomic_load_n(&_M_p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
          .                 {
          .           	memory_order __b = __m & __memory_order_mask;
          .           	__glibcxx_assert(__b != memory_order_release);
          .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 755 ----------------------------------------
-- line 756 ----------------------------------------
          .           
          .           	return __atomic_load_n(&_M_p, int(__m));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 exchange(__pointer_type __p,
          .           	       memory_order __m = memory_order_seq_cst) noexcept
          .                 {
      1,796 ( 0.00%)  	return __atomic_exchange_n(&_M_p, __p, int(__m));
          .                 }
          .           
          .           
          .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
          .                 exchange(__pointer_type __p,
          .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
          .                 {
          .           	return __atomic_exchange_n(&_M_p, __p, int(__m));
-- line 772 ----------------------------------------
-- line 778 ----------------------------------------
          .           			      memory_order __m2) noexcept
          .                 {
          .           	memory_order __b2 = __m2 & __memory_order_mask;
          .           	memory_order __b1 = __m1 & __memory_order_mask;
          .           	__glibcxx_assert(__b2 != memory_order_release);
          .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
          .           	__glibcxx_assert(__b2 <= __b1);
          .           
      1,039 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0,
          .           					   int(__m1), int(__m2));
          .                 }
          .           
          .                 _GLIBCXX_ALWAYS_INLINE bool
          .                 compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,
          .           			      memory_order __m1,
          .           			      memory_order __m2) volatile noexcept
          .                 {
-- line 794 ----------------------------------------

--------------------------------------------------------------------------------
The following files chosen for auto-annotation could not be found:
--------------------------------------------------------------------------------
  ./nptl/pthread_getspecific.c
  ./nptl/pthread_self.c
  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S

--------------------------------------------------------------------------------
Ir                     
--------------------------------------------------------------------------------
6,244,208,913 (31.14%)  events annotated

