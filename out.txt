--------------------------------------------------------------------------------
Profile data file 'callgrind.out.17514' (creator: callgrind-3.16.1)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 975805558
Trigger: Program termination
Profiled target:  ./build/nptest -m 4 ../real-time-task-generators-main/proper testsets/c4_slow.csv --por=priority --interfering=all (PID 17514, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
Ir                     
--------------------------------------------------------------------------------
3,582,837,800 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
Ir                      file:function
--------------------------------------------------------------------------------

3,582,837,800 (100.0%)  *  ???:0x0000000000001090 [/usr/lib/x86_64-linux-gnu/ld-2.31.so]
3,580,352,083 (99.93%)  >   ???:_start (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    2,305,249 ( 0.06%)  >   ./elf/rtld.c:_dl_start (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]
      180,449 ( 0.01%)  >   ./elf/dl-init.c:_dl_init (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]

3,580,352,083 (99.93%)  *  ???:_start [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,580,352,072 (99.93%)  >   ./csu/../csu/libc-start.c:(below main) (1x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]

3,580,352,072 (99.93%)  *  ./csu/../csu/libc-start.c:(below main) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
3,580,216,544 (99.93%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/src/nptest.cpp:main (1x)
      108,827 ( 0.00%)  >   ???:__libc_csu_init (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       26,540 ( 0.00%)  >   ./stdlib/exit.c:exit (1x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
           76 ( 0.00%)  >   ./stdlib/cxa_atexit.c:__cxa_atexit (1x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
           28 ( 0.00%)  >   ./setjmp/../sysdeps/x86_64/bsd-_setjmp.S:_setjmp (1x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]

3,580,216,544 (99.93%)  *  /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/src/nptest.cpp:main

3,580,184,279 (99.93%)  *  src/nptest.cpp:main [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,579,930,677 (99.92%)  >   src/nptest.cpp:process_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       59,689 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionParser::parse_args(int, char const* const*) (1x)
       53,176 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (11x)
       49,767 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (16x)
       18,317 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::OptionParser::~OptionParser() (1x)
       17,104 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator bool() (4x)
       14,794 ( 0.00%)  >   /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (104x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       12,118 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (5x)
        8,846 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (14x)
        7,447 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator unsigned int() (2x)
        4,182 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::Values(optparse::Values const&) (1x)
        3,664 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (5x)
        2,392 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::~Values() (1x)
          725 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (5x)
          552 ( 0.00%)  >   /usr/include/c++/10/bits/stl_vector.h:std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~vector() (2x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
          135 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionParser::OptionParser() (1x)

3,579,930,677 (99.92%)  *  src/nptest.cpp:process_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,579,863,206 (99.92%)  >   src/nptest.cpp:process_stream(std::istream&, std::istream&, std::istream&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       13,473 ( 0.00%)  >   ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (3x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]
        4,012 ( 0.00%)  >   ???:std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::basic_istringstream(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::_Ios_Openmode) (2x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
        2,204 ( 0.00%)  >   ???:std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream() (1x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
          376 ( 0.00%)  >   ???:std::ostream::operator<<(int) (1x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
           86 ( 0.00%)  >   /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

3,579,863,206 (99.92%)  *  src/nptest.cpp:process_stream(std::istream&, std::istream&, std::istream&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,579,863,189 (99.92%)  >   src/nptest.cpp:Analysis_result analyze<long long, NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> > >(std::istream&, std::istream&, std::istream&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

3,579,863,189 (99.92%)  *  src/nptest.cpp:Analysis_result analyze<long long, NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> > >(std::istream&, std::istream&, std::istream&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   13,183,670 ( 0.37%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:NP::Job<long long>::Job_set NP::parse_file<long long>(std::istream&) (1x)
      107,674 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/problem.hpp:NP::Scheduling_problem<long long>::Scheduling_problem(std::vector<NP::Job<long long>, std::allocator<NP::Job<long long> > >, std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > >, std::vector<NP::Abort_action<long long>, std::allocator<NP::Abort_action<long long> > >, unsigned int) (1x)
        3,876 ( 0.00%)  >   ???:std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream() (2x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
        1,303 ( 0.00%)  >   ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]
          177 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:NP::parse_dag_file(std::istream&) (1x)
          169 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:std::vector<NP::Abort_action<long long>, std::allocator<NP::Abort_action<long long> > > NP::parse_abort_file<long long>(std::istream&) (1x)
          154 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/problem.hpp:NP::Scheduling_problem<long long>::~Scheduling_problem() (1x)
           84 ( 0.00%)  >   ???:std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream() (1x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]

3,566,543,953 (99.55%)  *  include/global/por_space.hpp:Analysis_result analyze<long long, NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> > >(std::istream&, std::istream&, std::istream&)
3,549,977,181 (99.08%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::explore() (1x)
   11,709,177 ( 0.33%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::State_space(std::vector<NP::Job<long long>, std::allocator<NP::Job<long long> > > const&, std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > > const&, unsigned int, double, unsigned int, unsigned long) (1x)
    3,125,192 ( 0.09%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/precedence.hpp:NP::Job<long long>::Job_set NP::preprocess_jobs<long long>(std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > > const&, NP::Job<long long>::Job_set const&) (1x)
    1,708,693 ( 0.05%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::~State_space() (1x)
       15,201 ( 0.00%)  >   /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::~vector() (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
        8,462 ( 0.00%)  >   /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::vector(unsigned long, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > const&) [clone .constprop.0] (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

3,549,977,181 (99.08%)  *  /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::explore()

3,549,974,331 (99.08%)  *  include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::explore() [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,549,974,200 (99.08%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::explore(NP::Global::Schedule_state<long long> const&) (2x)

3,549,974,200 (99.08%)  *  /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::explore(NP::Global::Schedule_state<long long> const&)

3,549,876,420 (99.08%)  *  include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::explore(NP::Global::Schedule_state<long long> const&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
3,546,986,621 (99.00%)  >   include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    1,807,910 ( 0.05%)  >   include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::dispatch_reduction_set_merge(NP::Global::Schedule_state<long long> const&, NP::Global::Reduction_set<long long>&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
      957,231 ( 0.03%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::~Reduction_set() (1x)
      109,468 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::next_job_ready(NP::Global::Schedule_state<long long> const&, long long) const (2x)

3,546,986,621 (99.00%)  *  include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    3,586,179 ( 0.10%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::add_job(NP::Job<long long> const*, unsigned long) (2,499x)
       96,363 ( 0.00%)  >   /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set_statistics<long long>::Reduction_set_statistics(bool, NP::Global::Reduction_set<long long>&) (1x)
          165 ( 0.00%)  >   /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

3,516,859,934 (98.16%)  *  include/global/reduction_set.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&)
3,505,745,586 (97.85%)  >   include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() (508x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   11,005,728 ( 0.31%)  >   include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() (508x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       33,814 ( 0.00%)  >   include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >) (1x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

3,505,772,835 (97.85%)  *  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  949,426,473 (26.50%)  >   include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  519,454,837 (14.50%)  >   /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (1,920,390x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

1,053,628,821 (29.41%)  *  /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

  949,426,473 (26.50%)  *  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
  534,173,125 (14.91%)  >   /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (2,560,520x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   82,009,496 ( 2.29%)  >   /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (2,236,699x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   15,390,980 ( 0.43%)  >   /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::emplace_back<NP::Job<long long> const*&>(NP::Job<long long> const*&) (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

  807,178,547 (22.53%)  *  /usr/include/c++/10/ext/new_allocator.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)
  462,530,282 (12.91%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) (3,840,783x) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
  293,437,822 ( 8.19%)  >   ???:operator delete(void*, unsigned long) (2,560,522x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]

  614,190,419 (17.14%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
  572,856,974 (15.99%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (5,166,731x) [/usr/local/lib/libtbbmalloc.so.2.9]

  576,146,583 (16.08%)  *  ???:operator delete(void*, unsigned long) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
  565,813,125 (15.79%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator delete(void*) (5,166,729x) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
          750 ( 0.00%)  >   ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]

  572,862,564 (15.99%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc [/usr/local/lib/libtbbmalloc.so.2.9]
  531,528,363 (14.84%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (5,166,737x) [/usr/local/lib/libtbbmalloc.so.2.9]
        1,107 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long)'2 (6x) [/usr/local/lib/libtbbmalloc.so.2.9]

  565,813,242 (15.79%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator delete(void*) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]
  508,979,325 (14.21%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (5,166,731x) [/usr/local/lib/libtbbmalloc.so.2.9]

  531,675,468 (14.84%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
  412,697,586 (11.52%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) (5,166,738x) [/usr/local/lib/libtbbmalloc.so.2.9]
      142,905 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::doInitialization() (1x) [/usr/local/lib/libtbbmalloc.so.2.9]

  508,980,168 (14.21%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free [/usr/local/lib/libtbbmalloc.so.2.9]
  252,510,332 ( 7.05%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] (5,166,661x) [/usr/local/lib/libtbbmalloc.so.2.9]
   92,999,898 ( 2.60%)  >   /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (5,166,661x) [/usr/local/lib/libtbbmalloc.so.2.9]
    6,768,782 ( 0.19%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] (563,703x) [/usr/local/lib/libtbbmalloc.so.2.9]
        6,898 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::putToLLOCache(rml::internal::TLSData*, void*) (79x) [/usr/local/lib/libtbbmalloc.so.2.9]
        1,343 ( 0.00%)  >   ./nptl/pthread_getspecific.c:pthread_getspecific (79x) [/usr/lib/x86_64-linux-gnu/libpthread-2.31.so]

  412,697,586 (11.52%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) [/usr/local/lib/libtbbmalloc.so.2.9]
   87,834,529 ( 2.45%)  >   ./nptl/pthread_getspecific.c:pthread_getspecific (5,166,737x) [/usr/lib/x86_64-linux-gnu/libpthread-2.31.so]
   46,509,433 ( 1.30%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) (5,166,658x) [/usr/local/lib/libtbbmalloc.so.2.9]
       84,794 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (242x) [/usr/local/lib/libtbbmalloc.so.2.9]
       35,827 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getFromLLOCache(rml::internal::TLSData*, unsigned long, unsigned long) (80x) [/usr/local/lib/libtbbmalloc.so.2.9]
        8,912 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::get(rml::internal::TLSData*, unsigned int) (242x) [/usr/local/lib/libtbbmalloc.so.2.9]
        2,123 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::TLSKey::createTLS(rml::internal::MemoryPool*, rml::internal::Backend*) (1x) [/usr/local/lib/libtbbmalloc.so.2.9]
        1,178 ( 0.00%)  >   ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]

  362,451,257 (10.12%)  *  /usr/include/c++/10/ext/new_allocator.h:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex()
  277,319,454 ( 7.74%)  >   ???:operator delete(void*, unsigned long) (2,559,030x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
   72,974,820 ( 2.04%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) (640,130x) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]

  252,510,332 ( 7.05%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] [/usr/local/lib/libtbbmalloc.so.2.9]
   22,234,222 ( 0.62%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) (2,470,435x) [/usr/local/lib/libtbbmalloc.so.2.9]
   13,895,542 ( 0.39%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) (2,696,220x) [/usr/local/lib/libtbbmalloc.so.2.9]
   10,333,310 ( 0.29%)  >   ./nptl/pthread_self.c:pthread_self (5,166,655x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
       56,470 ( 0.00%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (217x) [/usr/local/lib/libtbbmalloc.so.2.9]

  146,653,025 ( 4.09%)  *  /usr/include/c++/10/bits/hashtable.h:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex()
  138,477,846 ( 3.87%)  >   /usr/include/c++/10/bits/hashtable.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0] (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    5,259,307 ( 0.15%)  >   ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (509x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]
      284,502 ( 0.01%)  >   /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&) (63x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
       56,724 ( 0.00%)  >   ???:std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const (508x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]
        1,148 ( 0.00%)  >   ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x) [/usr/lib/x86_64-linux-gnu/ld-2.31.so]

  138,477,846 ( 3.87%)  *  /usr/include/c++/10/bits/hashtable.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
   32,711,816 ( 0.91%)  >   /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, long long>, true>*, unsigned long) [clone .isra.0] (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   93,002,340 ( 2.60%)  *  /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) [/usr/local/lib/libtbbmalloc.so.2.9]

   87,843,793 ( 2.45%)  *  ./nptl/pthread_getspecific.c:pthread_getspecific [/usr/lib/x86_64-linux-gnu/libpthread-2.31.so]

   82,009,496 ( 2.29%)  *  /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   75,660,379 ( 2.11%)  *  /usr/include/c++/10/ext/new_allocator.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, long long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, long long&>(std::integral_constant<bool, true>, NP::JobID&&, long long&) [clone .isra.0]
   73,099,859 ( 2.04%)  >   /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) (640,130x) [/usr/local/lib/libtbbmalloc_proxy.so.2.9]

   75,299,399 ( 2.10%)  *  include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0]

   68,781,581 ( 1.92%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) [/usr/local/lib/libtbbmalloc.so.2.9]

   67,853,833 ( 1.89%)  *  /usr/include/c++/10/bits/stl_vector.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)

   53,770,920 ( 1.50%)  *  /usr/include/c++/10/bits/stl_algo.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
   44,809,100 ( 1.25%)  >   /usr/include/c++/10/bits/stl_algo.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    3,200,650 ( 0.09%)  >   /usr/include/c++/10/bits/stl_iterator.h:void std::__introsort_loop<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (640,130x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   49,930,179 ( 1.39%)  *  /usr/include/c++/10/bits/stl_uninitialized.h:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&)
   29,446,003 ( 0.82%)  >   ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms (2,560,522x) [/usr/lib/x86_64-linux-gnu/libc-2.31.so]

   46,501,272 ( 1.30%)  *  /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::getBackRef(rml::internal::BackRefIdx)

   44,809,240 ( 1.25%)  *  /usr/include/c++/10/bits/stl_algo.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   43,948,526 ( 1.23%)  *  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc-2.31.so]

   43,851,885 ( 1.22%)  *  /usr/include/c++/10/bits/unordered_map.h:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&)
   38,194,756 ( 1.07%)  >   /usr/include/c++/10/bits/hashtable_policy.h:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) (1,032,203x) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   38,194,756 ( 1.07%)  *  /usr/include/c++/10/bits/hashtable_policy.h:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]

   32,711,816 ( 0.91%)  *  /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, long long>, true>*, unsigned long) [clone .isra.0] [/home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/build/nptest]
    4,480,910 ( 0.13%)  >   ???:std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const (640,130x) [/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28]

--------------------------------------------------------------------------------
-- Auto-annotated source: include/global/space.hpp
--------------------------------------------------------------------------------
Ir              

-- line 31 ----------------------------------------
     .           
     .           	namespace Global
     .           	{
     .           
     .           		template <class Time>
     .           		class Null_IIP;
     .           
     .           		template <class Time, class IIP = Null_IIP<Time>>
    22 ( 0.00%)  		class State_space
19,197 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::~vector() (2x)
     .           		{
     .           		public:
     .           			typedef Scheduling_problem<Time> Problem;
     .           			typedef typename Scheduling_problem<Time>::Workload Workload;
     .           			typedef Schedule_state<Time> State;
     .           
     .           			typedef std::vector<std::size_t> Job_precedence_set;
     .           			std::vector<Job_precedence_set> job_precedence_sets;
-- line 47 ----------------------------------------
-- line 93 ----------------------------------------
     .           				else
     .           				{
     .           					return rbounds->second;
     .           				}
     .           			}
     .           
     .           			bool is_schedulable() const
     .           			{
     2 ( 0.00%)  				return !aborted;
     .           			}
     .           
     .           			bool was_timed_out() const
     .           			{
     .           				return timed_out;
     .           			}
     .           
     .           			unsigned long number_of_states() const
     .           			{
     2 ( 0.00%)  				return num_states;
     .           			}
     .           
     .           			unsigned long number_of_edges() const
     .           			{
     .           				return num_edges;
     .           			}
     .           
     .           			unsigned long max_exploration_front_width() const
-- line 119 ----------------------------------------
-- line 271 ----------------------------------------
     .           #ifdef CONFIG_PARALLEL
     .           			tbb::enumerable_thread_specific<unsigned long> edge_counter;
     .           #endif
     .           			Processor_clock cpu_time;
     .           			const double timeout;
     .           
     .           			const unsigned int num_cpus;
     .           
    12 ( 0.00%)  			State_space(const Workload &jobs,
     .           						const Precedence_constraints &dag_edges,
     .           						unsigned int num_cpus,
     .           						double max_cpu_time = 0,
     .           						unsigned int max_depth = 0,
     .           						std::size_t num_buckets = 1000)
     .           				: _jobs_by_win(Interval<Time>{0, max_deadline(jobs)},
     2 ( 0.00%)  							   max_deadline(jobs) / num_buckets),
    33 ( 0.00%)  				  jobs(jobs), aborted(false), timed_out(false), be_naive(false), timeout(max_cpu_time), max_depth(max_depth), num_states(0), num_edges(0), width(0), current_job_count(0), num_cpus(num_cpus), jobs_by_latest_arrival(_jobs_by_latest_arrival), jobs_by_earliest_arrival(_jobs_by_earliest_arrival), jobs_by_deadline(_jobs_by_deadline), jobs_by_win(_jobs_by_win), _predecessors(jobs.size()), predecessors(_predecessors)
 7,756 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::vector(unsigned long, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > const&) [clone .constprop.0] (1x)
     .           			{
 7,524 ( 0.00%)  				for (const Job<Time> &j : jobs)
     .           				{
     .           					_jobs_by_latest_arrival.insert({j.latest_arrival(), &j});
     .           					_jobs_by_earliest_arrival.insert({j.earliest_arrival(), &j});
     .           					_jobs_by_deadline.insert({j.get_deadline(), &j});
     .           					_jobs_by_win.insert(j);
     .           				}
     .           
     2 ( 0.00%)  				for (auto e : dag_edges)
     .           				{
     .           					const Job<Time> &from = lookup<Time>(jobs, e.first);
     .           					const Job<Time> &to = lookup<Time>(jobs, e.second);
     .           					_predecessors[index_of(to)].push_back(index_of(from));
     .           				}
     8 ( 0.00%)  			}
     .           
     .           		protected:
     .           			void count_edge()
     .           			{
     .           #ifdef CONFIG_PARALLEL
     .           				edge_counter.local()++;
     .           #else
     .           				num_edges++;
     .           #endif
     .           			}
     .           
     .           			static Time max_deadline(const Workload &jobs)
     .           			{
     2 ( 0.00%)  				Time dl = 0;
10,032 ( 0.00%)  				for (const auto &j : jobs)
     .           					dl = std::max(dl, j.get_deadline());
     .           				return dl;
     .           			}
     .           
     .           			void update_finish_times(Response_times &r, const JobID &id, Interval<Time> range)
     .           			{
     .           				auto rbounds = r.find(id);
     .           				if (rbounds == r.end())
-- line 326 ----------------------------------------
-- line 352 ----------------------------------------
     .           #else
     .           					rta;
     .           #endif
     .           				update_finish_times(r, j, range);
     .           			}
     .           
     .           			std::size_t index_of(const Job<Time> &j) const
     .           			{
57,783 ( 0.00%)  				return (std::size_t)(&j - &(jobs[0]));
     .           			}
     .           
     .           			const Job_precedence_set &predecessors_of(const Job<Time> &j) const
     .           			{
     .           				return predecessors[index_of(j)];
     .           			}
     .           
     .           			void check_for_deadline_misses(const State &old_s, const State &new_s)
-- line 368 ----------------------------------------
-- line 405 ----------------------------------------
     .           						break;
     .           				}
     .           			}
     .           
     .           			void make_initial_state()
     .           			{
     .           				// construct initial state
     .           				// Plaats een nieuwe item aan het einde van de lijst, dit gebruikt de standaard constructor van het type dat is gedefineerd
     2 ( 0.00%)  				states_storage.emplace_back();
     .           				new_state(num_cpus);
     .           			}
     .           
     .           			States &states()
     .           			{
     .           #ifdef CONFIG_PARALLEL
     .           				return states_storage.back().local();
     .           #else
-- line 421 ----------------------------------------
-- line 456 ----------------------------------------
     .           			// Hier willen we kijken of er een nieuwe mergeable state kan worden gemaakt
     .           			template <typename... Args>
     .           			State &new_or_merged_state(Args &&...args)
     .           			{
     .           				// std::cout << "Trying to merge two states" << std::endl;
     .           				State_ref s_ref = alloc_state(std::forward<Args>(args)...);
     .           
     .           				// try to merge the new state into an existing state
     4 ( 0.00%)  				State_ref s = merge_or_cache(s_ref);
   606 ( 0.00%)  => include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::merge_or_cache(std::_Deque_iterator<NP::Global::Schedule_state<long long>, NP::Global::Schedule_state<long long>&, NP::Global::Schedule_state<long long>*>) (1x)
     2 ( 0.00%)  				if (s != s_ref)
     .           				{
     .           					// great, we merged!
     .           					// clean up the just-created state that we no longer need
     .           					DM(<< "We merged the states:" << std::endl
     .           					   << "\t" << *s << std::endl
     .           					   << "\t\tand" << std::endl
     .           					   << "\t" << *s_ref << std::endl);
     .           					dealloc_state(s_ref);
-- line 473 ----------------------------------------
-- line 528 ----------------------------------------
     .           
     .           				auto pair_it = res.first;
     .           				State_refs &list = pair_it->second;
     .           
     .           				list.push_front(s);
     .           			}
     .           
     .           			// and: dit is de niet parallel merge
    10 ( 0.00%)  			State_ref merge_or_cache(State_ref s_ref)
     .           			{
     .           				State &s = *s_ref;
     .           				// std::cout<<"looking for "<<s.get_key()<<std::endl;
     .           				const auto pair_it = states_by_key.find(s.get_key());
     .           
     .           				// cannot merge if key doesn't exist
     .           				if (pair_it != states_by_key.end())
     .           				{
-- line 544 ----------------------------------------
-- line 548 ----------------------------------------
     .           				}
     .           				else
     .           				{
     .           					// std::cout<<"\tapparently nothing with the same key"<<std::endl;
     .           				}
     .           				// if we reach here, we failed to merge
     .           				cache_state(s_ref);
     .           				return s_ref;
     8 ( 0.00%)  			}
     .           #endif
     .           
     .           			void check_cpu_timeout()
     .           			{
    24 ( 0.00%)  				if (timeout && get_cpu_time() > timeout)
     .           				{
     .           					aborted = true;
     .           					timed_out = true;
     .           					DM("cpu timeout abort" << std::endl);
     .           				}
     .           			}
     .           
     .           			void check_depth_abort()
     .           			{
     6 ( 0.00%)  				if (max_depth && current_job_count > max_depth)
     .           				{
     .           					aborted = true;
     .           					DM("aborted due to exceeding max depth" << std::endl);
     .           				}
     .           			}
     .           
     .           			bool unfinished(const State &s, const Job<Time> &j) const
     .           			{
     .           				return s.job_incomplete(index_of(j));
     .           			}
     .           
 2,505 ( 0.00%)  			bool ready(const State &s, const Job<Time> &j) const
     .           			{
     .           				return unfinished(s, j) && s.job_ready(predecessors_of(j));
     .           			}
     .           
     .           			bool all_jobs_scheduled(const State &s) const
     .           			{
     3 ( 0.00%)  				return s.number_of_scheduled_jobs() == jobs.size();
     .           			}
     .           
     .           			// assumes j is ready
    40 ( 0.00%)  			Interval<Time> ready_times(const State &s, const Job<Time> &j) const
     .           			{
     .           				Interval<Time> r = j.arrival_window();
    35 ( 0.00%)  				for (auto pred : predecessors_of(j))
     .           				{
     .           					Interval<Time> ft{0, 0};
     .           					if (!s.get_finish_times(pred, ft))
     .           						ft = get_finish_times(jobs[pred]);
     .           					r.lower_bound(ft.min());
     .           					r.extend_to(ft.max());
     .           				}
     .           				return r;
    40 ( 0.00%)  			}
     .           
     .           			// assumes j is ready
     .           			Interval<Time> ready_times(
     .           				const State &s, const Job<Time> &j,
     .           				const Job_precedence_set &disregard) const
     .           			{
     .           				Interval<Time> r = j.arrival_window();
     .           				for (auto pred : predecessors_of(j))
-- line 614 ----------------------------------------
-- line 622 ----------------------------------------
     .           					r.lower_bound(ft.min());
     .           					r.extend_to(ft.max());
     .           				}
     .           				return r;
     .           			}
     .           
     .           			Time latest_ready_time(const State &s, const Job<Time> &j) const
     .           			{
    54 ( 0.00%)  				return ready_times(s, j).max();
   180 ( 0.00%)  => include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::ready_times(NP::Global::Schedule_state<long long> const&, NP::Job<long long> const&) const (5x)
     .           			}
     .           
     .           			Time earliest_ready_time(const State &s, const Job<Time> &j) const
     .           			{
     .           				return ready_times(s, j).min();
     .           			}
     .           
     .           			Time latest_ready_time(
-- line 638 ----------------------------------------
-- line 686 ----------------------------------------
     .           					}
     .           				}
     .           
     .           				return when;
     .           			}
     .           
     .           			// Find next time by which any job is certainly released.
     .           			// Note that this time may be in the past.
    24 ( 0.00%)  			Time next_job_ready(const State &s, const Time t_earliest) const
     .           			{
     4 ( 0.00%)  				Time when = Time_model::constants<Time>::infinity();
     .           
     .           				// check everything that overlaps with t_earliest
    38 ( 0.00%)  				for (const Job<Time> &j : jobs_by_win.lookup(t_earliest))
     .           					if (ready(s, j))
     .           						when = std::min(when, latest_ready_time(s, j));
     .           
     .           				// No point looking in the future when we've already
     .           				// found one in the present.
     6 ( 0.00%)  				if (when <= t_earliest)
     .           					return when;
     .           
     .           				// Ok, let's look also in the future.
     2 ( 0.00%)  				for (auto it = jobs_by_latest_arrival
     .           								   .lower_bound(t_earliest);
 5,030 ( 0.00%)  					 it != jobs_by_latest_arrival.end(); it++)
     .           				{
 2,513 ( 0.00%)  					const Job<Time> &j = *(it->second);
     .           
     .           					// check if we can stop looking
 5,026 ( 0.00%)  					if (when < j.latest_arrival())
     .           						break; // yep, nothing can lower 'when' at this point
     .           
     .           					// j is not relevant if it is already scheduled or blocked
     .           					if (ready(s, j))
     .           						// does it beat what we've already seen?
     .           						when = std::min(when, latest_ready_time(s, j));
     .           				}
     .           
     .           				return when;
    18 ( 0.00%)  			}
     .           
     .           			// assumes j is ready
     .           			// NOTE: we don't use Interval<Time> here because the
     .           			//       Interval c'tor sorts its arguments.
     .           			std::pair<Time, Time> start_times(
     .           				const State &s, const Job<Time> &j, Time t_wc) const
     .           			{
     .           				auto rt = ready_times(s, j);
-- line 734 ----------------------------------------
-- line 986 ----------------------------------------
     .           			// eerst gaan we een naive explore maken zonder merging
     .           			void explore_naively()
     .           			{
     .           				be_naive = true;
     .           				explore();
     .           			}
     .           
     .           			//============================================================== Normale explore =============================================================//
     8 ( 0.00%)  			void explore()
     .           			{
     .           				// make the initial state
     .           				make_initial_state();
     .           				int n_depth = 0;
     .           
     9 ( 0.00%)  				while (current_job_count < jobs.size())
     .           				{
     .           					unsigned long n;
     .           #ifdef CONFIG_PARALLEL
     .           					const auto &new_states_part = states_storage.back();
     .           					n = 0;
     .           					for (const States &new_states : new_states_part)
     .           					{
     .           						n += new_states.size();
-- line 1008 ----------------------------------------
-- line 1010 ----------------------------------------
     .           #else
     .           
     .           					// states returned de laatste state in de storage lijst
     .           					States &exploration_front = states();
     .           
     .           					// For the POR we need to sort the states_storage by the number of jobs it has scheduled.
     .           
     .           					// minimal states
     2 ( 0.00%)  					int minimal_scheduled_jobs = exploration_front.front().number_of_scheduled_jobs();
    12 ( 0.00%)  					for (const State &s : exploration_front)
     .           					{
     4 ( 0.00%)  						minimal_scheduled_jobs = (s.number_of_scheduled_jobs() < minimal_scheduled_jobs) ? s.number_of_scheduled_jobs() : minimal_scheduled_jobs;
     .           					}
     .           					// dus n geeft hier de grootte van de voorkant van de states aan
     .           					n = exploration_front.size();
     .           					// std::cout<<"Exploration front size :" << n <<std::endl;
     .           #endif
     .           
     .           					// allocate states space for next depth
     .           					// In de storage deque, plaats een nieuw object wat de nieuwe front wordt
     .           					states_storage.emplace_back();
     .           
     .           					// Moved this to where we explore, since we do not explore states that have a depth > minimal_scheduled_jobs
     4 ( 0.00%)  					num_states += n;
     .           
     .           					check_depth_abort();
     .           					check_cpu_timeout();
     6 ( 0.00%)  					if (aborted)
     .           						break;
     .           
     .           #ifdef CONFIG_PARALLEL
     .           
     .           					parallel_for(new_states_part.range(),
     .           								 [&](typename Split_states::const_range_type &r)
     .           								 {
     .           									 for (auto it = r.begin(); it != r.end(); it++)
-- line 1045 ----------------------------------------
-- line 1054 ----------------------------------------
     .           														   });
     .           									 }
     .           								 });
     .           
     .           #else
     .           					// and: Dit is een explore zonder parallelizatie mbv tbb
     .           					// Voor iedere state s i n de exploratie front, doe ze explroen
     .           					// states_storage.back().push_back(exploration_front.begin());
     8 ( 0.00%)  					for (const State &s : exploration_front)
     .           					{
     4 ( 0.00%)  						if (s.number_of_scheduled_jobs() == minimal_scheduled_jobs)
     .           						{
     .           							// num_states ++;
     .           							// std::cout<<"exploring state: "<<s<<std::endl;
     8 ( 0.00%)  							explore(s);
3,549,974,200 (99.08%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::explore(NP::Global::Schedule_state<long long> const&) (2x)
     .           							check_cpu_timeout();
     4 ( 0.00%)  							if (aborted)
     .           								break;
     .           						}
     .           						else
     .           						{
     .           							num_states--;
     .           							n--;
     .           							states_storage.back().push_back(s);
     .           						}
     .           					}
     .           #endif
     .           					// keep track of exploration front width here otherwise POR would get counted which is not wat we want
     8 ( 0.00%)  					width = std::max(width, n);
     .           
     .           					// clean up the state cache if necessary
     4 ( 0.00%)  					if (!be_naive)
     .           						states_by_key.clear();
     4 ( 0.00%)  					current_job_count = minimal_scheduled_jobs;
     .           					// std::cout << "d: " << current_job_count << " w: " << width <<std::endl;
     .           #ifdef CONFIG_PARALLEL
     .           					// propagate any updates to the response-time estimates
     .           					for (auto &r : partial_rta)
     .           						for (const auto &elem : r)
     .           							update_finish_times(rta, elem.first, elem.second);
     .           #endif
     .           
-- line 1095 ----------------------------------------
-- line 1106 ----------------------------------------
     .           								 });
     .           #endif
     .           					states_storage.pop_front();
     .           #endif
     .           				}
     .           
     .           #ifndef CONFIG_COLLECT_SCHEDULE_GRAPH
     .           				// clean out any remaining states
     6 ( 0.00%)  				while (!states_storage.empty())
     .           				{
     .           #ifdef CONFIG_PARALLEL
     .           					parallel_for(states_storage.front().range(),
     .           								 [](typename Split_states::range_type &r)
     .           								 {
     .           									 for (auto it = r.begin(); it != r.end(); it++)
     .           										 it->clear();
     .           								 });
-- line 1122 ----------------------------------------
-- line 1124 ----------------------------------------
     .           					states_storage.pop_front();
     .           				}
     .           #endif
     .           
     .           #ifdef CONFIG_PARALLEL
     .           				for (auto &c : edge_counter)
     .           					num_edges += c;
     .           #endif
     8 ( 0.00%)  			}
     .           			//========================================================== einde van normale explore ==================================================//
     .           
     .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
     .           			friend std::ostream &operator<<(std::ostream &out,
     .           											const State_space<Time> &space)
     .           			{
     .           				std::map<const Schedule_state<Time> *, unsigned int> state_id;
     .           				unsigned int i = 0;
-- line 1140 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp
--------------------------------------------------------------------------------
  No information has been collected for /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp
--------------------------------------------------------------------------------
Ir                  

-- line 180 ----------------------------------------
         .           
         .           class ThreadId {
         .           #if USE_PTHREAD
         .               std::atomic<pthread_t> tid;
         .           #else
         .               std::atomic<DWORD>     tid;
         .           #endif
         .           public:
       486 ( 0.00%)      ThreadId() : tid(GetMyTID()) {}
       486 ( 0.00%)  => ./nptl/pthread_self.c:pthread_self (243x)
         .           #if USE_PTHREAD
10,333,310 ( 0.29%)      bool isCurrentThreadId() const { return pthread_equal(pthread_self(), tid.load(std::memory_order_relaxed)); }
10,333,310 ( 0.29%)  => ./nptl/pthread_self.c:pthread_self (5,166,655x)
         .           #else
         .               bool isCurrentThreadId() const { return GetCurrentThreadId() == tid.load(std::memory_order_relaxed); }
         .           #endif
         .               ThreadId& operator=(const ThreadId& other) {
         .                   tid.store(other.tid.load(std::memory_order_relaxed), std::memory_order_relaxed);
         .                   return *this;
         .               }
         .               static bool init() { return true; }
-- line 198 ----------------------------------------
-- line 207 ----------------------------------------
         .           
         .           bool TLSKey::init()
         .           {
         .           #if USE_WINTHREAD
         .               TLS_pointer_key = TlsAlloc();
         .               if (TLS_pointer_key == TLS_ALLOC_FAILURE)
         .                   return false;
         .           #else
         9 ( 0.00%)      int status = pthread_key_create( &TLS_pointer_key, mallocThreadShutdownNotification );
     1,152 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         2 ( 0.00%)      if ( status )
         .                   return false;
         .           #endif /* USE_WINTHREAD */
         .               return true;
         .           }
         .           
         .           bool TLSKey::destroy()
         .           {
         .           #if USE_WINTHREAD
-- line 224 ----------------------------------------
-- line 227 ----------------------------------------
         .               int status1 = pthread_key_delete(TLS_pointer_key);
         .           #endif /* USE_WINTHREAD */
         .               MALLOC_ASSERT(!status1, "The memory manager cannot delete tls key.");
         .               return status1==0;
         .           }
         .           
         .           inline TLSData* TLSKey::getThreadMallocTLS() const
         .           {
20,668,977 ( 0.58%)      return (TLSData *)TlsGetValue_func( TLS_pointer_key );
     3,689 ( 0.00%)  => ./nptl/pthread_getspecific.c:pthread_getspecific (217x)
         .           }
         .           
         .           inline void TLSKey::setThreadMallocTLS( TLSData * newvalue ) {
         .               RecursiveMallocCallProtector scoped;
        13 ( 0.00%)      TlsSetValue_func( TLS_pointer_key, newvalue );
     1,197 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         .           }
         .           
         .           /* The 'next' field in the block header has to maintain some invariants:
         .            *   it needs to be on a 16K boundary and the first field in the block.
         .            *   Any value stored there needs to have the lower 14 bits set to 0
         .            *   so that various assert work. This means that if you want to smash this memory
         .            *   for debugging purposes you will need to obey this invariant.
         .            * The total size of the header needs to be a power of 2 to simplify
-- line 248 ----------------------------------------
-- line 346 ----------------------------------------
         .           
         .           // Use inheritance to guarantee that a user data start on next cache line.
         .           // Can't use member for it, because when LocalBlockFields already on cache line,
         .           // we must have no additional memory consumption for all compilers.
         .           class Block : public LocalBlockFields,
         .                         Padding<2*blockHeaderAlignment - sizeof(LocalBlockFields)> {
         .           public:
         .               bool empty() const {
10,333,360 ( 0.29%)          if (allocatedCount > 0) return false;
         .                   MALLOC_ASSERT(!isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT);
         .                   return true;
         .               }
         .               inline FreeObject* allocate();
         .               inline FreeObject *allocateFromFreeList();
         .           
         .               inline bool adjustFullness();
         .               void adjustPositionInBin(Bin* bin = nullptr);
-- line 362 ----------------------------------------
-- line 369 ----------------------------------------
         .               void privatizePublicFreeList( bool reset = true );
         .               void restoreBumpPtr();
         .               void privatizeOrphaned(TLSData *tls, unsigned index);
         .               bool readyToShare();
         .               void shareOrphaned(intptr_t binTag, unsigned index);
         .               unsigned int getSize() const {
         .                   MALLOC_ASSERT(isStartupAllocObject() || objectSize<minLargeObjectSize,
         .                                 "Invalid object size");
         9 ( 0.00%)          return isStartupAllocObject()? 0 : objectSize;
         .               }
         .               const BackRefIdx *getBackRefIdx() const { return &backRefIdx; }
         .               inline bool isOwnedByCurrentThread() const;
         3 ( 0.00%)      bool isStartupAllocObject() const { return objectSize == startupAllocObjSizeMark; }
         .               inline FreeObject *findObjectToFree(const void *object) const;
         .               void checkFreePrecond(const void *object) const {
         .           #if MALLOC_DEBUG
         .                   const char *msg = "Possible double free or heap corruption.";
         .                   // small objects are always at least sizeof(size_t) Byte aligned,
         .                   // try to check this before this dereference as for invalid objects
         .                   // this may be unreadable
         .                   MALLOC_ASSERT(isAligned(object, sizeof(size_t)), "Try to free invalid small object");
-- line 389 ----------------------------------------
-- line 453 ----------------------------------------
         .           class Bin {
         .           private:
         .           public:
         .               Block *activeBlk;
         .               std::atomic<Block*> mailbox;
         .               MallocMutex mailLock;
         .           
         .           public:
 5,166,687 ( 0.14%)      inline Block* getActiveBlock() const { return activeBlk; }
        25 ( 0.00%)      void resetActiveBlock() { activeBlk = nullptr; }
         .               inline void setActiveBlock(Block *block);
         .               inline Block* setPreviousBlockActive();
         .               Block* getPrivatizedFreeListBlock();
         .               void moveBlockToFront(Block *block);
         .               bool cleanPublicFreeLists();
         .               void processEmptyBlock(Block *block, bool poolTheBlock);
         .               void addPublicFreeListBlock(Block* block);
         .           
-- line 470 ----------------------------------------
-- line 546 ----------------------------------------
         .                   ResOfGet() = delete;
         .               public:
         .                   Block* block;
         .                   bool   lastAccMiss;
         .                   ResOfGet(Block *b, bool lastMiss) : block(b), lastAccMiss(lastMiss) {}
         .               };
         .           
         .               // allocated in zero-initialized memory
         1 ( 0.00%)      FreeBlockPool(Backend *bknd) : backend(bknd) {}
         .               ResOfGet getBlock();
         .               void returnBlock(Block *block);
         .               bool externalCleanup(); // can be called by another thread
         .           };
         .           
         .           template<int LOW_MARK, int HIGH_MARK>
         .           class LocalLOCImpl {
         .           private:
-- line 562 ----------------------------------------
-- line 586 ----------------------------------------
         .           public:
         .               Bin           bin[numBlockBinLimit];
         .               FreeBlockPool freeSlabBlocks;
         .               LocalLOC      lloc;
         .               unsigned      currCacheIdx;
         .           private:
         .               std::atomic<bool> unused;
         .           public:
        95 ( 0.00%)      TLSData(MemoryPool *mPool, Backend *bknd) : memPool(mPool), freeSlabBlocks(bknd) {}
         .               MemoryPool *getMemPool() const { return memPool; }
         .               Bin* getAllocationBin(size_t size);
         .               void release();
         .               bool externalCleanup(bool cleanOnlyUnused, bool cleanBins) {
         .                   if (!unused.load(std::memory_order_relaxed) && cleanOnlyUnused) return false;
         .                   // Heavy operation in terms of synchronization complexity,
         .                   // should be called only for the current thread
         .                   bool released = cleanBins ? cleanupBlockBins() : false;
         .                   // both cleanups to be called, and the order is not important
         4 ( 0.00%)          return released | lloc.externalCleanup(&memPool->extMemPool) | freeSlabBlocks.externalCleanup();
     3,028 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::externalCleanup() (1x)
         .               }
         .               bool cleanupBlockBins();
         .               void markUsed() { unused.store(false, std::memory_order_relaxed); } // called by owner when TLS touched
         .               void markUnused() { unused.store(true, std::memory_order_relaxed); } // can be called by not owner thread
         .           };
         .           
         .           TLSData *TLSKey::createTLS(MemoryPool *memPool, Backend *backend)
        11 ( 0.00%)  {
         .               MALLOC_ASSERT( sizeof(TLSData) >= sizeof(Bin) * numBlockBins + sizeof(FreeBlockPool), ASSERT_TEXT );
         1 ( 0.00%)      TLSData* tls = (TLSData*) memPool->bootStrapBlocks.allocate(memPool, sizeof(TLSData));
         .               if ( !tls )
         .                   return nullptr;
         .               new(tls) TLSData(memPool, backend);
         .               /* the block contains zeroes after bootStrapMalloc, so bins are initialized */
         .           #if MALLOC_DEBUG
         .               for (uint32_t i = 0; i < numBlockBinLimit; i++)
         .                   tls->bin[i].verifyInitState();
         .           #endif
         .               setThreadMallocTLS(tls);
         .               memPool->extMemPool.allLocalCaches.registerThread(tls);
         .               return tls;
         9 ( 0.00%)  }
         .           
         .           bool TLSData::cleanupBlockBins()
         .           {
         .               bool released = false;
         .               for (uint32_t i = 0; i < numBlockBinLimit; i++) {
         .                   released |= bin[i].cleanPublicFreeLists();
         .                   // After cleaning public free lists, only the active block might be empty.
         .                   // Do not use processEmptyBlock because it will just restore bumpPtr.
-- line 634 ----------------------------------------
-- line 651 ----------------------------------------
         .               if (TLSData *tlsData = tlsPointerKey.getThreadMallocTLS())
         .                   released |= tlsData->cleanupBlockBins();
         .           
         .               return released;
         .           }
         .           
         .           void AllLocalCaches::registerThread(TLSRemote *tls)
         .           {
         1 ( 0.00%)      tls->prev = nullptr;
         1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
         .               MALLOC_ASSERT(head!=tls, ASSERT_TEXT);
         2 ( 0.00%)      tls->next = head;
         2 ( 0.00%)      if (head)
         .                   head->prev = tls;
         1 ( 0.00%)      head = tls;
         .               MALLOC_ASSERT(head->next!=head, ASSERT_TEXT);
         .           }
         .           
         .           void AllLocalCaches::unregisterThread(TLSRemote *tls)
         .           {
         1 ( 0.00%)      MallocMutex::scoped_lock lock(listLock);
         .               MALLOC_ASSERT(head, "Can't unregister thread: no threads are registered.");
         2 ( 0.00%)      if (head == tls)
         3 ( 0.00%)          head = tls->next;
         2 ( 0.00%)      if (tls->next)
         1 ( 0.00%)          tls->next->prev = tls->prev;
         2 ( 0.00%)      if (tls->prev)
         .                   tls->prev->next = tls->next;
         .               MALLOC_ASSERT(!tls->next || tls->next->next!=tls->next, ASSERT_TEXT);
         .           }
         .           
         .           bool AllLocalCaches::cleanup(bool cleanOnlyUnused)
         .           {
         .               bool released = false;
         .               {
-- line 685 ----------------------------------------
-- line 688 ----------------------------------------
         .                       released |= static_cast<TLSData*>(curr)->externalCleanup(cleanOnlyUnused, /*cleanBins=*/false);
         .               }
         .               return released;
         .           }
         .           
         .           void AllLocalCaches::markUnused()
         .           {
         .               bool locked;
         2 ( 0.00%)      MallocMutex::scoped_lock lock(listLock, /*block=*/false, &locked);
         4 ( 0.00%)      if (!locked) // not wait for marking if someone doing something with it
         .                   return;
         .           
        14 ( 0.00%)      for (TLSRemote *curr=head; curr; curr=curr->next)
         .                   static_cast<TLSData*>(curr)->markUnused();
         .           }
         .           
         .           #if MALLOC_CHECK_RECURSION
         .           MallocMutex RecursiveMallocCallProtector::rmc_mutex;
         .           std::atomic<pthread_t> RecursiveMallocCallProtector::owner_thread;
         .           std::atomic<void*> RecursiveMallocCallProtector::autoObjPtr;
         .           bool        RecursiveMallocCallProtector::mallocRecursionDetected;
-- line 708 ----------------------------------------
-- line 768 ----------------------------------------
         .           #endif
         .           static inline unsigned int highestBitPos(unsigned int n)
         .           {
         .               MALLOC_ASSERT( n>=64 && n<1024, ASSERT_TEXT ); // only needed for bsr array lookup, but always true
         .               unsigned int pos;
         .           #if __ARCH_x86_32||__ARCH_x86_64
         .           
         .           # if __unix__||__APPLE__||__MINGW32__
    10,050 ( 0.00%)      __asm__ ("bsr %1,%0" : "=r"(pos) : "r"(n));
         .           # elif (_WIN32 && (!_WIN64 || __INTEL_COMPILER))
         .               __asm
         .               {
         .                   bsr eax, n
         .                   mov pos, eax
         .               }
         .           # elif _WIN64 && _MSC_VER>=1400
         .               _BitScanReverse((unsigned long*)&pos, (unsigned long)n);
-- line 784 ----------------------------------------
-- line 796 ----------------------------------------
         .               static unsigned int bsr[16] = {0/*N/A*/,6,7,7,8,8,8,8,9,9,9,9,9,9,9,9};
         .               pos = bsr[ n>>6 ];
         .           #endif /* __ARCH_* */
         .               return pos;
         .           }
         .           
         .           unsigned int getSmallObjectIndex(unsigned int size)
         .           {
15,262,572 ( 0.43%)      unsigned int result = (size-1)>>3;
         .               if (sizeof(void*)==8) {
         .                   // For 64-bit malloc, 16 byte alignment is needed except for bin 0.
38,156,534 ( 1.06%)          if (result) result |= 1; // 0,1,3,5,7; bins 2,4,6 are not aligned to 16 bytes
         .               }
         .               return result;
         .           }
         .           
         .           /*
         .            * Depending on indexRequest, for a given size return either the index into the bin
         .            * for objects of this size, or the actual size of objects in this bin.
         .            */
         .           template<bool indexRequest>
         .           static unsigned int getIndexOrObjectSize (unsigned int size)
         .           {
15,282,974 ( 0.43%)      if (size <= maxSmallObjectSize) { // selection from 8/16/24/32/40/48/56/64
         .                   unsigned int index = getSmallObjectIndex( size );
         .                    /* Bin 0 is for 8 bytes, bin 1 is for 16, and so forth */
         .                   return indexRequest ? index : (index+1)<<3;
         .               }
    20,332 ( 0.00%)      else if (size <= maxSegregatedObjectSize ) { // 80/96/112/128 / 160/192/224/256 / 320/384/448/512 / 640/768/896/1024
    10,050 ( 0.00%)          unsigned int order = highestBitPos(size-1); // which group of bin sizes?
         .                   MALLOC_ASSERT( 6<=order && order<=9, ASSERT_TEXT );
         .                   if (indexRequest)
    39,540 ( 0.00%)              return minSegregatedObjectIndex - (4*6) - 4 + (4*order) + ((size-1)>>(order-2));
         .                   else {
       825 ( 0.00%)              unsigned int alignment = 128 >> (9-order); // alignment in the group
         .                       MALLOC_ASSERT( alignment==16 || alignment==32 || alignment==64 || alignment==128, ASSERT_TEXT );
         .                       return alignUp(size,alignment);
         .                   }
         .               }
         .               else {
       236 ( 0.00%)          if( size <= fittingSize3 ) {
       132 ( 0.00%)              if( size <= fittingSize2 ) {
        19 ( 0.00%)                  if( size <= fittingSize1 )
         .                               return indexRequest ? minFittingIndex : fittingSize1;
         .                           else
       260 ( 0.00%)                      return indexRequest ? minFittingIndex+1 : fittingSize2;
         .                       } else
        62 ( 0.00%)                  return indexRequest ? minFittingIndex+2 : fittingSize3;
         .                   } else {
       100 ( 0.00%)              if( size <= fittingSize5 ) {
        20 ( 0.00%)                  if( size <= fittingSize4 )
         .                               return indexRequest ? minFittingIndex+3 : fittingSize4;
         .                           else
       184 ( 0.00%)                      return indexRequest ? minFittingIndex+4 : fittingSize5;
         .                       } else {
         .                           MALLOC_ASSERT( 0,ASSERT_TEXT ); // this should not happen
        46 ( 0.00%)                  return ~0U;
         .                       }
         .                   }
         .               }
        56 ( 0.00%)  }
         .           
         .           static unsigned int getIndex (unsigned int size)
         .           {
12,808,147 ( 0.36%)      return getIndexOrObjectSize</*indexRequest=*/true>(size);
     2,378 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:unsigned int rml::internal::getIndexOrObjectSize<true>(unsigned int) (242x)
         .           }
         .           
         .           static unsigned int getObjectSize (unsigned int size)
         .           {
         .               return getIndexOrObjectSize</*indexRequest=*/false>(size);
         .           }
         .           
         .           
-- line 868 ----------------------------------------
-- line 870 ----------------------------------------
         .           {
         .               FreeObject *result;
         .           
         .               MALLOC_ASSERT( size == sizeof(TLSData), ASSERT_TEXT );
         .           
         .               { // Lock with acquire
         .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
         .           
         3 ( 0.00%)          if( bootStrapObjectList) {
         .                       result = bootStrapObjectList;
         .                       bootStrapObjectList = bootStrapObjectList->next;
         .                   } else {
         3 ( 0.00%)              if (!bootStrapBlock) {
         4 ( 0.00%)                  bootStrapBlock = memPool->getEmptyBlock(size);
       584 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (1x)
         2 ( 0.00%)                  if (!bootStrapBlock) return nullptr;
         .                       }
         1 ( 0.00%)              result = bootStrapBlock->bumpPtr;
         5 ( 0.00%)              bootStrapBlock->bumpPtr = (FreeObject *)((uintptr_t)bootStrapBlock->bumpPtr - bootStrapBlock->objectSize);
         3 ( 0.00%)              if ((uintptr_t)bootStrapBlock->bumpPtr < (uintptr_t)bootStrapBlock+sizeof(Block)) {
         .                           bootStrapBlock->bumpPtr = nullptr;
         .                           bootStrapBlock->next = bootStrapBlockUsed;
         .                           bootStrapBlockUsed = bootStrapBlock;
         .                           bootStrapBlock = nullptr;
         .                       }
         .                   }
         .               } // Unlock with release
       115 ( 0.00%)      memset (result, 0, size);
         .               return (void*)result;
         .           }
         .           
         .           void BootStrapBlocks::free(void* ptr)
         .           {
         .               MALLOC_ASSERT( ptr, ASSERT_TEXT );
         .               { // Lock with acquire
         .                   MallocMutex::scoped_lock scoped_cs(bootStrapLock);
         3 ( 0.00%)          ((FreeObject*)ptr)->next = bootStrapObjectList;
         1 ( 0.00%)          bootStrapObjectList = (FreeObject*)ptr;
         .               } // Unlock with release
         .           }
         .           
         .           void BootStrapBlocks::reset()
         .           {
         .               bootStrapBlock = bootStrapBlockUsed = nullptr;
         .               bootStrapObjectList = nullptr;
         .           }
-- line 914 ----------------------------------------
-- line 927 ----------------------------------------
         .           LifoList::LifoList( ) : top(nullptr)
         .           {
         .               // MallocMutex assumes zero initialization
         .               memset(&lock, 0, sizeof(MallocMutex));
         .           }
         .           
         .           void LifoList::push(Block *block)
         .           {
         3 ( 0.00%)      MallocMutex::scoped_lock scoped_cs(lock);
         3 ( 0.00%)      block->next = top.load(std::memory_order_relaxed);
         .               top.store(block, std::memory_order_relaxed);
         .           }
         .           
         .           Block *LifoList::pop()
         .           {
       242 ( 0.00%)      Block* block = nullptr;
       484 ( 0.00%)      if (top.load(std::memory_order_relaxed)) {
         .                   MallocMutex::scoped_lock scoped_cs(lock);
         .                   block = top.load(std::memory_order_relaxed);
         .                   if (block) {
         .                       top.store(block->next, std::memory_order_relaxed);
         .                   }
         .               }
         .               return block;
         .           }
-- line 951 ----------------------------------------
-- line 980 ----------------------------------------
         .                        backend->returnLargeObject(lmb);
         .                    }
         .                }
         .           }
         .           
         .           TLSData* MemoryPool::getTLS(bool create)
         .           {
         .               TLSData* tls = extMemPool.tlsPointerKey.getThreadMallocTLS();
10,333,476 ( 0.29%)      if (create && !tls)
10,333,481 ( 0.29%)          tls = extMemPool.tlsPointerKey.createTLS(this, &extMemPool.backend);
     2,123 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::TLSKey::createTLS(rml::internal::MemoryPool*, rml::internal::Backend*) (1x)
         .               return tls;
         .           }
         .           
         .           /*
         .            * Return the bin for the given size.
         .            */
         .           inline Bin* TLSData::getAllocationBin(size_t size)
         .           {
23,155,387 ( 0.65%)      return bin + getIndex(size);
         .           }
         .           
         .           /* Return an empty uninitialized block in a non-blocking fashion. */
         .           Block *MemoryPool::getEmptyBlock(size_t size)
     2,187 ( 0.00%)  {
         .               TLSData* tls = getTLS(/*create=*/false);
         .               // try to use per-thread cache, if TLS available
         .               FreeBlockPool::ResOfGet resOfGet = tls?
       728 ( 0.00%)          tls->freeSlabBlocks.getBlock() : FreeBlockPool::ResOfGet(nullptr, false);
         .               Block *result = resOfGet.block;
         .           
         .               if (!result) { // not found in local cache, asks backend for slabs
        95 ( 0.00%)          int num = resOfGet.lastAccMiss? Backend::numOfSlabAllocOnMiss : 1;
         .                   BackRefIdx backRefIdx[Backend::numOfSlabAllocOnMiss];
         .           
        94 ( 0.00%)          result = static_cast<Block*>(extMemPool.backend.getSlabBlock(num));
       188 ( 0.00%)          if (!result) return nullptr;
         .           
       188 ( 0.00%)          if (!extMemPool.userPool())
       188 ( 0.00%)              for (int i=0; i<num; i++) {
     1,309 ( 0.00%)                  backRefIdx[i] = BackRefIdx::newBackRef(/*largeObj=*/false);
    12,716 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (187x)
       374 ( 0.00%)                  if (backRefIdx[i].isInvalid()) {
         .                               // roll back resource allocation
         .                               for (int j=0; j<i; j++)
         .                                   removeBackRef(backRefIdx[j]);
         .                               Block *b = result;
         .                               for (int j=0; j<num; b=(Block*)((uintptr_t)b+slabSize), j++)
         .                                   extMemPool.backend.putSlabBlock(b);
         .                               return nullptr;
         .                           }
         .                       }
         .                   // resources were allocated, register blocks
         .                   Block *b = result;
       188 ( 0.00%)          for (int i=0; i<num; b=(Block*)((uintptr_t)b+slabSize), i++) {
         .                       // slab block in user's pool must have invalid backRefIdx
       374 ( 0.00%)              if (extMemPool.userPool()) {
         .                           new (&b->backRefIdx) BackRefIdx();
         .                       } else {
       187 ( 0.00%)                  setBackRef(backRefIdx[i], b);
       935 ( 0.00%)                  b->backRefIdx = backRefIdx[i];
         .                       }
         .                       b->tlsPtr.store(tls, std::memory_order_relaxed);
       187 ( 0.00%)              b->poolPtr = this;
         .                       // all but first one go to per-thread pool
       374 ( 0.00%)              if (i > 0) {
         .                           MALLOC_ASSERT(tls, ASSERT_TEXT);
       186 ( 0.00%)                  tls->freeSlabBlocks.returnBlock(b);
     1,953 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (93x)
         .                       }
         .                   }
         .               }
         .               MALLOC_ASSERT(result, ASSERT_TEXT);
         .               result->initEmptyBlock(tls, size);
         .               STAT_increment(getThreadId(), getIndex(result->objectSize), allocBlockNew);
         .               return result;
     2,187 ( 0.00%)  }
         .           
         .           void MemoryPool::returnEmptyBlock(Block *block, bool poolTheBlock)
     1,195 ( 0.00%)  {
         .               block->reset();
       478 ( 0.00%)      if (poolTheBlock) {
       651 ( 0.00%)          getTLS(/*create=*/false)->freeSlabBlocks.returnBlock(block);
    46,488 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::FreeBlockPool::returnBlock(rml::internal::Block*) (217x)
         .               } else {
         .                   // slab blocks in user's pools do not have valid backRefIdx
        44 ( 0.00%)          if (!extMemPool.userPool())
        66 ( 0.00%)              removeBackRef(*(block->getBackRefIdx()));
       902 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (22x)
        66 ( 0.00%)          extMemPool.backend.putSlabBlock(block);
     7,125 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (22x)
         .               }
       717 ( 0.00%)  }
         .           
         .           bool ExtMemoryPool::init(intptr_t poolId, rawAllocType rawAlloc,
         .                                    rawFreeType rawFree, size_t granularity,
         .                                    bool keepAllMemory, bool fixedPool)
         .           {
         1 ( 0.00%)      this->poolId = poolId;
         1 ( 0.00%)      this->rawAlloc = rawAlloc;
         1 ( 0.00%)      this->rawFree = rawFree;
         1 ( 0.00%)      this->granularity = granularity;
         2 ( 0.00%)      this->keepAllMemory = keepAllMemory;
         1 ( 0.00%)      this->fixedPool = fixedPool;
         .               this->delayRegsReleasing = false;
         .               if (!initTLS())
         .                   return false;
         3 ( 0.00%)      loc.init(this);
     1,296 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/large_objects.cpp:rml::internal::LargeObjectCache::init(rml::internal::ExtMemoryPool*) [clone .part.0] (1x)
         .               backend.init(this);
         .               MALLOC_ASSERT(isPoolValid(), nullptr);
         .               return true;
         .           }
         .           
         .           bool ExtMemoryPool::initTLS() { return tlsPointerKey.init(); }
         .           
         .           bool MemoryPool::init(intptr_t poolId, const MemPoolPolicy *policy)
-- line 1089 ----------------------------------------
-- line 1148 ----------------------------------------
         .                   // for user pool, because it's just about to be released. But for system
         .                   // pool restoring, we do not want to do zeroing of it on subsequent reload.
         .                   bootStrapBlocks.reset();
         .                   extMemPool.orphanedBlocks.reset();
         .               }
         .               return extMemPool.destroy();
         .           }
         .           
         9 ( 0.00%)  void MemoryPool::onThreadShutdown(TLSData *tlsData)
         .           {
         5 ( 0.00%)      if (tlsData) { // might be called for "empty" TLS
    15,761 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::onThreadShutdown(rml::internal::TLSData*) [clone .part.0] (1x)
         .                   tlsData->release();
         2 ( 0.00%)          bootStrapBlocks.free(tlsData);
         .                   clearTLS();
         .               }
         8 ( 0.00%)  }
         .           
         .           #if MALLOC_DEBUG
         .           void Bin::verifyTLSBin (size_t size) const
         .           {
         .           /* The debug version verifies the TLSBin as needed */
         .               uint32_t objSize = getObjectSize(size);
         .           
         .               if (activeBlk) {
-- line 1171 ----------------------------------------
-- line 1209 ----------------------------------------
         .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
         .               MALLOC_ASSERT( block->objectSize != 0, ASSERT_TEXT );
         .               MALLOC_ASSERT( block->next == nullptr, ASSERT_TEXT );
         .               MALLOC_ASSERT( block->previous == nullptr, ASSERT_TEXT );
         .           
         .               MALLOC_ASSERT( this, ASSERT_TEXT );
         .               verifyTLSBin(size);
         .           
     4,148 ( 0.00%)      block->next = activeBlk;
     7,812 ( 0.00%)      if( activeBlk ) {
     7,762 ( 0.00%)          block->previous = activeBlk->previous;
     3,881 ( 0.00%)          activeBlk->previous = block;
    11,643 ( 0.00%)          if( block->previous )
     6,768 ( 0.00%)              block->previous->next = block;
         .               } else {
         .                   activeBlk = block;
         .               }
         .           
         .               verifyTLSBin(size);
         .           }
         .           
         .           /*
-- line 1230 ----------------------------------------
-- line 1239 ----------------------------------------
         .           
         .               MALLOC_ASSERT( this, ASSERT_TEXT );
         .               verifyTLSBin(size);
         .           
         .               if (block == activeBlk) {
         .                   activeBlk = block->previous? block->previous : block->next;
         .               }
         .               /* Unlink the block */
    11,643 ( 0.00%)      if (block->previous) {
         .                   MALLOC_ASSERT( block->previous->next == block, ASSERT_TEXT );
     7,700 ( 0.00%)          block->previous->next = block->next;
         .               }
     7,762 ( 0.00%)      if (block->next) {
         .                   MALLOC_ASSERT( block->next->previous == block, ASSERT_TEXT );
     3,858 ( 0.00%)          block->next->previous = block->previous;
         .               }
       217 ( 0.00%)      block->next = nullptr;
     3,881 ( 0.00%)      block->previous = nullptr;
         .           
         .               verifyTLSBin(size);
         .           }
         .           
         .           Block* Bin::getPrivatizedFreeListBlock()
         .           {
         .               Block* block;
         .               MALLOC_ASSERT( this, ASSERT_TEXT );
         .               // if this method is called, active block usage must be unsuccessful
         .               MALLOC_ASSERT( !activeBlk && !mailbox.load(std::memory_order_relaxed) || activeBlk && activeBlk->isFull, ASSERT_TEXT );
         .           
         .           // the counter should be changed    STAT_increment(getThreadId(), ThreadCommonCounters, lockPublicFreeList);
       484 ( 0.00%)      if (!mailbox.load(std::memory_order_acquire)) // hotpath is empty mailbox
         .                   return nullptr;
         .               else { // mailbox is not empty, take lock and inspect it
         .                   MallocMutex::scoped_lock scoped_cs(mailLock);
         .                   block = mailbox.load(std::memory_order_relaxed);
         .                   if( block ) {
         .                       MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
         .                       MALLOC_ASSERT( !isNotForUse(block->nextPrivatizable.load(std::memory_order_relaxed)), ASSERT_TEXT );
         .                       mailbox.store(block->nextPrivatizable.load(std::memory_order_relaxed), std::memory_order_relaxed);
-- line 1277 ----------------------------------------
-- line 1319 ----------------------------------------
         .                       block->adjustPositionInBin(this);
         .                   block = tmp;
         .               }
         .               return released;
         .           }
         .           
         .           bool Block::adjustFullness()
         .           {
   592,848 ( 0.02%)      if (bumpPtr) {
         .                   /* If we are still using a bump ptr for this block it is empty enough to use. */
         .                   STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
         .                   isFull = false;
         .               } else {
         .                   const float threshold = (slabSize - sizeof(Block)) * (1 - emptyEnoughRatio);
         .                   /* allocatedCount shows how many objects in the block are in use; however it still counts
         .                    * blocks freed by other threads; so prior call to privatizePublicFreeList() is recommended */
 2,074,968 ( 0.06%)          isFull = (allocatedCount*objectSize > threshold) ? true : false;
         .           #if COLLECT_STATISTICS
         .                   if (isFull)
         .                       STAT_increment(getThreadId(), getIndex(objectSize), examineNotEmpty);
         .                   else
         .                       STAT_increment(getThreadId(), getIndex(objectSize), examineEmptyEnough);
         .           #endif
         .               }
         .               return isFull;
         .           }
         .           
         .           // This method resides in class Block, and not in class Bin, in order to avoid
         .           // calling getAllocationBin on a reasonably hot path in Block::freeOwnObject
         .           void Block::adjustPositionInBin(Bin* bin/*=nullptr*/)
 2,696,220 ( 0.08%)  {
         .               // If the block were full, but became empty enough to use,
         .               // move it to the front of the list
 5,688,864 ( 0.16%)      if (isFull && !adjustFullness()) {
     7,328 ( 0.00%)          if (!bin)
         .                       bin = tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize);
         .                   bin->moveBlockToFront(this);
         .               }
 2,692,836 ( 0.08%)  }
         .           
         .           /* Restore the bump pointer for an empty block that is planned to use */
         .           void Block::restoreBumpPtr()
         .           {
         .               MALLOC_ASSERT( allocatedCount == 0, ASSERT_TEXT );
         .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
         .               STAT_increment(getThreadId(), getIndex(objectSize), freeRestoreBumpPtr);
 7,410,654 ( 0.21%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
 2,470,218 ( 0.07%)      freeList = nullptr;
 2,470,218 ( 0.07%)      isFull = false;
 2,470,218 ( 0.07%)  }
         .           
         .           void Block::freeOwnObject(void *object)
         .           {
         .               tlsPtr.load(std::memory_order_relaxed)->markUsed();
         .               allocatedCount--;
         .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
         .           #if COLLECT_STATISTICS
         .               // Note that getAllocationBin is not called on the hottest path with statistics off.
-- line 1376 ----------------------------------------
-- line 1377 ----------------------------------------
         .               if (tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->getActiveBlock() != this)
         .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToInactiveBlock);
         .               else
         .                   STAT_increment(getThreadId(), getIndex(objectSize), freeToActiveBlock);
         .           #endif
         .               if (empty()) {
         .                   // If the last object of a slab is freed, the slab cannot be marked full
         .                   MALLOC_ASSERT(!isFull, ASSERT_TEXT);
 4,940,870 ( 0.14%)          tlsPtr.load(std::memory_order_relaxed)->getAllocationBin(objectSize)->processEmptyBlock(this, /*poolTheBlock=*/true);
         .               } else { // hot path
         .                   FreeObject *objectToFree = findObjectToFree(object);
 5,392,440 ( 0.15%)          objectToFree->next = freeList;
 2,696,220 ( 0.08%)          freeList = objectToFree;
 8,088,660 ( 0.23%)          adjustPositionInBin();
13,895,542 ( 0.39%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::Block::adjustPositionInBin(rml::internal::Bin*) (2,696,220x)
         .               }
         .           }
         .           
         .           void Block::freePublicObject (FreeObject *objectToFree)
         .           {
         .               FreeObject* localPublicFreeList{};
         .           
         .               MALLOC_ITT_SYNC_RELEASING(&publicFreeList);
-- line 1398 ----------------------------------------
-- line 1524 ----------------------------------------
         .           
         .           void Block::shareOrphaned(intptr_t binTag, unsigned index)
         .           {
         .               MALLOC_ASSERT( binTag, ASSERT_TEXT );
         .               // unreferenced formal parameter warning
         .               tbb::detail::suppress_unused_warning(index);
         .               STAT_increment(getThreadId(), index, freeBlockPublic);
         .               markOrphaned();
         6 ( 0.00%)      if ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
         .                   // First check passed: the block is not in mailbox yet.
         .                   // Need to set publicFreeList to non-zero, so other threads
         .                   // will not change nextPrivatizable and it can be zeroed.
         6 ( 0.00%)          if ( !readyToShare() ) {
         .                       // another thread freed an object; we need to wait until it finishes.
         .                       // There is no need for exponential backoff, as the wait here is not for a lock;
         .                       // but need to yield, so the thread we wait has a chance to run.
         .                       // TODO: add a pause to also be friendly to hyperthreads
         .                       int count = 256;
         .                       while ((intptr_t)nextPrivatizable.load(std::memory_order_relaxed) == binTag) {
         .                           if (--count==0) {
         .                               do_yield();
         .                               count = 256;
         .                           }
         .                       }
         .                   }
         .               }
         .               MALLOC_ASSERT( publicFreeList.load(std::memory_order_relaxed) !=nullptr, ASSERT_TEXT );
         .               // now it is safe to change our data
         3 ( 0.00%)      previous = nullptr;
         .               // it is caller responsibility to ensure that the list of blocks
         .               // formed by nextPrivatizable pointers is kept consistent if required.
         .               // if only called from thread shutdown code, it does not matter.
         .               nextPrivatizable.store((Block*)UNUSABLE, std::memory_order_relaxed);
         .           }
         .           
         .           void Block::cleanBlockHeader()
         .           {
       483 ( 0.00%)      next = nullptr;
       483 ( 0.00%)      previous = nullptr;
       484 ( 0.00%)      freeList = nullptr;
       967 ( 0.00%)      allocatedCount = 0;
       483 ( 0.00%)      isFull = false;
         .               tlsPtr.store(nullptr, std::memory_order_relaxed);
         .           
         .               publicFreeList.store(nullptr, std::memory_order_relaxed);
         .           }
         .           
         .           void Block::initEmptyBlock(TLSData *tls, size_t size)
         .           {
         .               // Having getIndex and getObjectSize called next to each other
         .               // allows better compiler optimization as they basically share the code.
         .               unsigned int index = getIndex(size);
         .               unsigned int objSz = getObjectSize(size);
         .           
         .               cleanBlockHeader();
       546 ( 0.00%)      objectSize = objSz;
         .               markOwned(tls);
         .               // bump pointer should be prepared for first allocation - thus mode it down to objectSize
     1,309 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize - objectSize);
         .           
         .               // each block should have the address where the head of the list of "privatizable" blocks is kept
         .               // the only exception is a block for boot strap which is initialized when TLS is yet nullptr
     1,212 ( 0.00%)      nextPrivatizable.store( tls? (Block*)(tls->bin + index) : nullptr, std::memory_order_relaxed);
         .               TRACEF(( "[ScalableMalloc trace] Empty block %p is initialized, owner is %ld, objectSize is %d, bumpPtr is %p\n",
         .                        this, tlsPtr.load(std::memory_order_relaxed) ? getThreadId() : -1, objectSize, bumpPtr ));
         .           }
         .           
         .           Block *OrphanedBlocks::get(TLSData *tls, unsigned int size)
     2,178 ( 0.00%)  {
         .               // TODO: try to use index from getAllocationBin
         .               unsigned int index = getIndex(size);
       968 ( 0.00%)      Block *block = bins[index].pop();
         .               if (block) {
         .                   MALLOC_ITT_SYNC_ACQUIRED(bins+index);
         .                   block->privatizeOrphaned(tls, index);
         .               }
         .               return block;
     2,178 ( 0.00%)  }
         .           
         .           void OrphanedBlocks::put(intptr_t binTag, Block *block)
        21 ( 0.00%)  {
         .               unsigned int index = getIndex(block->getSize());
         .               block->shareOrphaned(binTag, index);
        15 ( 0.00%)      MALLOC_ITT_SYNC_RELEASING(bins+index);
         .               bins[index].push(block);
        24 ( 0.00%)  }
         .           
         .           void OrphanedBlocks::reset()
         .           {
         .               for (uint32_t i=0; i<numBlockBinLimit; i++)
         .                   new (bins+i) LifoList();
         .           }
         .           
         .           bool OrphanedBlocks::cleanup(Backend* backend)
-- line 1617 ----------------------------------------
-- line 1639 ----------------------------------------
         .               }
         .               return released;
         .           }
         .           
         .           FreeBlockPool::ResOfGet FreeBlockPool::getBlock()
         .           {
         .               Block *b = head.exchange(nullptr);
         .           
       484 ( 0.00%)      if (b) {
       149 ( 0.00%)          size--;
       149 ( 0.00%)          Block *newHead = b->next;
       149 ( 0.00%)          lastAccessMiss = false;
         .                   head.store(newHead, std::memory_order_release);
         .               } else {
        93 ( 0.00%)          lastAccessMiss = true;
         .               }
         .               return ResOfGet(b, lastAccessMiss);
         .           }
         .           
         .           void FreeBlockPool::returnBlock(Block *block)
     2,170 ( 0.00%)  {
         .               MALLOC_ASSERT( size <= POOL_HIGH_MARK, ASSERT_TEXT );
         .               Block *localHead = head.exchange(nullptr);
         .           
       930 ( 0.00%)      if (!localHead) {
         .                   size = 0; // head was stolen by externalClean, correct size accordingly
       639 ( 0.00%)      } else if (size == POOL_HIGH_MARK) {
         .                   // release cold blocks and add hot one,
         .                   // so keep POOL_LOW_MARK-1 blocks and add new block to head
         .                   Block *headToFree = localHead, *helper;
         .                   for (int i=0; i<POOL_LOW_MARK-2; i++)
        36 ( 0.00%)              headToFree = headToFree->next;
         .                   Block *last = headToFree;
         6 ( 0.00%)          headToFree = headToFree->next;
         6 ( 0.00%)          last->next = nullptr;
         6 ( 0.00%)          size = POOL_LOW_MARK-1;
       318 ( 0.00%)          for (Block *currBl = headToFree; currBl; currBl = helper) {
       150 ( 0.00%)              helper = currBl->next;
         .                       // slab blocks in user's pools do not have valid backRefIdx
       600 ( 0.00%)              if (!backend->inUserPool())
       450 ( 0.00%)                  removeBackRef(currBl->backRefIdx);
     6,150 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (150x)
       450 ( 0.00%)              backend->putSlabBlock(currBl);
    32,739 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::putSlabBlock(rml::internal::BlockI*) (150x)
         .                   }
         .               }
       541 ( 0.00%)      size++;
       310 ( 0.00%)      block->next = localHead;
         .               head.store(block, std::memory_order_release);
     1,860 ( 0.00%)  }
         .           
         .           bool FreeBlockPool::externalCleanup()
         5 ( 0.00%)  {
         .               Block *helper;
         .               bool released = false;
         .           
        26 ( 0.00%)      for (Block *currBl=head.exchange(nullptr); currBl; currBl=helper) {
        11 ( 0.00%)          helper = currBl->next;
         .                   // slab blocks in user's pools do not have valid backRefIdx
        44 ( 0.00%)          if (!backend->inUserPool())
        33 ( 0.00%)              removeBackRef(currBl->backRefIdx);
       451 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::removeBackRef(rml::internal::BackRefIdx) (11x)
        33 ( 0.00%)          backend->putSlabBlock(currBl);
         1 ( 0.00%)          released = true;
         .               }
         .               return released;
         6 ( 0.00%)  }
         .           
         .           /* Prepare the block for returning to FreeBlockPool */
         .           void Block::reset()
         .           {
         .               // it is caller's responsibility to ensure no data is lost before calling this
         .               MALLOC_ASSERT( allocatedCount==0, ASSERT_TEXT );
         .               MALLOC_ASSERT( !isSolidPtr(publicFreeList.load(std::memory_order_relaxed)), ASSERT_TEXT );
         .               if (!isStartupAllocObject())
         .                   STAT_increment(getThreadId(), getIndex(objectSize), freeBlockBack);
         .           
         .               cleanBlockHeader();
         .           
         .               nextPrivatizable.store(nullptr, std::memory_order_relaxed);
         .           
       478 ( 0.00%)      objectSize = 0;
         .               // for an empty block, bump pointer should point right after the end of the block
       478 ( 0.00%)      bumpPtr = (FreeObject *)((uintptr_t)this + slabSize);
         .           }
         .           
         .           inline void Bin::setActiveBlock (Block *block)
         .           {
         .           //    MALLOC_ASSERT( bin, ASSERT_TEXT );
         .               MALLOC_ASSERT( block->isOwnedByCurrentThread(), ASSERT_TEXT );
         .               // it is the caller responsibility to keep bin consistence (i.e. ensure this block is in the bin list)
       242 ( 0.00%)      activeBlk = block;
         .           }
         .           
         .           inline Block* Bin::setPreviousBlockActive()
         .           {
         .               MALLOC_ASSERT( activeBlk, ASSERT_TEXT );
     7,328 ( 0.00%)      Block* temp = activeBlk->previous;
     7,328 ( 0.00%)      if( temp ) {
         .                   MALLOC_ASSERT( !(temp->isFull), ASSERT_TEXT );
     3,447 ( 0.00%)          activeBlk = temp;
         .               }
         .               return temp;
         .           }
         .           
         .           inline bool Block::isOwnedByCurrentThread() const {
20,666,620 ( 0.58%)      return tlsPtr.load(std::memory_order_relaxed) && ownerTid.isCurrentThreadId();
         .           }
         .           
         .           FreeObject *Block::findObjectToFree(const void *object) const
         .           {
         .               FreeObject *objectToFree;
         .               // Due to aligned allocations, a pointer passed to scalable_free
         .               // might differ from the address of internally allocated object.
         .               // Small objects however should always be fine.
 8,088,660 ( 0.23%)      if (objectSize <= maxSegregatedObjectSize)
         .                   objectToFree = (FreeObject*)object;
         .               // "Fitting size" allocations are suspicious if aligned higher than naturally
         .               else {
        74 ( 0.00%)          if ( ! isAligned(object,2*fittingAlignment) )
         .                       // TODO: the above check is questionable - it gives false negatives in ~50% cases,
         .                       //       so might even be slower in average than unconditional use of findAllocatedObject.
         .                       // here it should be a "real" object
         .                       objectToFree = (FreeObject*)object;
         .                   else
         .                       // here object can be an aligned address, so applying additional checks
         .                       objectToFree = findAllocatedObject(object);
         .                   MALLOC_ASSERT( isAligned(objectToFree,fittingAlignment), ASSERT_TEXT );
-- line 1763 ----------------------------------------
-- line 1764 ----------------------------------------
         .               }
         .               MALLOC_ASSERT( isProperlyPlaced(objectToFree), ASSERT_TEXT );
         .           
         .               return objectToFree;
         .           }
         .           
         .           void TLSData::release()
         .           {
         1 ( 0.00%)      memPool->extMemPool.allLocalCaches.unregisterThread(this);
         .               externalCleanup(/*cleanOnlyUnused=*/false, /*cleanBins=*/false);
         .           
        90 ( 0.00%)      for (unsigned index = 0; index < numBlockBins; index++) {
         .                   Block *activeBlk = bin[index].getActiveBlock();
        58 ( 0.00%)          if (!activeBlk)
         .                       continue;
        25 ( 0.00%)          Block *threadlessBlock = activeBlk->previous;
        25 ( 0.00%)          bool syncOnMailbox = false;
        75 ( 0.00%)          while (threadlessBlock) {
         .                       Block *threadBlock = threadlessBlock->previous;
         .                       if (threadlessBlock->empty()) {
         .                           /* we destroy the thread, so not use its block pool */
         .                           memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
         .                       } else {
         .                           memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
         .                           syncOnMailbox = true;
         .                       }
         .                       threadlessBlock = threadBlock;
         .                   }
         .                   threadlessBlock = activeBlk;
        50 ( 0.00%)          while (threadlessBlock) {
        25 ( 0.00%)              Block *threadBlock = threadlessBlock->next;
         .                       if (threadlessBlock->empty()) {
         .                           /* we destroy the thread, so not use its block pool */
        66 ( 0.00%)                  memPool->returnEmptyBlock(threadlessBlock, /*poolTheBlock=*/false);
     8,709 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (22x)
         .                       } else {
        37 ( 0.00%)                  memPool->extMemPool.orphanedBlocks.put(intptr_t(bin+index), threadlessBlock);
       176 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::put(long, rml::internal::Block*) (3x)
         4 ( 0.00%)                  syncOnMailbox = true;
         .                       }
         .                       threadlessBlock = threadBlock;
         .                   }
         .                   bin[index].resetActiveBlock();
         .           
        50 ( 0.00%)          if (syncOnMailbox) {
         .                       // Although, we synchronized on nextPrivatizable inside a block, we still need to
         .                       // synchronize on the bin lifetime because the thread releasing an object into the public 
         .                       // free list is touching the bin (mailbox and mailLock)
         3 ( 0.00%)              MallocMutex::scoped_lock scoped_cs(bin[index].mailLock);
         .                   }
         .               }
         .           }
         .           
         .           
         .           #if MALLOC_CHECK_RECURSION
         .           // TODO: Use dedicated heap for this
         .           
-- line 1818 ----------------------------------------
-- line 1824 ----------------------------------------
         .            * allocations are performed by moving bump pointer and increasing of object counter,
         .            * releasing is done via counter of objects allocated in the block
         .            * or moving bump pointer if releasing object is on a bound.
         .            * TODO: make bump pointer to grow to the same backward direction as all the others.
         .            */
         .           
         .           class StartupBlock : public Block {
         .               size_t availableSize() const {
        24 ( 0.00%)          return slabSize - ((uintptr_t)bumpPtr - (uintptr_t)this);
         .               }
         .               static StartupBlock *getBlock();
         .           public:
         .               static FreeObject *allocate(size_t size);
         .               static size_t msize(void *ptr) { return *((size_t*)ptr - 1); }
         .               void free(void *ptr);
         .           };
         .           
         .           static MallocMutex startupMallocLock;
         .           static StartupBlock *firstStartupBlock;
         .           
         .           StartupBlock *StartupBlock::getBlock()
         .           {
         3 ( 0.00%)      BackRefIdx backRefIdx = BackRefIdx::newBackRef(/*largeObj=*/false);
        72 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefIdx::newBackRef(bool) (1x)
         2 ( 0.00%)      if (backRefIdx.isInvalid()) return nullptr;
         .           
         .               StartupBlock *block = static_cast<StartupBlock*>(
         2 ( 0.00%)          defaultMemPool->extMemPool.backend.getSlabBlock(1));
         2 ( 0.00%)      if (!block) return nullptr;
         .           
         .               block->cleanBlockHeader();
         .               setBackRef(backRefIdx, block);
         4 ( 0.00%)      block->backRefIdx = backRefIdx;
         .               // use startupAllocObjSizeMark to mark objects from startup block marker
         2 ( 0.00%)      block->objectSize = startupAllocObjSizeMark;
         3 ( 0.00%)      block->bumpPtr = (FreeObject *)((uintptr_t)block + sizeof(StartupBlock));
         .               return block;
         .           }
         .           
         .           FreeObject *StartupBlock::allocate(size_t size)
         .           {
         .               FreeObject *result;
         .               StartupBlock *newBlock = nullptr;
         .               bool newBlockUnused = false;
         .           
         .               /* Objects must be aligned on their natural bounds,
         .                  and objects bigger than word on word's bound. */
         .               size = alignUp(size, sizeof(size_t));
         .               // We need size of an object to implement msize.
         7 ( 0.00%)      size_t reqSize = size + sizeof(size_t);
         .               {
         .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
         .                   // Re-check whether we need a new block (conditions might have changed)
        33 ( 0.00%)          if (!firstStartupBlock || firstStartupBlock->availableSize() < reqSize) {
         .                       if (!newBlock) {
         .                           newBlock = StartupBlock::getBlock();
         .                           if (!newBlock) return nullptr;
         .                       }
         2 ( 0.00%)              newBlock->next = (Block*)firstStartupBlock;
         2 ( 0.00%)              if (firstStartupBlock)
         .                           firstStartupBlock->previous = (Block*)newBlock;
         1 ( 0.00%)              firstStartupBlock = newBlock;
         .                   }
         .                   result = firstStartupBlock->bumpPtr;
         7 ( 0.00%)          firstStartupBlock->allocatedCount++;
         .                   firstStartupBlock->bumpPtr =
        14 ( 0.00%)              (FreeObject *)((uintptr_t)firstStartupBlock->bumpPtr + reqSize);
         .               }
         .           
         .               // keep object size at the negative offset
         7 ( 0.00%)      *((size_t*)result) = size;
         7 ( 0.00%)      return (FreeObject*)((size_t*)result+1);
         .           }
         .           
         .           void StartupBlock::free(void *ptr)
         .           {
         .               Block* blockToRelease = nullptr;
         .               {
         .                   MallocMutex::scoped_lock scoped_cs(startupMallocLock);
         .           
         .                   MALLOC_ASSERT(firstStartupBlock, ASSERT_TEXT);
         .                   MALLOC_ASSERT(startupAllocObjSizeMark==objectSize
         .                                 && allocatedCount>0, ASSERT_TEXT);
         .                   MALLOC_ASSERT((uintptr_t)ptr>=(uintptr_t)this+sizeof(StartupBlock)
         .                                 && (uintptr_t)ptr+StartupBlock::msize(ptr)<=(uintptr_t)this+slabSize,
         .                                 ASSERT_TEXT);
        12 ( 0.00%)          if (0 == --allocatedCount) {
         .                       if (this == firstStartupBlock)
         .                           firstStartupBlock = (StartupBlock*)firstStartupBlock->next;
         .                       if (previous)
         .                           previous->next = next;
         .                       if (next)
         .                           next->previous = previous;
         .                       blockToRelease = this;
        24 ( 0.00%)          } else if ((uintptr_t)ptr + StartupBlock::msize(ptr) == (uintptr_t)bumpPtr) {
         .                       // last object in the block released
         8 ( 0.00%)              FreeObject *newBump = (FreeObject*)((size_t*)ptr - 1);
         .                       MALLOC_ASSERT((uintptr_t)newBump>(uintptr_t)this+sizeof(StartupBlock),
         .                                     ASSERT_TEXT);
         .                       bumpPtr = newBump;
         .                   }
         .               }
         .               if (blockToRelease) {
         .                   blockToRelease->previous = blockToRelease->next = nullptr;
       217 ( 0.00%)          defaultMemPool->returnEmptyBlock(blockToRelease, /*poolTheBlock=*/false);
    56,470 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::returnEmptyBlock(rml::internal::Block*, bool) (217x)
         .               }
         .           }
         .           
         .           #endif /* MALLOC_CHECK_RECURSION */
         .           
         .           /********* End thread related code  *************/
         .           
         .           /********* Library initialization *************/
-- line 1935 ----------------------------------------
-- line 1997 ----------------------------------------
         .           
         .           inline bool isMallocInitialized() {
         .               // Load must have acquire fence; otherwise thread taking "initialized" path
         .               // might perform textually later loads *before* mallocInitialized becomes 2.
         .               return 2 == mallocInitialized.load(std::memory_order_acquire);
         .           }
         .           
         .           /* Caller is responsible for ensuring this routine is called exactly once. */
         6 ( 0.00%)  extern "C" void MallocInitializeITT() {
         .           #if __TBB_USE_ITT_NOTIFY
         .               if (!usedBySrcIncluded)
         .                   tbb::detail::r1::__TBB_load_ittnotify();
         .           #endif
         6 ( 0.00%)  }
         .           
         .           void MemoryPool::initDefaultPool() {
         .               hugePages.init();
         .           }
         .           
         .           /*
         .            * Allocator initialization routine;
         .            * it is called lazily on the very first scalable_malloc call.
-- line 2018 ----------------------------------------
-- line 2025 ----------------------------------------
         .               MALLOC_ASSERT( sizeof(FreeObject) == sizeof(void*), ASSERT_TEXT );
         .               MALLOC_ASSERT( isAligned(defaultMemPool, sizeof(intptr_t)),
         .                              "Memory pool must be void*-aligned for atomic to work over aligned arguments.");
         .           
         .           #if USE_WINTHREAD
         .               const size_t granularity = 64*1024; // granulatity of VirtualAlloc
         .           #else
         .               // POSIX.1-2001-compliant way to get page size
         7 ( 0.00%)      const size_t granularity = sysconf(_SC_PAGESIZE);
       829 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         .           #endif
         3 ( 0.00%)      if (!defaultMemPool) {
         .                   // Do not rely on static constructors and do the assignment in case
         .                   // of library static section not initialized at this call yet.
         .                   defaultMemPool = (MemoryPool*)defaultMemPool_space;
         .               }
         .               bool initOk = defaultMemPool->
         1 ( 0.00%)          extMemPool.init(0, nullptr, nullptr, granularity,
         .                                   /*keepAllMemory=*/false, /*fixedPool=*/false);
         .           // TODO: extMemPool.init() to not allocate memory
         2 ( 0.00%)      if (!initOk || !initBackRefMain(&defaultMemPool->extMemPool.backend) || !ThreadId::init())
         .                   return false;
         .               MemoryPool::initDefaultPool();
         .               // init() is required iff initMemoryManager() is called
         .               // after mallocProcessShutdownNotification()
         .               shutdownSync.init();
         .           #if COLLECT_STATISTICS
         .               initStatisticsCollection();
         .           #endif
-- line 2052 ----------------------------------------
-- line 2056 ----------------------------------------
         .           static bool GetBoolEnvironmentVariable(const char* name) {
         .               return tbb::detail::r1::GetBoolEnvironmentVariable(name);
         .           }
         .           
         .           //! Ensures that initMemoryManager() is called once and only once.
         .           /** Does not return until initMemoryManager() has been completed by a thread.
         .               There is no need to call this routine if mallocInitialized==2 . */
         .           static bool doInitialization()
         8 ( 0.00%)  {
         .               MallocMutex::scoped_lock lock( initMutex );
         2 ( 0.00%)      if (mallocInitialized.load(std::memory_order_relaxed)!=2) {
         .                   MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==0, ASSERT_TEXT );
         .                   mallocInitialized.store(1, std::memory_order_relaxed);
         .                   RecursiveMallocCallProtector scoped;
         .                   if (!initMemoryManager()) {
         .                       mallocInitialized.store(0, std::memory_order_relaxed); // restore and out
         .                       return false;
         .                   }
         .           #ifdef  MALLOC_EXTRA_INITIALIZATION
-- line 2074 ----------------------------------------
-- line 2084 ----------------------------------------
         .                   mallocInitialized.store(2, std::memory_order_release);
         .                   if( GetBoolEnvironmentVariable("TBB_VERSION") ) {
         .                       fputs(VersionString+1,stderr);
         .                       hugePages.printStatus();
         .                   }
         .               }
         .               /* It can't be 0 or I would have initialized it */
         .               MALLOC_ASSERT( mallocInitialized.load(std::memory_order_relaxed)==2, ASSERT_TEXT );
         3 ( 0.00%)      return true;
         9 ( 0.00%)  }
         .           
         .           /********* End library initialization *************/
         .           
         .           /********* The malloc show begins     *************/
         .           
         .           
         .           FreeObject *Block::allocateFromFreeList()
         .           {
         .               FreeObject *result;
         .           
15,510,966 ( 0.43%)      if (!freeList) return nullptr;
         .           
         .               result = freeList;
         .               MALLOC_ASSERT( result, ASSERT_TEXT );
         .           
 5,333,680 ( 0.15%)      freeList = result->next;
         .               MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
 2,666,840 ( 0.07%)      allocatedCount++;
         .               STAT_increment(getThreadId(), getIndex(objectSize), allocFreeListUsed);
         .           
         .               return result;
         .           }
         .           
         .           FreeObject *Block::allocateFromBumpPtr()
         .           {
 2,503,482 ( 0.07%)      FreeObject *result = bumpPtr;
 5,006,964 ( 0.14%)      if (result) {
17,498,726 ( 0.49%)          bumpPtr = (FreeObject *) ((uintptr_t) bumpPtr - objectSize);
 2,499,818 ( 0.07%)          if ( (uintptr_t)bumpPtr < (uintptr_t)this+sizeof(Block) ) {
         .                       bumpPtr = nullptr;
         .                   }
         .                   MALLOC_ASSERT( allocatedCount < (slabSize-sizeof(Block))/objectSize, ASSERT_TEXT );
 2,499,818 ( 0.07%)          allocatedCount++;
         .                   STAT_increment(getThreadId(), getIndex(objectSize), allocBumpPtrUsed);
         .               }
 2,499,818 ( 0.07%)      return result;
         .           }
         .           
         .           inline FreeObject* Block::allocate()
         .           {
         .               MALLOC_ASSERT( isOwnedByCurrentThread(), ASSERT_TEXT );
         .           
         .               /* for better cache locality, first looking in the free list. */
         .               if ( FreeObject *result = allocateFromFreeList() ) {
-- line 2137 ----------------------------------------
-- line 2141 ----------------------------------------
         .           
         .               /* if free list is empty, try thread local bump pointer allocation. */
         .               if ( FreeObject *result = allocateFromBumpPtr() ) {
         .                   return result;
         .               }
         .               MALLOC_ASSERT( !bumpPtr, ASSERT_TEXT );
         .           
         .               /* the block is considered full. */
     3,664 ( 0.00%)      isFull = true;
         .               return nullptr;
         .           }
         .           
         .           size_t Block::findObjectSize(void *object) const
         .           {
         .               size_t blSize = getSize();
         .           #if MALLOC_CHECK_RECURSION
         .               // Currently, there is no aligned allocations from startup blocks,
-- line 2157 ----------------------------------------
-- line 2165 ----------------------------------------
         .                   blSize - ((uintptr_t)object - (uintptr_t)findObjectToFree(object));
         .               MALLOC_ASSERT(size>0 && size<minLargeObjectSize, ASSERT_TEXT);
         .               return size;
         .           }
         .           
         .           void Bin::moveBlockToFront(Block *block)
         .           {
         .               /* move the block to the front of the bin */
    10,992 ( 0.00%)      if (block == activeBlk) return;
         .               outofTLSBin(block);
         .               pushTLSBin(block);
         .           }
         .           
         .           void Bin::processEmptyBlock(Block *block, bool poolTheBlock)
         .           {
 7,411,305 ( 0.21%)      if (block != activeBlk) {
         .                   /* We are not using this block; return it to the pool */
         .                   outofTLSBin(block);
       651 ( 0.00%)          block->getMemPool()->returnEmptyBlock(block, poolTheBlock);
         .               } else {
         .                   /* all objects are free - let's restore the bump pointer */
         .                   block->restoreBumpPtr();
         .               }
         .           }
         .           
         .           template<int LOW_MARK, int HIGH_MARK>
         .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::put(LargeMemoryBlock *object, ExtMemoryPool *extMemPool)
         .           {
        79 ( 0.00%)      const size_t size = object->unalignedSize;
         .               // not spoil cache with too large object, that can cause its total cleanup
       158 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
         .                   return false;
         .               LargeMemoryBlock *localHead = head.exchange(nullptr);
         .           
        79 ( 0.00%)      object->prev = nullptr;
        79 ( 0.00%)      object->next = localHead;
       158 ( 0.00%)      if (localHead)
        78 ( 0.00%)          localHead->prev = object;
         .               else {
         .                   // those might not be cleaned during local cache stealing, correct them
         .                   totalSize = 0;
         .                   numOfBlocks = 0;
         1 ( 0.00%)          tail = object;
         .               }
         .               localHead = object;
       157 ( 0.00%)      totalSize += size;
       236 ( 0.00%)      numOfBlocks++;
         .               // must meet both size and number of cached objects constrains
       389 ( 0.00%)      if (totalSize > MAX_TOTAL_SIZE || numOfBlocks >= HIGH_MARK) {
         .                   // scanning from tail until meet conditions
       100 ( 0.00%)          while (totalSize > MAX_TOTAL_SIZE || numOfBlocks > LOW_MARK) {
        48 ( 0.00%)              totalSize -= tail->unalignedSize;
        48 ( 0.00%)              numOfBlocks--;
        48 ( 0.00%)              tail = tail->prev;
         .                   }
         3 ( 0.00%)          LargeMemoryBlock *headToRelease = tail->next;
         1 ( 0.00%)          tail->next = nullptr;
         .           
         .                   extMemPool->freeLargeObjectList(headToRelease);
         .               }
         .           
         .               head.store(localHead, std::memory_order_release);
         .               return true;
         .           }
         .           
         .           template<int LOW_MARK, int HIGH_MARK>
         .           LargeMemoryBlock *LocalLOCImpl<LOW_MARK, HIGH_MARK>::get(size_t size)
         .           {
         .               LargeMemoryBlock *localHead, *res = nullptr;
         .           
       160 ( 0.00%)      if (size > MAX_TOTAL_SIZE)
         .                   return nullptr;
         .           
         .               // TBB_REVAMP_TODO: review this line
       464 ( 0.00%)      if (!head.load(std::memory_order_acquire) || (localHead = head.exchange(nullptr)) == nullptr) {
         .                   // do not restore totalSize, numOfBlocks and tail at this point,
         .                   // as they are used only in put(), where they must be restored
         .                   return nullptr;
         .               }
         .           
       748 ( 0.00%)      for (LargeMemoryBlock *curr = localHead; curr; curr=curr->next) {
     1,668 ( 0.00%)          if (curr->unalignedSize == size) {
         .                       res = curr;
       503 ( 0.00%)              if (curr->next)
        79 ( 0.00%)                  curr->next->prev = curr->prev;
         .                       else
        14 ( 0.00%)                  tail = curr->prev;
        86 ( 0.00%)              if (curr != localHead)
        44 ( 0.00%)                  curr->prev->next = curr->next;
         .                       else
         .                           localHead = curr->next;
        43 ( 0.00%)              totalSize -= size;
        43 ( 0.00%)              numOfBlocks--;
         .                       break;
         .                   }
         .               }
         .           
         .               head.store(localHead, std::memory_order_release);
         .               return res;
         .           }
         .           
         .           template<int LOW_MARK, int HIGH_MARK>
         .           bool LocalLOCImpl<LOW_MARK, HIGH_MARK>::externalCleanup(ExtMemoryPool *extMemPool)
         .           {
         2 ( 0.00%)      if (LargeMemoryBlock *localHead = head.exchange(nullptr)) {
         .                   extMemPool->freeLargeObjectList(localHead);
         .                   return true;
         .               }
         .               return false;
         .           }
         .           
         .           void *MemoryPool::getFromLLOCache(TLSData* tls, size_t size, size_t alignment)
       880 ( 0.00%)  {
         .               LargeMemoryBlock *lmb = nullptr;
         .           
         .               size_t headersSize = sizeof(LargeMemoryBlock)+sizeof(LargeObjectHdr);
        80 ( 0.00%)      size_t allocationSize = LargeObjectCache::alignToBin(size+headersSize+alignment);
       160 ( 0.00%)      if (allocationSize < size) // allocationSize is wrapped around after alignToBin
         .                   return nullptr;
         .               MALLOC_ASSERT(allocationSize >= alignment, "Overflow must be checked before.");
         .           
       160 ( 0.00%)      if (tls) {
         .                   tls->markUsed();
         .                   lmb = tls->lloc.get(allocationSize);
         .               }
         .               if (!lmb)
         .                   lmb = extMemPool.mallocLargeObject(this, allocationSize);
         .           
         .               if (lmb) {
         .                   // doing shuffle we suppose that alignment offset guarantees
         .                   // that different cache lines are in use
         .                   MALLOC_ASSERT(alignment >= estimatedCacheLineSize, ASSERT_TEXT);
         .           
         .                   void *alignedArea = (void*)alignUp((uintptr_t)lmb+headersSize, alignment);
         .                   uintptr_t alignedRight =
       246 ( 0.00%)              alignDown((uintptr_t)lmb+lmb->unalignedSize - size, alignment);
         .                   // Has some room to shuffle object between cache lines?
         .                   // Note that alignedRight and alignedArea are aligned at alignment.
         .                   unsigned ptrDelta = alignedRight - (uintptr_t)alignedArea;
       320 ( 0.00%)          if (ptrDelta && tls) { // !tls is cold path
         .                       // for the hot path of alignment==estimatedCacheLineSize,
         .                       // allow compilers to use shift for division
         .                       // (since estimatedCacheLineSize is a power-of-2 constant)
       320 ( 0.00%)              unsigned numOfPossibleOffsets = alignment == estimatedCacheLineSize?
         .                             ptrDelta / estimatedCacheLineSize :
         .                             ptrDelta / alignment;
       240 ( 0.00%)              unsigned myCacheIdx = ++tls->currCacheIdx;
         .                       unsigned offset = myCacheIdx % numOfPossibleOffsets;
         .           
         .                       // Move object to a cache line with an offset that is different from
         .                       // previous allocation. This supposedly allows us to use cache
         .                       // associativity more efficiently.
       320 ( 0.00%)              alignedArea = (void*)((uintptr_t)alignedArea + offset*alignment);
         .                   }
         .                   MALLOC_ASSERT((uintptr_t)lmb+lmb->unalignedSize >=
         .                                 (uintptr_t)alignedArea+size, "Object doesn't fit the block.");
        80 ( 0.00%)          LargeObjectHdr *header = (LargeObjectHdr*)alignedArea-1;
        80 ( 0.00%)          header->memoryBlock = lmb;
       320 ( 0.00%)          header->backRefIdx = lmb->backRefIdx;
         .                   setBackRef(header->backRefIdx, header);
         .           
        80 ( 0.00%)          lmb->objectSize = size;
         .           
         .                   MALLOC_ASSERT( isLargeObject<unknownMem>(alignedArea), ASSERT_TEXT );
         .                   MALLOC_ASSERT( isAligned(alignedArea, alignment), ASSERT_TEXT );
         .           
         .                   return alignedArea;
         .               }
         .               return nullptr;
       640 ( 0.00%)  }
         .           
         .           void MemoryPool::putToLLOCache(TLSData *tls, void *object)
       237 ( 0.00%)  {
         .               LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
         .               // overwrite backRefIdx to simplify double free detection
       237 ( 0.00%)      header->backRefIdx = BackRefIdx();
         .           
       158 ( 0.00%)      if (tls) {
         .                   tls->markUsed();
        79 ( 0.00%)          if (tls->lloc.put(header->memoryBlock, &extMemPool))
         .                       return;
         .               }
         .               extMemPool.freeLargeObject(header->memoryBlock);
       316 ( 0.00%)  }
         .           
         .           /*
         .            * All aligned allocations fall into one of the following categories:
         .            *  1. if both request size and alignment are <= maxSegregatedObjectSize,
         .            *       we just align the size up, and request this amount, because for every size
         .            *       aligned to some power of 2, the allocated object is at least that aligned.
         .            * 2. for size<minLargeObjectSize, check if already guaranteed fittingAlignment is enough.
         .            * 3. if size+alignment<minLargeObjectSize, we take an object of fittingSizeN and align
-- line 2356 ----------------------------------------
-- line 2453 ----------------------------------------
         .               return 0 == ((uintptr_t)this + slabSize - (uintptr_t)object) % objectSize;
         .           }
         .           #endif
         .           
         .           /* Finds the real object inside the block */
         .           FreeObject *Block::findAllocatedObject(const void *address) const
         .           {
         .               // calculate offset from the end of the block space
        96 ( 0.00%)      uint16_t offset = (uintptr_t)this + slabSize - (uintptr_t)address;
         .               MALLOC_ASSERT( offset<=slabSize-sizeof(Block), ASSERT_TEXT );
         .               // find offset difference from a multiple of allocation size
        64 ( 0.00%)      offset %= objectSize;
         .               // and move the address down to where the real object starts.
        64 ( 0.00%)      return (FreeObject*)((uintptr_t)address - (offset? objectSize-offset: 0));
         .           }
         .           
         .           /*
         .            * Bad dereference caused by a foreign pointer is possible only here, not earlier in call chain.
         .            * Separate function isolates SEH code, as it has bad influence on compiler optimization.
         .            */
         .           static inline BackRefIdx safer_dereference (const BackRefIdx *ptr)
         .           {
-- line 2474 ----------------------------------------
-- line 2478 ----------------------------------------
         .           #endif
         .                   id = dereference(ptr);
         .           #if _MSC_VER
         .               } __except( GetExceptionCode() == EXCEPTION_ACCESS_VIOLATION?
         .                           EXCEPTION_EXECUTE_HANDLER : EXCEPTION_CONTINUE_SEARCH ) {
         .                   id = BackRefIdx();
         .               }
         .           #endif
25,833,305 ( 0.72%)      return id;
         .           }
         .           
         .           template<MemoryOrigin memOrigin>
         .           bool isLargeObject(void *object)
         .           {
10,897,183 ( 0.30%)      if (!isAligned(object, largeObjectAlignment))
 6,768,782 ( 0.19%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/tbbmalloc_internal.h:bool rml::internal::isLargeObject<(rml::internal::MemoryOrigin)1>(void*) [clone .part.0] (563,703x)
         .                   return false;
       167 ( 0.00%)      LargeObjectHdr *header = (LargeObjectHdr*)object - 1;
         .               BackRefIdx idx = (memOrigin == unknownMem) ?
         .                   safer_dereference(&header->backRefIdx) : dereference(&header->backRefIdx);
         .           
         .               return idx.isLargeObject()
         .                   // in valid LargeObjectHdr memoryBlock is not nullptr
       501 ( 0.00%)          && header->memoryBlock
         .                   // in valid LargeObjectHdr memoryBlock points somewhere before header
         .                   // TODO: more strict check
       334 ( 0.00%)          && (uintptr_t)header->memoryBlock < (uintptr_t)header
 1,128,308 ( 0.03%)          && getBackRef(idx) == header;
     2,442 ( 0.00%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (147x)
   563,703 ( 0.02%)  }
         .           
         .           static inline bool isSmallObject (void *ptr)
         .           {
         .               Block* expectedBlock = (Block*)alignDown(ptr, slabSize);
         .               const BackRefIdx* idx = expectedBlock->getBackRefIdx();
         .           
 5,166,661 ( 0.14%)      bool isSmall = expectedBlock == getBackRef(safer_dereference(idx));
92,999,898 ( 2.60%)  => /usr/include/c++/10/bits/atomic_base.h:rml::internal::getBackRef(rml::internal::BackRefIdx) (5,166,661x)
         .               if (isSmall)
         .                   expectedBlock->checkFreePrecond(ptr);
         .               return isSmall;
         .           }
         .           
         .           /**** Check if an object was allocated by scalable_malloc ****/
         .           static inline bool isRecognized (void* ptr)
         .           {
         .               return defaultMemPool->extMemPool.backend.ptrCanBeValid(ptr) &&
         .                   (isLargeObject<unknownMem>(ptr) || isSmallObject(ptr));
         .           }
         .           
         .           static inline void freeSmallObject(void *object)
30,999,966 ( 0.87%)  {
         .               /* mask low bits to get the block */
         .               Block *block = (Block *)alignDown(object, slabSize);
         .               block->checkFreePrecond(object);
         .           
         .           #if MALLOC_CHECK_RECURSION
10,333,322 ( 0.29%)      if (block->isStartupAllocObject()) {
         .                   ((StartupBlock *)block)->free(object);
         .                   return;
         .               }
         .           #endif
         .               if (block->isOwnedByCurrentThread()) {
         .                   block->freeOwnObject(object);
         .               } else { /* Slower path to add to the shared list, the allocatedCount is updated by the owner thread in malloc. */
         .                   FreeObject *objectToFree = block->findObjectToFree(object);
         .                   block->freePublicObject(objectToFree);
         .               }
28,303,529 ( 0.79%)  }
         .           
         .           static void *internalPoolMalloc(MemoryPool* memPool, size_t size)
46,500,642 ( 1.30%)  {
         .               Bin* bin;
         .               Block * mallocBlock;
         .           
10,333,476 ( 0.29%)      if (!memPool) return nullptr;
         .           
10,333,476 ( 0.29%)      if (!size) size = sizeof(size_t);
         .           
         .               TLSData *tls = memPool->getTLS(/*create=*/true);
         .           
         .               /* Allocate a large object */
10,333,476 ( 0.29%)      if (size >= minLargeObjectSize)
       400 ( 0.00%)          return memPool->getFromLLOCache(tls, size, largeObjectAlignment);
    35,827 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getFromLLOCache(rml::internal::TLSData*, unsigned long, unsigned long) (80x)
         .           
         2 ( 0.00%)      if (!tls) return nullptr;
         .           
         .               tls->markUsed();
         .               /*
         .                * Get an element in thread-local array corresponding to the given size;
         .                * It keeps ptr to the active block for allocations of this size
         .                */
         .               bin = tls->getAllocationBin(size);
         .               if ( !bin ) return nullptr;
         .           
         .               /* Get a block to try to allocate in. */
15,503,396 ( 0.43%)      for( mallocBlock = bin->getActiveBlock(); mallocBlock;
         .                    mallocBlock = bin->setPreviousBlockActive() ) // the previous block should be empty enough
         .               {
         .                   if( FreeObject *result = mallocBlock->allocate() )
         .                       return result;
         .               }
         .           
         .               /*
         .                * else privatize publicly freed objects in some block and allocate from it
-- line 2579 ----------------------------------------
-- line 2586 ----------------------------------------
         .                   /* Else something strange happened, need to retry from the beginning; */
         .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in public free list; reentering.\n" ));
         .                   return internalPoolMalloc(memPool, size);
         .               }
         .           
         .               /*
         .                * no suitable own blocks, try to get a partial block that some other thread has discarded.
         .                */
     1,452 ( 0.00%)      mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
     8,912 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::OrphanedBlocks::get(rml::internal::TLSData*, unsigned int) (242x)
       484 ( 0.00%)      while (mallocBlock) {
         .                   bin->pushTLSBin(mallocBlock);
         .                   bin->setActiveBlock(mallocBlock); // TODO: move under the below condition?
         .                   if( FreeObject *result = mallocBlock->allocate() )
         .                       return result;
         .                   mallocBlock = memPool->extMemPool.orphanedBlocks.get(tls, size);
         .               }
         .           
         .               /*
         .                * else try to get a new empty block
         .                */
       726 ( 0.00%)      mallocBlock = memPool->getEmptyBlock(size);
    84,794 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::getEmptyBlock(unsigned long) (242x)
       484 ( 0.00%)      if (mallocBlock) {
         .                   bin->pushTLSBin(mallocBlock);
         .                   bin->setActiveBlock(mallocBlock);
         .                   if( FreeObject *result = mallocBlock->allocate() )
         .                       return result;
         .                   /* Else something strange happened, need to retry from the beginning; */
         .                   TRACEF(( "[ScalableMalloc trace] Something is wrong: no objects in empty block; reentering.\n" ));
         .                   return internalPoolMalloc(memPool, size);
         .               }
         .               /*
         .                * else nothing works so return nullptr
         .                */
         .               TRACEF(( "[ScalableMalloc trace] No memory found, returning nullptr.\n" ));
         .               return nullptr;
46,500,482 ( 1.30%)  }
         .           
         .           // When size==0 (i.e. unknown), detect here whether the object is large.
         .           // For size is known and < minLargeObjectSize, we still need to check
         .           // if the actual object is large, because large objects might be used
         .           // for aligned small allocations.
         .           static bool internalPoolFree(MemoryPool *memPool, void *object, size_t size)
         .           {
         .               if (!memPool || !object) return false;
-- line 2629 ----------------------------------------
-- line 2637 ----------------------------------------
         .               if (size >= minLargeObjectSize || isLargeObject<ourMem>(object))
         .                   memPool->putToLLOCache(memPool->getTLS(/*create=*/false), object);
         .               else
         .                   freeSmallObject(object);
         .               return true;
         .           }
         .           
         .           static void *internalMalloc(size_t size)
31,000,470 ( 0.87%)  {
15,500,235 ( 0.43%)      if (!size) size = sizeof(size_t);
         .           
         .           #if MALLOC_CHECK_RECURSION
         .               if (RecursiveMallocCallProtector::sameThreadActive())
        14 ( 0.00%)          return size<minLargeObjectSize? StartupBlock::allocate(size) :
         .                       // nested allocation, so skip tls
         .                       (FreeObject*)defaultMemPool->getFromLLOCache(nullptr, size, slabSize);
         .           #endif
         .           
10,333,476 ( 0.29%)      if (!isMallocInitialized())
         3 ( 0.00%)          if (!doInitialization())
   142,905 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::doInitialization() (1x)
         .                       return nullptr;
15,500,214 ( 0.43%)      return internalPoolMalloc(defaultMemPool, size);
412,697,586 (11.52%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalPoolMalloc(rml::internal::MemoryPool*, unsigned long) (5,166,738x)
25,833,732 ( 0.72%)  }
         .           
         .           static void internalFree(void *object)
         .           {
         .               internalPoolFree(defaultMemPool, object, 0);
         .           }
         .           
         .           static size_t internalMsize(void* ptr)
         .           {
-- line 2667 ----------------------------------------
-- line 2840 ----------------------------------------
         .            * unless that value is nullptr.
         .            * For Windows, it is called from DllMain( DLL_THREAD_DETACH ).
         .            *
         .            * However neither of the above is called for the main process thread, so the routine
         .            * also needs to be called during the process shutdown.
         .            *
         .           */
         .           // TODO: Consider making this function part of class MemoryPool.
         4 ( 0.00%)  void doThreadShutdownNotification(TLSData* tls, bool main_thread)
         .           {
         .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return start %d\n",
         .                        getThreadId(),  threadGoingDownCount++ ));
         .           
         .           #if USE_PTHREAD
         .               if (tls) {
         .                   if (!shutdownSync.threadDtorStart()) return;
         .                   tls->getMemPool()->onThreadShutdown(tls);
-- line 2856 ----------------------------------------
-- line 2858 ----------------------------------------
         .               } else
         .           #endif
         .               {
         .                   suppress_unused_warning(tls); // not used on Windows
         .                   // The default pool is safe to use at this point:
         .                   //   on Linux, only the main thread can go here before destroying defaultMemPool;
         .                   //   on Windows, shutdown is synchronized via loader lock and isMallocInitialized().
         .                   // See also __TBB_mallocProcessShutdownNotification()
         1 ( 0.00%)          defaultMemPool->onThreadShutdown(defaultMemPool->getTLS(/*create=*/false));
         .                   // Take lock to walk through other pools; but waiting might be dangerous at this point
         .                   // (e.g. on Windows the main thread might deadlock)
         .                   bool locked;
         .                   MallocMutex::scoped_lock lock(MemoryPool::memPoolListLock, /*wait=*/!main_thread, &locked);
         3 ( 0.00%)          if (locked) { // the list is safe to process
         4 ( 0.00%)              for (MemoryPool *memPool = defaultMemPool->next; memPool; memPool = memPool->next)
         .                           memPool->onThreadShutdown(memPool->getTLS(/*create=*/false));
         .                   }
         .               }
         .           
         .               TRACEF(( "[ScalableMalloc trace] Thread id %d blocks return end\n", getThreadId() ));
         4 ( 0.00%)  }
         .           
         .           #if USE_PTHREAD
         .           void mallocThreadShutdownNotification(void* arg)
         .           {
         .               // The routine is called for each pool (as TLS dtor) on each thread, except for the main thread
         .               if (!isMallocInitialized()) return;
         .               doThreadShutdownNotification((TLSData*)arg, false);
         .           }
-- line 2886 ----------------------------------------
-- line 2890 ----------------------------------------
         .               // The routine is called once per thread on Windows
         .               if (!isMallocInitialized()) return;
         .               doThreadShutdownNotification(nullptr, false);
         .           }
         .           #endif
         .           
         .           extern "C" void __TBB_mallocProcessShutdownNotification(bool windows_process_dying)
         .           {
         2 ( 0.00%)      if (!isMallocInitialized()) return;
         .           
         .               // Don't clean allocator internals if the entire process is exiting
         .               if (!windows_process_dying) {
         .                   doThreadShutdownNotification(nullptr, /*main_thread=*/true);
         .               }
         .           #if  __TBB_MALLOC_LOCACHE_STAT
         .               printf("cache hit ratio %f, size hit %f\n",
         .                      1.*cacheHits/mallocCalls, 1.*memHitKB/memAllocKB);
-- line 2906 ----------------------------------------
-- line 2925 ----------------------------------------
         .               for( int i=1; i<=nThreads && i<MAX_THREADS; ++i )
         .                   STAT_print(i);
         .           #endif
         .               if (!usedBySrcIncluded)
         .                   MALLOC_ITT_FINI_ITTLIB();
         .           }
         .           
         .           extern "C" void * scalable_malloc(size_t size)
 5,166,743 ( 0.14%)  {
10,333,486 ( 0.29%)      void *ptr = internalMalloc(size);
531,528,363 (14.84%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (5,166,737x)
     1,107 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long)'2 (6x)
10,333,486 ( 0.29%)      if (!ptr) errno = ENOMEM;
         .               return ptr;
15,500,229 ( 0.43%)  }
         .           
         .           extern "C" void scalable_free(void *object)
         .           {
         .               internalFree(object);
         .           }
         .           
         .           #if MALLOC_ZONE_OVERLOAD_ENABLED
         .           extern "C" void __TBB_malloc_free_definite_size(void *object, size_t size)
-- line 2945 ----------------------------------------
-- line 2948 ----------------------------------------
         .           }
         .           #endif
         .           
         .           /*
         .            * A variant that provides additional memory safety, by checking whether the given address
         .            * was obtained with this allocator, and if not redirecting to the provided alternative call.
         .            */
         .           extern "C" TBBMALLOC_EXPORT void __TBB_malloc_safer_free(void *object, void (*original_free)(void*))
 5,166,740 ( 0.14%)  {
10,333,480 ( 0.29%)      if (!object)
         .                   return;
         .           
         .               // tbbmalloc can allocate object only when tbbmalloc has been initialized
15,500,220 ( 0.43%)      if (mallocInitialized.load(std::memory_order_acquire) && defaultMemPool->extMemPool.backend.ptrCanBeValid(object)) {
 1,127,406 ( 0.03%)          if (isLargeObject<unknownMem>(object)) {
         .                       // must check 1st for large object, because small object check touches 4 pages on left,
         .                       // and it can be inaccessible
         .                       TLSData *tls = defaultMemPool->getTLS(/*create=*/false);
         .           
       237 ( 0.00%)              defaultMemPool->putToLLOCache(tls, object);
     6,898 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::MemoryPool::putToLLOCache(rml::internal::TLSData*, void*) (79x)
         .                       return;
10,333,322 ( 0.29%)          } else if (isSmallObject(object)) {
10,333,322 ( 0.29%)              freeSmallObject(object);
252,510,332 ( 7.05%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::freeSmallObject(void*) [clone .lto_priv.0] (5,166,661x)
         .                       return;
         .                   }
         .               }
         .               if (original_free)
         .                   original_free(object);
 5,166,740 ( 0.14%)  }
         .           
         .           /********* End the free code        *************/
         .           
         .           /********* Code for scalable_realloc       ***********/
         .           
         .           /*
         .            * From K&R
         .            * "realloc changes the size of the object pointed to by p to size. The contents will
-- line 2984 ----------------------------------------
-- line 3058 ----------------------------------------
         .            * From K&R
         .            * calloc returns a pointer to space for an array of nobj objects,
         .            * each of size size, or nullptr if the request cannot be satisfied.
         .            * The space is initialized to zero bytes.
         .            *
         .            */
         .           
         .           extern "C" void * scalable_calloc(size_t nobj, size_t size)
         6 ( 0.00%)  {
         .               // it's square root of maximal size_t value
         .               const size_t mult_not_overflow = size_t(1) << (sizeof(size_t)*CHAR_BIT/2);
         4 ( 0.00%)      const size_t arraySize = nobj * size;
         .           
         .               // check for overflow during multiplication:
        10 ( 0.00%)      if (nobj>=mult_not_overflow || size>=mult_not_overflow) // 1) heuristic check
         4 ( 0.00%)          if (nobj && arraySize / nobj != size) {             // 2) exact check
         .                       errno = ENOMEM;
         .                       return nullptr;
         .                   }
         6 ( 0.00%)      void* result = internalMalloc(arraySize);
   147,105 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:rml::internal::internalMalloc(unsigned long) (1x)
         4 ( 0.00%)      if (result)
        10 ( 0.00%)          memset(result, 0, arraySize);
        11 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (1x)
         .               else
         .                   errno = ENOMEM;
         .               return result;
        10 ( 0.00%)  }
         .           
         .           /********* End code for scalable_calloc   ***********/
         .           
         .           /********* Code for aligned allocation API **********/
         .           
         .           extern "C" int scalable_posix_memalign(void **memptr, size_t alignment, size_t size)
         .           {
         .               if ( !isPowerOfTwoAtLeast(alignment, sizeof(void*)) )
-- line 3091 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/atomic_base.h
--------------------------------------------------------------------------------
Ir                  

-- line 188 ----------------------------------------
         .               atomic_flag() noexcept = default;
         .               ~atomic_flag() noexcept = default;
         .               atomic_flag(const atomic_flag&) = delete;
         .               atomic_flag& operator=(const atomic_flag&) = delete;
         .               atomic_flag& operator=(const atomic_flag&) volatile = delete;
         .           
         .               // Conversion to ATOMIC_FLAG_INIT.
         .               constexpr atomic_flag(bool __i) noexcept
        31 ( 0.00%)        : __atomic_flag_base{ _S_init(__i) }
         .               { }
         .           
         .               _GLIBCXX_ALWAYS_INLINE bool
         .               test_and_set(memory_order __m = memory_order_seq_cst) noexcept
         .               {
     3,741 ( 0.00%)        return __atomic_test_and_set (&_M_i, int(__m));
         .               }
         .           
         .               _GLIBCXX_ALWAYS_INLINE bool
         .               test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept
         .               {
         .                 return __atomic_test_and_set (&_M_i, int(__m));
         .               }
         .           
-- line 210 ----------------------------------------
-- line 211 ----------------------------------------
         .               _GLIBCXX_ALWAYS_INLINE void
         .               clear(memory_order __m = memory_order_seq_cst) noexcept
         .               {
         .                 memory_order __b = __m & __memory_order_mask;
         .                 __glibcxx_assert(__b != memory_order_consume);
         .                 __glibcxx_assert(__b != memory_order_acquire);
         .                 __glibcxx_assert(__b != memory_order_acq_rel);
         .           
     1,121 ( 0.00%)        __atomic_clear (&_M_i, int(__m));
         .               }
         .           
         .               _GLIBCXX_ALWAYS_INLINE void
         .               clear(memory_order __m = memory_order_seq_cst) volatile noexcept
         .               {
         .                 memory_order __b = __m & __memory_order_mask;
         .                 __glibcxx_assert(__b != memory_order_consume);
         .                 __glibcxx_assert(__b != memory_order_acquire);
-- line 227 ----------------------------------------
-- line 278 ----------------------------------------
         .               public:
         .                 __atomic_base() noexcept = default;
         .                 ~__atomic_base() noexcept = default;
         .                 __atomic_base(const __atomic_base&) = delete;
         .                 __atomic_base& operator=(const __atomic_base&) = delete;
         .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
         .           
         .                 // Requires __int_type convertible to _M_i.
       251 ( 0.00%)        constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }
         .           
         .                 operator __int_type() const noexcept
         .                 { return load(); }
         .           
         .                 operator __int_type() const volatile noexcept
         .                 { return load(); }
         .           
         .                 __int_type
-- line 294 ----------------------------------------
-- line 396 ----------------------------------------
         .                 _GLIBCXX_ALWAYS_INLINE void
         .                 store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_acquire);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
         .           	__glibcxx_assert(__b != memory_order_consume);
         .           
10,336,410 ( 0.29%)  	__atomic_store_n(&_M_i, __i, int(__m));
        11 ( 0.00%)        }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE void
         .                 store(__int_type __i,
         .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_acquire);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 413 ----------------------------------------
-- line 418 ----------------------------------------
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 load(memory_order __m = memory_order_seq_cst) const noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_release);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
         .           
46,506,964 ( 1.30%)  	return __atomic_load_n(&_M_i, int(__m));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_release);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 434 ----------------------------------------
-- line 501 ----------------------------------------
         .           			      memory_order __m1, memory_order __m2) noexcept
         .                 {
         .           	memory_order __b2 = __m2 & __memory_order_mask;
         .           	memory_order __b1 = __m1 & __memory_order_mask;
         .           	__glibcxx_assert(__b2 != memory_order_release);
         .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
         .           	__glibcxx_assert(__b2 <= __b1);
         .           
     2,062 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0,
         .           					   int(__m1), int(__m2));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE bool
         .                 compare_exchange_strong(__int_type& __i1, __int_type __i2,
         .           			      memory_order __m1,
         .           			      memory_order __m2) volatile noexcept
         .                 {
-- line 517 ----------------------------------------
-- line 540 ----------------------------------------
         .                 {
         .           	return compare_exchange_strong(__i1, __i2, __m,
         .           				       __cmpexch_failure_order(__m));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_add(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) noexcept
     1,520 ( 0.00%)        { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_add(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 { return __atomic_fetch_add(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_sub(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) noexcept
       327 ( 0.00%)        { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_sub(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 { return __atomic_fetch_sub(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_and(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) noexcept
       399 ( 0.00%)        { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_and(__int_type __i,
         .           		memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 { return __atomic_fetch_and(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_or(__int_type __i,
         .           	       memory_order __m = memory_order_seq_cst) noexcept
       752 ( 0.00%)        { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_or(__int_type __i,
         .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 { return __atomic_fetch_or(&_M_i, __i, int(__m)); }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __int_type
         .                 fetch_xor(__int_type __i,
-- line 586 ----------------------------------------
-- line 613 ----------------------------------------
         .               public:
         .                 __atomic_base() noexcept = default;
         .                 ~__atomic_base() noexcept = default;
         .                 __atomic_base(const __atomic_base&) = delete;
         .                 __atomic_base& operator=(const __atomic_base&) = delete;
         .                 __atomic_base& operator=(const __atomic_base&) volatile = delete;
         .           
         .                 // Requires __pointer_type convertible to _M_p.
        50 ( 0.00%)        constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }
         .           
         .                 operator __pointer_type() const noexcept
         .                 { return load(); }
         .           
         .                 operator __pointer_type() const volatile noexcept
         .                 { return load(); }
         .           
         .                 __pointer_type
-- line 629 ----------------------------------------
-- line 717 ----------------------------------------
         .           	    memory_order __m = memory_order_seq_cst) noexcept
         .                 {
         .                   memory_order __b = __m & __memory_order_mask;
         .           
         .           	__glibcxx_assert(__b != memory_order_acquire);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
         .           	__glibcxx_assert(__b != memory_order_consume);
         .           
     4,607 ( 0.00%)  	__atomic_store_n(&_M_p, __p, int(__m));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE void
         .                 store(__pointer_type __p,
         .           	    memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_acquire);
-- line 733 ----------------------------------------
-- line 739 ----------------------------------------
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
         .                 load(memory_order __m = memory_order_seq_cst) const noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_release);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
         .           
64,479,469 ( 1.80%)  	return __atomic_load_n(&_M_p, int(__m));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
         .                 load(memory_order __m = memory_order_seq_cst) const volatile noexcept
         .                 {
         .           	memory_order __b = __m & __memory_order_mask;
         .           	__glibcxx_assert(__b != memory_order_release);
         .           	__glibcxx_assert(__b != memory_order_acq_rel);
-- line 755 ----------------------------------------
-- line 756 ----------------------------------------
         .           
         .           	return __atomic_load_n(&_M_p, int(__m));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
         .                 exchange(__pointer_type __p,
         .           	       memory_order __m = memory_order_seq_cst) noexcept
         .                 {
     1,796 ( 0.00%)  	return __atomic_exchange_n(&_M_p, __p, int(__m));
         .                 }
         .           
         .           
         .                 _GLIBCXX_ALWAYS_INLINE __pointer_type
         .                 exchange(__pointer_type __p,
         .           	       memory_order __m = memory_order_seq_cst) volatile noexcept
         .                 {
         .           	return __atomic_exchange_n(&_M_p, __p, int(__m));
-- line 772 ----------------------------------------
-- line 778 ----------------------------------------
         .           			      memory_order __m2) noexcept
         .                 {
         .           	memory_order __b2 = __m2 & __memory_order_mask;
         .           	memory_order __b1 = __m1 & __memory_order_mask;
         .           	__glibcxx_assert(__b2 != memory_order_release);
         .           	__glibcxx_assert(__b2 != memory_order_acq_rel);
         .           	__glibcxx_assert(__b2 <= __b1);
         .           
     1,033 ( 0.00%)  	return __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0,
         .           					   int(__m1), int(__m2));
         .                 }
         .           
         .                 _GLIBCXX_ALWAYS_INLINE bool
         .                 compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,
         .           			      memory_order __m1,
         .           			      memory_order __m2) volatile noexcept
         .                 {
-- line 794 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/por_space.hpp
--------------------------------------------------------------------------------
  No information has been collected for /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/por_space.hpp

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/hashtable.h
--------------------------------------------------------------------------------
Ir                 

-- line 268 ----------------------------------------
        .           	// Allocate a node and construct an element within it.
        .           	template<typename... _Args>
        .           	  _Scoped_node(__hashtable_alloc* __h, _Args&&... __args)
        .           	  : _M_h(__h),
        .           	    _M_node(__h->_M_allocate_node(std::forward<_Args>(__args)...))
        .           	  { }
        .           
        .           	// Destroy element and deallocate node.
  647,653 ( 0.02%)  	~_Scoped_node() { if (_M_node) _M_h->_M_deallocate_node(_M_node); };
        .           
        .           	_Scoped_node(const _Scoped_node&) = delete;
        .           	_Scoped_node& operator=(const _Scoped_node&) = delete;
        .           
        .           	__hashtable_alloc* _M_h;
        .           	__node_type* _M_node;
        .                 };
        .           
-- line 284 ----------------------------------------
-- line 372 ----------------------------------------
        .                 // which is not allocated so that we can have those operations noexcept
        .                 // qualified.
        .                 // Note that we can't leave hashtable with 0 bucket without adding
        .                 // numerous checks in the code to avoid 0 modulus.
        .                 __bucket_type		_M_single_bucket	= nullptr;
        .           
        .                 bool
        .                 _M_uses_single_bucket(__bucket_type* __bkts) const
        7 ( 0.00%)        { return __builtin_expect(__bkts == &_M_single_bucket, false); }
        .           
        .                 bool
        .                 _M_uses_single_bucket() const
        .                 { return _M_uses_single_bucket(_M_buckets); }
        .           
        .                 __hashtable_alloc&
        .                 _M_base_alloc() { return *this; }
        .           
        .                 __bucket_type*
        .                 _M_allocate_buckets(size_type __bkt_count)
        .                 {
      200 ( 0.00%)  	if (__builtin_expect(__bkt_count == 1, false))
        .           	  {
        .           	    _M_single_bucket = nullptr;
      100 ( 0.00%)  	    return &_M_single_bucket;
        .           	  }
        .           
        .           	return __hashtable_alloc::_M_allocate_buckets(__bkt_count);
        .                 }
        .           
        .                 void
        .                 _M_deallocate_buckets(__bucket_type* __bkts, size_type __bkt_count)
        .                 {
      214 ( 0.00%)  	if (_M_uses_single_bucket(__bkts))
        .           	  return;
        .           
        .           	__hashtable_alloc::_M_deallocate_buckets(__bkts, __bkt_count);
        .                 }
        .           
        .                 void
        .                 _M_deallocate_buckets()
      214 ( 0.00%)        { _M_deallocate_buckets(_M_buckets, _M_bucket_count); }
        .           
        .                 // Gets bucket begin, deals with the fact that non-empty buckets contain
        .                 // their before begin node.
        .                 __node_type*
        .                 _M_bucket_begin(size_type __bkt) const;
        .           
        .                 __node_type*
        .                 _M_begin() const
      618 ( 0.00%)        { return static_cast<__node_type*>(_M_before_begin._M_nxt); }
        .           
        .                 // Assign *this using another _Hashtable instance. Whether elements
        .                 // are copied or moved depends on the _Ht reference.
        .                 template<typename _Ht>
        .           	void
        .           	_M_assign_elements(_Ht&&);
        .           
        .                 template<typename _Ht, typename _NodeGenerator>
-- line 429 ----------------------------------------
-- line 443 ----------------------------------------
        .           		 const _Equal& __eq, const _ExtractKey& __exk,
        .           		 const allocator_type& __a)
        .                 : __hashtable_base(__exk, __h1, __h2, __h, __eq),
        .           	__hashtable_alloc(__node_alloc_type(__a))
        .                 { }
        .           
        .               public:
        .                 // Constructor, destructor, assignment, swap
       28 ( 0.00%)        _Hashtable() = default;
        .                 _Hashtable(size_type __bkt_count_hint,
        .           		 const _H1&, const _H2&, const _Hash&,
        .           		 const _Equal&, const _ExtractKey&,
        .           		 const allocator_type&);
        .           
        .                 template<typename _InputIterator>
        .           	_Hashtable(_InputIterator __first, _InputIterator __last,
        .           		   size_type __bkt_count_hint,
-- line 459 ----------------------------------------
-- line 674 ----------------------------------------
        .               protected:
        .                 // Bucket index computation helpers.
        .                 size_type
        .                 _M_bucket_index(__node_type* __n) const noexcept
        .                 { return __hash_code_base::_M_bucket_index(__n, _M_bucket_count); }
        .           
        .                 size_type
        .                 _M_bucket_index(const key_type& __k, __hash_code __c) const
1,692,391 ( 0.05%)        { return __hash_code_base::_M_bucket_index(__k, __c, _M_bucket_count); }
        .           
        .                 // Find and insert helper functions and types
        .                 // Find the node before the one matching the criteria.
        .                 __node_base*
        .                 _M_find_before_node(size_type, const key_type&, __hash_code) const;
        .           
        .                 __node_type*
        .                 _M_find_node(size_type __bkt, const key_type& __key,
-- line 690 ----------------------------------------
-- line 780 ----------------------------------------
        .                 iterator
        .                 _M_erase(size_type __bkt, __node_base* __prev_n, __node_type* __n);
        .           
        .               public:
        .                 // Emplace
        .                 template<typename... _Args>
        .           	__ireturn_type
        .           	emplace(_Args&&... __args)
2,593,103 ( 0.07%)  	{ return _M_emplace(__unique_keys(), std::forward<_Args>(__args)...); }
  735,994 ( 0.02%)  => /usr/include/c++/10/bits/hashtable.h:std::pair<std::__detail::_Node_iterator<std::pair<NP::JobID const, unsigned long>, false, true>, bool> std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_emplace<NP::JobID, unsigned long>(std::integral_constant<bool, true>, NP::JobID&&, unsigned long&&) [clone .constprop.0] (2,507x)
        .           
        .                 template<typename... _Args>
        .           	iterator
        .           	emplace_hint(const_iterator __hint, _Args&&... __args)
        .           	{
        .           	  return _M_emplace(__hint, __unique_keys(),
        .           			    std::forward<_Args>(__args)...);
        .           	}
-- line 796 ----------------------------------------
-- line 1375 ----------------------------------------
        .           	  __ht.clear();
        .           	}
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
       20 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               ~_Hashtable() noexcept
        .               {
        .                 clear();
        .                 _M_deallocate_buckets();
       16 ( 0.00%)      }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
-- line 1397 ----------------------------------------
-- line 1439 ----------------------------------------
        .           	  = &__x._M_before_begin;
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               auto
    5,014 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               find(const key_type& __k)
        .               -> iterator
        .               {
        .                 __hash_code __code = this->_M_hash_code(__k);
        .                 std::size_t __bkt = _M_bucket_index(__k, __code);
        .                 __node_type* __p = _M_find_node(__bkt, __k, __code);
        .                 return __p ? iterator(__p) : end();
-- line 1455 ----------------------------------------
-- line 1565 ----------------------------------------
        .           	   typename _Traits>
        .               auto
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               _M_find_before_node(size_type __bkt, const key_type& __k,
        .           			__hash_code __code) const
        .               -> __node_base*
        .               {
3,384,783 ( 0.09%)        __node_base* __prev_p = _M_buckets[__bkt];
3,384,782 ( 0.09%)        if (!__prev_p)
        .           	return nullptr;
        .           
1,179,280 ( 0.03%)        for (__node_type* __p = static_cast<__node_type*>(__prev_p->_M_nxt);;
        .           	   __p = __p->_M_next())
        .           	{
        .           	  if (this->_M_equals(__k, __code, __p))
        .           	    return __prev_p;
        .           
  738,419 ( 0.02%)  	  if (!__p->_M_nxt || _M_bucket_index(__p->_M_next()) != __bkt)
        .           	    break;
        .           	  __prev_p = __p;
        .           	}
        .                 return nullptr;
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               _M_insert_bucket_begin(size_type __bkt, __node_type* __node)
        .               {
3,250,797 ( 0.09%)        if (_M_buckets[__bkt])
        .           	{
        .           	  // Bucket is not empty, we just need to insert the new node
        .           	  // after the bucket before begin.
  277,012 ( 0.01%)  	  __node->_M_nxt = _M_buckets[__bkt]->_M_nxt;
  277,012 ( 0.01%)  	  _M_buckets[__bkt]->_M_nxt = __node;
        .           	}
        .                 else
        .           	{
        .           	  // The bucket is empty, the new node is inserted at the
        .           	  // beginning of the singly-linked list and the bucket will
        .           	  // contain _M_before_begin pointer.
1,023,306 ( 0.03%)  	  __node->_M_nxt = _M_before_begin._M_nxt;
  511,653 ( 0.01%)  	  _M_before_begin._M_nxt = __node;
1,534,958 ( 0.04%)  	  if (__node->_M_nxt)
        .           	    // We must update former begin bucket that is pointing to
        .           	    // _M_before_begin.
  511,139 ( 0.01%)  	    _M_buckets[_M_bucket_index(__node->_M_next())] = __node;
1,534,959 ( 0.04%)  	  _M_buckets[__bkt] = &_M_before_begin;
        .           	}
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
-- line 1625 ----------------------------------------
-- line 1659 ----------------------------------------
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               template<typename... _Args>
        .                 auto
5,181,208 ( 0.14%)        _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           		 _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .                 _M_emplace(true_type, _Args&&... __args)
        .                 -> pair<iterator, bool>
        .                 {
        .           	// First build the node to get access to the hash code
        .           	_Scoped_node __node { this, std::forward<_Args>(__args)...  };
        .           	const key_type& __k = this->_M_extract()(__node._M_node->_M_v());
        .           	__hash_code __code = this->_M_hash_code(__k);
        .           	size_type __bkt = _M_bucket_index(__k, __code);
        .           	if (__node_type* __p = _M_find_node(__bkt, __k, __code))
        .           	  // There is already an equivalent node, no insertion
        .           	  return std::make_pair(iterator(__p), false);
        .           
        .           	// Insert the node
4,551,106 ( 0.13%)  	auto __pos = _M_insert_unique_node(__k, __bkt, __code, __node._M_node);
  285,008 ( 0.01%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, Interval<long long> >, std::allocator<std::pair<NP::JobID const, Interval<long long> > >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_insert_unique_node(NP::JobID const&, unsigned long, unsigned long, std::__detail::_Hash_node<std::pair<NP::JobID const, Interval<long long> >, true>*, unsigned long) [clone .isra.0] (2,507x)
        .           	__node._M_node = nullptr;
  647,651 ( 0.02%)  	return { __pos, true };
5,181,208 ( 0.14%)        }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               template<typename... _Args>
        .                 auto
        .                 _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
-- line 1693 ----------------------------------------
-- line 1706 ----------------------------------------
        .           	return __pos;
        .                 }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               auto
6,501,580 ( 0.18%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               _M_insert_unique_node(const key_type& __k, size_type __bkt,
        .           			  __hash_code __code, __node_type* __node,
        .           			  size_type __n_elt)
        .               -> iterator
        .               {
1,300,318 ( 0.04%)        const __rehash_state& __saved_state = _M_rehash_policy._M_state();
        .                 std::pair<bool, std::size_t> __do_rehash
3,250,801 ( 0.09%)  	= _M_rehash_policy._M_need_rehash(_M_bucket_count, _M_element_count,
       89 ( 0.00%)  => ???:std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const (1x)
        .           					  __n_elt);
        .           
1,300,318 ( 0.04%)        if (__do_rehash.first)
        .           	{
      110 ( 0.00%)  	  _M_rehash(__do_rehash.second, __saved_state);
      182 ( 0.00%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<unsigned long, std::pair<unsigned long const, std::forward_list<std::_Deque_iterator<NP::Global::Schedule_state<long long>, NP::Global::Schedule_state<long long>&, NP::Global::Schedule_state<long long>*>, std::allocator<std::_Deque_iterator<NP::Global::Schedule_state<long long>, NP::Global::Schedule_state<long long>&, NP::Global::Schedule_state<long long>*> > > >, std::allocator<std::pair<unsigned long const, std::forward_list<std::_Deque_iterator<NP::Global::Schedule_state<long long>, NP::Global::Schedule_state<long long>&, NP::Global::Schedule_state<long long>*>, std::allocator<std::_Deque_iterator<NP::Global::Schedule_state<long long>, NP::Global::Schedule_state<long long>&, NP::Global::Schedule_state<long long>*> > > > >, std::__detail::_Select1st, std::equal_to<unsigned long>, std::hash<unsigned long>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::_M_rehash(unsigned long, unsigned long const&) (1x)
        .           	  __bkt = _M_bucket_index(__k, __code);
        .           	}
        .           
        .                 this->_M_store_code(__node, __code);
        .           
        .                 // Always insert at the beginning of the bucket.
        .                 _M_insert_bucket_begin(__bkt, __node);
  650,159 ( 0.02%)        ++_M_element_count;
        .                 return iterator(__node);
4,551,142 ( 0.13%)      }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               auto
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
-- line 1746 ----------------------------------------
-- line 2023 ----------------------------------------
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               clear() noexcept
        .               {
        .                 this->_M_deallocate_nodes(_M_begin());
    3,619 ( 0.00%)        __builtin_memset(_M_buckets, 0, _M_bucket_count * sizeof(__bucket_type));
       17 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (1x)
      519 ( 0.00%)        _M_element_count = 0;
      518 ( 0.00%)        _M_before_begin._M_nxt = nullptr;
        2 ( 0.00%)      }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               rehash(size_type __bkt_count)
        .               {
    1,527 ( 0.00%)        const __rehash_state& __saved_state = _M_rehash_policy._M_state();
        .                 __bkt_count
    1,524 ( 0.00%)  	= std::max(_M_rehash_policy._M_bkt_for_elements(_M_element_count + 1),
        .           		   __bkt_count);
    3,567 ( 0.00%)        __bkt_count = _M_rehash_policy._M_next_bkt(__bkt_count);
   56,724 ( 0.00%)  => ???:std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const (508x)
    1,148 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
        .           
    1,018 ( 0.00%)        if (__bkt_count != _M_bucket_count)
      252 ( 0.00%)  	_M_rehash(__bkt_count, __saved_state);
  284,502 ( 0.01%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&) (63x)
        .                 else
        .           	// No rehash, restore previous state to keep it consistent with
        .           	// container state.
        .           	_M_rehash_policy._M_reset(__saved_state);
        .               }
        .           
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
      700 ( 0.00%)      _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               _M_rehash(size_type __bkt_count, const __rehash_state& __state)
        .               {
        .                 __try
        .           	{
        .           	  _M_rehash_aux(__bkt_count, __unique_keys());
        .           	}
        .                 __catch(...)
        .           	{
        .           	  // A failure here means that buckets allocation failed.  We only
        .           	  // have to restore hash policy previous state.
        .           	  _M_rehash_policy._M_reset(__state);
        .           	  __throw_exception_again;
        .           	}
      600 ( 0.00%)      }
        .           
        .             // Rehash when there is no equivalent elements.
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
        .               void
        .               _Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal,
        .           	       _H1, _H2, _Hash, _RehashPolicy, _Traits>::
        .               _M_rehash_aux(size_type __bkt_count, true_type)
        .               {
        .                 __bucket_type* __new_buckets = _M_allocate_buckets(__bkt_count);
        .                 __node_type* __p = _M_begin();
      100 ( 0.00%)        _M_before_begin._M_nxt = nullptr;
       32 ( 0.00%)        std::size_t __bbegin_bkt = 0;
   36,162 ( 0.00%)        while (__p)
        .           	{
        .           	  __node_type* __next = __p->_M_next();
        .           	  std::size_t __bkt
        .           	    = __hash_code_base::_M_bucket_index(__p, __bkt_count);
   71,872 ( 0.00%)  	  if (!__new_buckets[__bkt])
        .           	    {
   27,400 ( 0.00%)  	      __p->_M_nxt = _M_before_begin._M_nxt;
   13,700 ( 0.00%)  	      _M_before_begin._M_nxt = __p;
   13,764 ( 0.00%)  	      __new_buckets[__bkt] = &_M_before_begin;
   27,400 ( 0.00%)  	      if (__p->_M_nxt)
   13,668 ( 0.00%)  		__new_buckets[__bbegin_bkt] = __p;
   13,732 ( 0.00%)  	      __bbegin_bkt = __bkt;
        .           	    }
        .           	  else
        .           	    {
    8,536 ( 0.00%)  	      __p->_M_nxt = __new_buckets[__bkt]->_M_nxt;
    8,536 ( 0.00%)  	      __new_buckets[__bkt]->_M_nxt = __p;
        .           	    }
        .           	  __p = __next;
        .           	}
        .           
        .                 _M_deallocate_buckets();
      100 ( 0.00%)        _M_bucket_count = __bkt_count;
      100 ( 0.00%)        _M_buckets = __new_buckets;
        .               }
        .           
        .             // Rehash when there can be equivalent elements, preserve their relative
        .             // order.
        .             template<typename _Key, typename _Value,
        .           	   typename _Alloc, typename _ExtractKey, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash, typename _RehashPolicy,
        .           	   typename _Traits>
-- line 2127 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp
--------------------------------------------------------------------------------
Ir                  

-- line 76 ----------------------------------------
         .           
         .           // In case there is no std::get_new_handler function
         .           // which provides synchronized access to std::new_handler
         .           #if !__TBB_CPP11_GET_NEW_HANDLER_PRESENT
         .           static ProxyMutex new_lock;
         .           #endif
         .           
         .           static inline void* InternalOperatorNew(size_t sz) {
10,333,462 ( 0.29%)      void* res = scalable_malloc(sz);
572,856,974 (15.99%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (5,166,731x)
         .           #if TBB_USE_EXCEPTIONS
10,333,462 ( 0.29%)      while (!res) {
         .                   std::new_handler handler;
         .           #if __TBB_CPP11_GET_NEW_HANDLER_PRESENT
         .                   handler = std::get_new_handler();
         .           #else
         .                   {
         .                       ProxyMutex::scoped_lock lock(new_lock);
         .                       handler = std::set_new_handler(0);
         .                       std::set_new_handler(handler);
-- line 94 ----------------------------------------
-- line 156 ----------------------------------------
         .               *orig_libc_realloc;
         .           
         .           // We already tried to find ptr to original functions.
         .           static std::atomic<bool> origFuncSearched{false};
         .           
         .           inline void InitOrigPointers()
         .           {
         .               // race is OK here, as different threads found same functions
10,333,476 ( 0.29%)      if (!origFuncSearched.load(std::memory_order_acquire)) {
         9 ( 0.00%)          orig_free = dlsym(RTLD_NEXT, "free");
     4,794 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         5 ( 0.00%)          orig_realloc = dlsym(RTLD_NEXT, "realloc");
       873 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
         5 ( 0.00%)          orig_msize = dlsym(RTLD_NEXT, "malloc_usable_size");
     1,027 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
         5 ( 0.00%)          orig_libc_free = dlsym(RTLD_NEXT, "__libc_free");
       905 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
         5 ( 0.00%)          orig_libc_realloc = dlsym(RTLD_NEXT, "__libc_realloc");
       972 ( 0.00%)  => ./dlfcn/dlsym.c:dlsym (1x)
         .           
         .                   origFuncSearched.store(true, std::memory_order_release);
         .               }
         .           }
         .           
         .           /*** replacements for malloc and the family ***/
         .           extern "C" {
         .           #elif MALLOC_ZONE_OVERLOAD_ENABLED
-- line 177 ----------------------------------------
-- line 180 ----------------------------------------
         .           #define ZONE_ARG struct _malloc_zone_t *,
         .           #define PREFIX(name) impl_##name
         .           // not interested in original functions for zone overload
         .           inline void InitOrigPointers() {}
         .           
         .           #endif // MALLOC_UNIXLIKE_OVERLOAD_ENABLED and MALLOC_ZONE_OVERLOAD_ENABLED
         .           
         .           void *PREFIX(malloc)(ZONE_ARG size_t size) __THROW
        10 ( 0.00%)  {
        24 ( 0.00%)      return scalable_malloc(size);
     3,871 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (9x)
     1,677 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
        20 ( 0.00%)  }
         .           
         .           void *PREFIX(calloc)(ZONE_ARG size_t num, size_t size) __THROW
         2 ( 0.00%)  {
         8 ( 0.00%)      return scalable_calloc(num, size);
   147,962 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         4 ( 0.00%)  }
         .           
         .           void PREFIX(free)(ZONE_ARG void *object) __THROW
        14 ( 0.00%)  {
         .               InitOrigPointers();
        32 ( 0.00%)      __TBB_malloc_safer_free(object, (void (*)(void*))orig_free);
     1,022 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
       593 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (6x)
        14 ( 0.00%)  }
         .           
         .           void *PREFIX(realloc)(ZONE_ARG void* ptr, size_t sz) __THROW
         .           {
         .               InitOrigPointers();
         .               return __TBB_malloc_safer_realloc(ptr, sz, orig_realloc);
         .           }
         .           
         .           /* The older *NIX interface for aligned allocations;
-- line 209 ----------------------------------------
-- line 303 ----------------------------------------
         .               return __TBB_malloc_safer_realloc(ptr, size, orig_libc_realloc);
         .           }
         .           #endif // !__ANDROID__
         .           
         .           } /* extern "C" */
         .           
         .           /*** replacements for global operators new and delete ***/
         .           
10,333,462 ( 0.29%)  void* operator new(size_t sz) {
         .               return InternalOperatorNew(sz);
10,333,478 ( 0.29%)  }
     1,719 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:scalable_malloc (2x)
         .           void* operator new[](size_t sz) {
         .               return InternalOperatorNew(sz);
         .           }
10,333,462 ( 0.29%)  void operator delete(void* ptr) noexcept {
         .               InitOrigPointers();
20,666,924 ( 0.58%)      __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
508,979,325 (14.21%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/frontend.cpp:__TBB_malloc_safer_free (5,166,731x)
10,333,462 ( 0.29%)  }
         .           void operator delete[](void* ptr) noexcept {
         .               InitOrigPointers();
         .               __TBB_malloc_safer_free(ptr, (void (*)(void*))orig_free);
         .           }
         .           void* operator new(size_t sz, const std::nothrow_t&) noexcept {
         .               return scalable_malloc(sz);
         .           }
         .           void* operator new[](std::size_t sz, const std::nothrow_t&) noexcept {
-- line 328 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/unordered_map.h
--------------------------------------------------------------------------------
Ir                 

-- line 94 ----------------------------------------
        .              *
        .              *  Base is _Hashtable, dispatched at compile time via template
        .              *  alias __umap_hashtable.
        .              */
        .             template<typename _Key, typename _Tp,
        .           	   typename _Hash = hash<_Key>,
        .           	   typename _Pred = equal_to<_Key>,
        .           	   typename _Alloc = allocator<std::pair<const _Key, _Tp>>>
       14 ( 0.00%)      class unordered_map
  335,208 ( 0.01%)  => /usr/include/c++/10/bits/hashtable.h:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable() (1x)
        .               {
        .                 typedef __umap_hashtable<_Key, _Tp, _Hash, _Pred, _Alloc>  _Hashtable;
        .                 _Hashtable _M_h;
        .           
        .               public:
        .                 // typedefs:
        .                 //@{
        .                 /// Public typedefs.
-- line 110 ----------------------------------------
-- line 283 ----------------------------------------
        .                  *
        .                  *  Note that the assignment completely changes the %unordered_map and
        .                  *  that the resulting %unordered_map's size is the same as the number
        .                  *  of elements assigned.
        .                  */
        .                 unordered_map&
        .                 operator=(initializer_list<value_type> __l)
        .                 {
    1,018 ( 0.00%)  	_M_h = __l;
        .           	return *this;
        .                 }
        .           
        .                 ///  Returns the allocator object used by the %unordered_map.
        .                 allocator_type
        .                 get_allocator() const noexcept
        .                 { return _M_h.get_allocator(); }
        .           
-- line 299 ----------------------------------------
-- line 380 ----------------------------------------
        .                  *  inserted if its first element (the key) is not already present in the
        .                  *  %unordered_map.
        .                  *
        .                  *  Insertion requires amortized constant time.
        .                  */
        .                 template<typename... _Args>
        .           	std::pair<iterator, bool>
        .           	emplace(_Args&&... __args)
    2,500 ( 0.00%)  	{ return _M_h.emplace(std::forward<_Args>(__args)...); }
        .           
        .                 /**
        .                  *  @brief Attempts to build and insert a std::pair into the
        .                  *  %unordered_map.
        .                  *
        .                  *  @param  __pos  An iterator that serves as a hint as to where the pair
        .                  *                should be inserted.
        .                  *  @param  __args  Arguments used to generate a new pair instance (see
-- line 396 ----------------------------------------
-- line 912 ----------------------------------------
        .                  *
        .                  *  This function takes a key and tries to locate the element with which
        .                  *  the key matches.  If successful the function returns an iterator
        .                  *  pointing to the sought after element.  If unsuccessful it returns the
        .                  *  past-the-end ( @c end() ) iterator.
        .                  */
        .                 iterator
        .                 find(const key_type& __x)
   15,046 ( 0.00%)        { return _M_h.find(__x); }
   76,776 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/jobs.hpp:std::_Hashtable<NP::JobID, std::pair<NP::JobID const, unsigned long>, std::allocator<std::pair<NP::JobID const, unsigned long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::find(NP::JobID const&) (2,507x)
        .           
        .                 const_iterator
        .                 find(const key_type& __x) const
        .                 { return _M_h.find(__x); }
        .                 //@}
        .           
        .                 /**
        .                  *  @brief  Finds the number of elements.
-- line 928 ----------------------------------------
-- line 980 ----------------------------------------
        .                  *  Lookup requires constant time.
        .                  */
        .                 mapped_type&
        .                 operator[](const key_type& __k)
        .                 { return _M_h[__k]; }
        .           
        .                 mapped_type&
        .                 operator[](key_type&& __k)
5,657,129 ( 0.16%)        { return _M_h[std::move(__k)]; }
38,194,756 ( 1.07%)  => /usr/include/c++/10/bits/hashtable_policy.h:std::__detail::_Map_base<NP::JobID, std::pair<NP::JobID const, long long>, std::allocator<std::pair<NP::JobID const, long long> >, std::__detail::_Select1st, std::equal_to<NP::JobID>, std::hash<NP::JobID>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](NP::JobID&&) (1,032,203x)
        .                 //@}
        .           
        .                 //@{
        .                 /**
        .                  *  @brief  Access to %unordered_map data.
        .                  *  @param  __k  The key for which data should be retrieved.
        .                  *  @return  A reference to the data whose key is equal to @a __k, if
        .                  *           such a data is present in the %unordered_map.
-- line 996 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: include/global/reduction_set.hpp
--------------------------------------------------------------------------------
Ir                   

-- line 145 ----------------------------------------
          .           		};
          .           
          .           		template <class Time>
          .           		class lst_Job
          .           		{
          .           		public:
          .           			const Job<Time> *J;
          .           			Time LST;
     75,612 ( 0.00%)  			lst_Job(const Job<Time> *base_J, Time base_LST) : J{base_J}, LST{base_LST} {}
          .           		};
          .           
          .           		template <class Time>
          7 ( 0.00%)  		class Reduction_set
          .           		{
          .           		public:
          .           			typedef std::vector<const Job<Time> *> Job_set;
          .           			typedef std::vector<std::size_t> Job_precedence_set;
          .           			typedef std::unordered_map<JobID, Time> Job_map;
          .           			typedef typename Job<Time>::Priority Priority;
          .           			typedef std::unordered_map<JobID, LST_container<Time>> LST_values;
          .           
-- line 165 ----------------------------------------
-- line 192 ----------------------------------------
          .           			std::map<std::size_t, const Job<Time> *> job_by_index;
          .           			Time total_count = 0;
          .           			LST_values LST_map;
          .           
          .           			bool deadline_miss;
          .           
          .           		public:
          .           			// CPU availability gaat dus wat anders worden a.d.h.v de meerdere cores
         12 ( 0.00%)  			Reduction_set(std::vector<Interval<Time>> cpu_availability, const Job_set &jobs, std::vector<std::size_t> &indices, const Job_precedence_set job_precedence_sets)
          .           				: cpu_availability{cpu_availability},
          .           				  jobs{jobs},
          .           				  indices{indices},
          .           				  job_precedence_sets{job_precedence_sets},
          .           				  jobs_by_latest_arrival{jobs},
          .           				  jobs_by_earliest_arrival{jobs},
          .           				  // jobs_by_wcet{jobs},
          .           				  jobs_by_rmax_cmin{jobs},
          .           				  jobs_by_rmin_cmin{jobs},
          .           				  key{0},
          .           				  num_interfering_jobs_added{0},
          .           				  index_by_job(),
          .           				  job_by_index(),
         31 ( 0.00%)  				  LST_map()
        825 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::vector(std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&) (5x)
        195 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (2x)
          .           			{
          .           				// std::cout << "INITIALIZE NEW REDUCITON SET" << std::endl;
          1 ( 0.00%)  				deadline_miss = false;
          .           				std::sort(jobs_by_latest_arrival.begin(), jobs_by_latest_arrival.end(),
          .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
         28 ( 0.00%)  						  { if(i->latest_arrival() < j->latest_arrival()){
          .           							return true;
         21 ( 0.00%)  						  }else if(i->latest_arrival() > j->latest_arrival()){
          .           							return false;
          .           						  }else{
          .           							return i->get_priority() < j->get_priority();
          .           						  }; });
          .           
          .           				std::sort(jobs_by_earliest_arrival.begin(), jobs_by_earliest_arrival.end(),
          .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
          .           						  { return i->earliest_arrival() < j->earliest_arrival(); });
-- line 230 ----------------------------------------
-- line 231 ----------------------------------------
          .           
          .           				// std::sort(jobs_by_wcet.begin(), jobs_by_wcet.end(),
          .           				//		  [](const Job<Time> *i, const Job<Time> *j) -> bool
          .           				//		  { return i->maximal_cost() < j->maximal_cost(); });
          .           
          .           				// these sorts are required for the reverse fill algorithm
          .           				std::sort(jobs_by_rmin_cmin.begin(), jobs_by_rmin_cmin.end(),
          .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
         44 ( 0.00%)  						  { return i->earliest_arrival() + i->minimal_cost() < j->earliest_arrival() + j->minimal_cost(); });
          .           
          .           				std::sort(jobs_by_rmax_cmin.begin(), jobs_by_rmax_cmin.end(),
          .           						  [](const Job<Time> *i, const Job<Time> *j) -> bool
         46 ( 0.00%)  						  { return i->earliest_arrival() + i->maximal_cost() < j->earliest_arrival() + j->maximal_cost(); });
          .           
          .           				// No clue yet why this exists but i dont want to remove it yet.
         27 ( 0.00%)  				for (int i = 0; i < jobs.size(); i++)
          .           				{
          8 ( 0.00%)  					auto j = jobs[i];
         24 ( 0.00%)  					std::size_t idx = indices[i];
          .           
          .           					index_by_job.emplace(j->get_id(), idx);
         24 ( 0.00%)  					job_by_index.emplace(std::make_pair(idx, jobs[i]));
          .           				}
          .           
          .           				// latest_busy_time = compute_latest_busy_time();
          4 ( 0.00%)  				latest_idle_time = compute_latest_idle_time();
        358 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() (1x)
          .           				// compute_latest_start_times();
          2 ( 0.00%)  				compute_latest_start_time_complex();
     27,249 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() (1x)
          1 ( 0.00%)  				max_priority = compute_max_priority();
          .           				initialize_key();
          .           				// std::cout << "Candidate reduction set contains " << jobs.size() << " jobs" << std::endl;
          .           
          .           				// for (const Job<Time>* j : jobs) {
          .           				//	std::cout<<j->get_id()<<" ";
          .           				// }
          .           				// std::cout<<std::endl;
          8 ( 0.00%)  			}
          .           
          .           			// For test purposes
          .           			Reduction_set(std::vector<Interval<Time>> cpu_availability, const Job_set &jobs, std::vector<std::size_t> indices)
          9 ( 0.00%)  				: Reduction_set(cpu_availability, jobs, indices, {})
     33,814 ( 0.00%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >) (1x)
          .           			{
          .           			}
          .           
          .           			int get_number_of_jobs()
          .           			{
          .           				return jobs.size();
          .           			}
          .           
          .           			void initialize_key()
          .           			{
         34 ( 0.00%)  				for (const Job<Time> *j : jobs)
          .           				{
          8 ( 0.00%)  					key = key ^ j->get_key();
          .           				}
          .           			}
          .           
          .           			hash_value_t get_key() const
          .           			{
          .           				return key;
          .           			}
          .           
          .           			bool has_potential_deadline_misses() const
          .           			{
      1,020 ( 0.00%)  				if (deadline_miss)
          .           				{
          .           					return true;
          .           				}
          .           				return false;
          .           			}
          .           
     19,992 ( 0.00%)  			void add_job(const Job<Time> *jx, std::size_t index)
          .           			{
      2,499 ( 0.00%)  				deadline_miss = false;
      2,499 ( 0.00%)  				num_interfering_jobs_added++;
          9 ( 0.00%)  				jobs.push_back(jx);
          .           
          .           				index_by_job.emplace(jx->get_id(), index);
      7,497 ( 0.00%)  				job_by_index.emplace(std::make_pair(index, jobs.back()));
          9 ( 0.00%)  				indices.push_back(index);
          .           
      2,499 ( 0.00%)  				insert_sorted(jobs_by_latest_arrival, jx,
          .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
     46,962 ( 0.00%)  							  { if(i->latest_arrival() < j->latest_arrival()){
          .           							return true;
     23,481 ( 0.00%)  						  }else if(i->latest_arrival() > j->latest_arrival()){
          .           							return false;
          .           						  }else{
          .           							return i->get_priority() < j->get_priority();
          .           						  }; });
      2,499 ( 0.00%)  				insert_sorted(jobs_by_earliest_arrival, jx,
          .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
          .           							  { return i->earliest_arrival() < j->earliest_arrival(); });
          .           				// insert_sorted(jobs_by_wcet, jx,
          .           				//			  [](const Job<Time> *i, const Job<Time> *j) -> bool
          .           				//			  { return i->maximal_cost() < j->maximal_cost(); });
      2,499 ( 0.00%)  				insert_sorted(jobs_by_rmax_cmin, jx,
          .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
     94,872 ( 0.00%)  							  { return i->latest_arrival() + i->minimal_cost() < j->latest_arrival() + j->minimal_cost(); });
      2,499 ( 0.00%)  				insert_sorted(jobs_by_rmin_cmin, jx,
          .           							  [](const Job<Time> *i, const Job<Time> *j) -> bool
     94,872 ( 0.00%)  							  { return i->earliest_arrival() + i->minimal_cost() < j->earliest_arrival() + j->minimal_cost(); });
          .           
          .           				// latest_busy_time = compute_latest_busy_time();
      7,497 ( 0.00%)  				key = key ^ jx->get_key();
          .           				// latest_idle_time = compute_latest_idle_time();
          .           				//  compute_latest_start_times();
          .           				// compute_latest_start_time_complex();
          .           
      4,998 ( 0.00%)  				if (!jx->priority_at_least(max_priority))
          .           				{
        272 ( 0.00%)  					max_priority = jx->get_priority();
          .           				}
     14,994 ( 0.00%)  			}
          .           
          .           			void update_set()
          .           			{
      1,524 ( 0.00%)  				latest_idle_time = compute_latest_idle_time();
 11,005,728 ( 0.31%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_idle_time() (508x)
      1,016 ( 0.00%)  				compute_latest_start_time_complex();
3,505,745,586 (97.85%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_latest_start_time_complex() (508x)
          .           			}
          .           
          .           			Job_set get_jobs() const
          .           			{
         16 ( 0.00%)  				return jobs;
     20,304 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::vector(std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&) (1x)
          .           			}
          .           
          .           			Time get_latest_busy_time() const
          .           			{
          .           				return latest_busy_time;
          .           			}
          .           
          .           			Time get_latest_idle_time() const
          .           			{
          .           				return latest_idle_time;
          .           			}
          .           
          .           			Time get_latest_LST()
          .           			{
     61,227 ( 0.00%)  				return latest_LST;
          .           			}
          .           
          .           			Job_map get_latest_start_times() const
          .           			{
          .           				return latest_start_times;
          .           			}
          .           
          .           			Time get_latest_start_time(const Job<Time> &job) const
          .           			{
          .           				auto iterator = latest_start_times.find(job.get_id());
      2,507 ( 0.00%)  				return iterator == latest_start_times.end() ? -1 : iterator->second;
          .           			}
          .           
          .           			Time earliest_finish_time(const Job<Time> &job) const
          .           			{
     12,535 ( 0.00%)  				return std::max(cpu_availability[0].min(), job.earliest_arrival()) + job.least_cost();
          .           			}
          .           
          .           			Time latest_finish_time(const Job<Time> &job) const
          .           			{
      2,507 ( 0.00%)  				return get_latest_start_time(job) + job.maximal_cost();
          .           			}
          .           
          .           			Priority compute_max_priority() const
          .           			{
          2 ( 0.00%)  				Priority max_prio{};
          .           
         35 ( 0.00%)  				for (const Job<Time> *j : jobs)
          .           				{
          .           					if (!j->priority_exceeds(max_prio))
          .           					{
          .           						max_prio = j->get_priority();
          .           					}
          .           				}
          .           
          .           				return max_prio;
-- line 405 ----------------------------------------
-- line 418 ----------------------------------------
          .           					Time LST_j = compute_latest_start_time(j);
          .           					latest_start_times.emplace(j->get_id(), LST_j);
          .           				}
          .           
          .           				return;
          .           			}
          .           
          .           			// Hierin gaan we voor iedere job de LST berekenen en opslaan dan hoeven we dat niet voor iedere job te doen opnieuw
      5,090 ( 0.00%)  			void compute_latest_start_time_complex()
          .           			{
          .           				// std::cout << "starting complex calculation :0" << std::endl;
          .           				latest_start_times = {};
          .           				// reserve the size for the latest start times preemtively
          .           				latest_start_times.reserve(jobs.size());
          .           				std::vector<lst_Job<Time>> LST_list;
          .           				std::vector<const Job<Time> *> no_LST_list;
          .           				//possible optimization:
          .           				//have an input J_new for the new job, iterate over jobs by earliest arrival and find the job whose LST <= r_new^max, 
          .           				//only update those new LST's and copy the rest of them to the LST list.
      1,527 ( 0.00%)  				typename std::vector<const Job<Time> *>::iterator lb_LPIW = jobs_by_earliest_arrival.begin();
  4,218,640 ( 0.12%)  				for (const Job<Time> *j : jobs_by_latest_arrival)
          .           				{
    640,130 ( 0.02%)  					Time HPIW = 0;
  7,041,430 ( 0.20%)  					std::vector<Time> BIW = complexBIW(LST_list, no_LST_list, lb_LPIW, j, HPIW);
949,426,473 (26.50%)  => include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::complexBIW(std::vector<NP::Global::lst_Job<long long>, std::allocator<NP::Global::lst_Job<long long> > >&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&, __gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >&, NP::Job<long long> const*, long long&) (640,130x)
  3,200,650 ( 0.09%)  					Time LST_j = complexHPIW(HPIW, lb_LPIW, BIW, j);
  1,280,260 ( 0.04%)  					latest_start_times.emplace(j->get_id(), LST_j);
  1,280,260 ( 0.04%)  					if (j->exceeds_deadline(LST_j + j->maximal_cost()))
          .           					{
          .           						deadline_miss = true;
          .           						return;
          .           					}
          .           				}
          .           
          .           				return;
      4,072 ( 0.00%)  			}
          .           
          .           			Time complexHPIW(Time HPIW, typename std::vector<const Job<Time> *>::iterator it_LPIW, std::vector<Time> BIW, const Job<Time> *j_i)
          .           			{
          .           				//Vector for to store the differences between the blocking interfering workload
          .           				std::vector<Time> Ceq;
  6,401,300 ( 0.18%)  				for (int i = 1; i < cpu_availability.size(); i++)
          .           				{
 11,522,340 ( 0.32%)  					Ceq.emplace_back(BIW[i] - BIW[i - 1]);
519,454,837 (14.50%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (1,920,390x)
          .           				}
          .           
          .           				//make this value cumilative
 11,522,340 ( 0.32%)  				for (int i = 1; i < cpu_availability.size() - 1; i++)
          .           				{
  5,121,040 ( 0.14%)  					Ceq[i] = Ceq[i] * (1 + i) + Ceq[i - 1];
          .           				}
          .           				//our reference point is the lowest value
          .           				Time ref = BIW[0];
          .           				Time HPIW_i = HPIW;
          .           
          .           				// initial LST using the HPIW from previous jobs
  3,840,780 ( 0.11%)  				Time LST_i = j_i->latest_arrival() + ref + HPIW;
  5,042,315 ( 0.14%)  				for (int i = 0; i < cpu_availability.size() - 1; i++)
          .           				{
  4,389,678 ( 0.12%)  					if (HPIW >= Ceq[cpu_availability.size() - 2 - i])
          .           					{
  5,911,919 ( 0.17%)  						LST_i = j_i->latest_arrival() + ref + BIW[cpu_availability.size() - 1 - i] + floor((HPIW - Ceq[cpu_availability.size() - 2 - i]) / (cpu_availability.size() - i));
          .           						break;
          .           					}
          .           				}
          .           
          .           				// we restart this loop where we ended when calculating the BIW
          .           				// this means that the earliest arrival of all the upcoming jobs are >= than the current job.
          .           				//	meaning that they shoudl never have an lst
          .           				typename std::vector<const Job<Time> *>::iterator it_HPIW = it_LPIW;
467,529,481 (13.05%)  				while (it_HPIW != jobs_by_earliest_arrival.end())
          .           				{
155,229,934 ( 4.33%)  					const Job<Time> *j_j = *it_HPIW;
          .           					// only take high/same prio jobs into account
776,149,670 (21.66%)  					if (j_j != j_i && j_j->get_priority() <= j_i->get_priority())
          .           					{
          .           						Time current_j_j_lst = (j_j->latest_arrival() < j_j->latest_arrival()) ? get_latest_start_time(*j_j) : j_j->latest_arrival();
  1,341,900 ( 0.04%)  						if (j_j->earliest_arrival() <= LST_i)
          .           						{
    438,850 ( 0.01%)  							if (current_j_j_lst >= j_i->latest_arrival())
          .           							{
    877,700 ( 0.02%)  								HPIW += j_j->maximal_cost();
    438,850 ( 0.01%)  								LST_i = j_i->latest_arrival() + ref + HPIW;
          .           
  1,382,798 ( 0.04%)  								for (int i = 0; i < cpu_availability.size() - 1; i++)
          .           								{
  1,163,373 ( 0.03%)  									if (HPIW >= Ceq[cpu_availability.size() - 2 - i])
          .           									{
  2,413,675 ( 0.07%)  										LST_i = j_i->latest_arrival() + ref + BIW[cpu_availability.size() - 1 - i] + floor((HPIW - Ceq[cpu_availability.size() - 2 - i]) / (cpu_availability.size() - i));
          .           										break;
          .           									}
          .           								}
          .           								//checking for deadline misses here enables us to discard reduction sets faster
    438,850 ( 0.01%)  								if (j_i->exceeds_deadline(LST_i + j_i->maximal_cost()))
          .           									return LST_i;
          .           							}
          .           						}
          .           						else
          .           						{
  2,709,150 ( 0.08%)  							latest_LST = std::max(latest_LST, LST_i);
    451,525 ( 0.01%)  							return LST_i;
          .           						}
          .           					}
          .           					it_HPIW++;
          .           				}
    943,025 ( 0.03%)  				latest_LST = std::max(latest_LST, LST_i);
    188,605 ( 0.01%)  				return LST_i;
          .           			}
          .           
  9,601,950 ( 0.27%)  			std::vector<Time> complexBIW(std::vector<lst_Job<Time>> &LST_list, std::vector<const Job<Time> *> &no_LST_list, typename std::vector<const Job<Time> *>::iterator &it_LPIW, const Job<Time> *j_i, Time &pre_calc_high)
          .           			{
          .           				// we will loop over this indefinately
          .           				std::vector<Time> Cmax;
          .           				// number of cores that we have
    640,130 ( 0.02%)  				int m = cpu_availability.size();
  9,601,950 ( 0.27%)  				for (int i = 0; i < m; i++)
          .           				{
          .           					// init this list with only zeroes
 10,242,080 ( 0.29%)  					Cmax.emplace_back((Time)0);
534,173,125 (14.91%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (2,560,520x)
          .           				}
          .           				// first, this list with jobs that we have an LST for
          .           				auto lst_it = LST_list.begin();
  2,009,978 ( 0.06%)  				while (lst_it != LST_list.end())
          .           				{
          .           					lst_Job<Time> &lst_job = *lst_it;
          .           					// we have to go through the entire list, as we cannot sort i properly since that is reliant on the current j_i->latest_arrival value.
          .           					// but we can remove jobs that we do not need anymore
          .           					// namely jobs that do not cross the current j_i->latest_arrival value anymore.
    258,910 ( 0.01%)  					if (lst_job.LST + lst_job.J->maximal_cost() <= j_i->latest_arrival())
          .           					{
          .           						// update the iterator and continue
          .           						lst_it = LST_list.erase(lst_it);
          .           						continue;
          .           					}
          .           					// okay so now we know that the job still traverses the r_i^max
     41,928 ( 0.00%)  					if (lst_job.J->get_priority() > j_i->get_priority())
          .           					{
          .           						// if its a lower prio job, then we add it as follows to the Cmax
      5,222 ( 0.00%)  						Time C_aug = lst_job.LST + lst_job.J->maximal_cost() - (j_i->latest_arrival() - 1);
      5,222 ( 0.00%)  						if (C_aug > Cmax[0])
          .           						{
          .           							linear_inserter(Cmax, C_aug);
          .           						}
          .           					}
          .           					else
          .           					{
          .           						// if its a high prio job, then we we can do two things
     22,730 ( 0.00%)  						if (lst_job.LST >= j_i->latest_arrival())
          .           						{
          .           							// if its larger or equal to the upcoming latest arrival, then add it to the HPIW already
          .           							// we're allowed to do this as we know that the earliest release is before rimax and the LST is after, so it can interfere
     22,730 ( 0.00%)  							pre_calc_high += lst_job.J->maximal_cost();
          .           						}
          .           						else
          .           						{
          .           							// otherwise we can do the same kind of calculation as performed for the Low prio interference :)
          .           							Time C_aug = lst_job.LST + lst_job.J->maximal_cost() - (j_i->latest_arrival() - 1);
          .           							if (C_aug > Cmax[0])
          .           							{
          .           								linear_inserter(Cmax, C_aug);
-- line 576 ----------------------------------------
-- line 578 ----------------------------------------
          .           						}
          .           					}
          .           					lst_it++;
          .           				}
          .           
          .           				// okay now we just have to check all jobs that had a higher rmax than the previous job, and thus did not have an LST yet
          .           				// std::cout << "we have " << LST_list.size() << " jobs without a LST" << std::endl;
          .           				auto no_lst_it = no_LST_list.begin();
 10,217,636 ( 0.29%)  				while (no_lst_it != no_LST_list.end())
          .           				{
  4,468,688 ( 0.12%)  					const Job<Time> *no_lst_job = *no_lst_it;
          .           
  8,937,376 ( 0.25%)  					if (no_lst_job == j_i)
          .           					{
          .           						no_lst_it++;
          .           						continue;
          .           					}
          .           					// first check if it might have an LST now
 11,487,201 ( 0.32%)  					if (no_lst_job->latest_arrival() >= j_i->latest_arrival())
          .           					{
          .           						// now we can assume that it doesnt have an LST yet (high prio ones will have but does not matter at this point)
  8,390,592 ( 0.23%)  						if (no_lst_job->get_priority() <= j_i->get_priority())
          .           						{
          .           							// so if its a higher prio job, then we can add it to HPIW as well
          .           
  2,607,844 ( 0.07%)  							pre_calc_high += no_lst_job->maximal_cost();
          .           						}
          .           						else
          .           						{
          .           							// otherwise its a lower prio job, and thus can block maximally so lets check if its large enough to block
          .           							Time C = no_lst_job->maximal_cost();
  2,985,884 ( 0.08%)  							if (C > Cmax[0])
          .           							{
  4,359,774 ( 0.12%)  								linear_inserter(Cmax, C);
 53,000,018 ( 1.48%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (1,453,258x)
          .           							}
          .           						}
          .           					}
          .           					else
          .           					{
          .           						// arriving here means that the job has a rmax lower than the current rmax, and thus MUST have an LST
  3,096,609 ( 0.09%)  						Time lst = latest_start_times[no_lst_job->get_id()];
  3,096,609 ( 0.09%)  						if (lst + no_lst_job->maximal_cost() <= j_i->latest_arrival())
          .           						{
          .           							// if it will never interfere, then just remove it from the list
          .           
          .           							no_lst_it = no_LST_list.erase(no_lst_it);
          .           							continue;
          .           						}
  1,292,571 ( 0.04%)  						if (no_lst_job->get_priority() <= j_i->get_priority())
          .           						{
    109,890 ( 0.00%)  							if (lst >= j_i->latest_arrival())
          .           							{
          .           								// if it has a higher priority then that still means that it can interfere with the current job and therefor we add it to the HPIW and the other list
     75,612 ( 0.00%)  								pre_calc_high += no_lst_job->maximal_cost();
          .           								// and higher prio jobs also have an LST already so lets move it to the other list
          .           								LST_list.emplace_back(lst_Job<Time>(no_lst_job, lst));
          .           								// and remove it from the current
          .           
          .           								no_lst_it = no_LST_list.erase(no_lst_it);
    639,152 ( 0.02%)  								continue;
          .           							}
          .           							else
          .           							{
          .           								// otherwise we can do the same kind of calculation as performed for the Low prio interference :)
     34,278 ( 0.00%)  								Time C_aug = lst + no_lst_job->maximal_cost() - (j_i->latest_arrival() - 1);
    410,190 ( 0.01%)  								if (C_aug > Cmax[0])
          .           								{
          .           									linear_inserter(Cmax, C_aug);
          .           								}
          .           							}
          .           						}
          .           						else
          .           						{
          .           							// so if its a lower prio job then we can
    751,824 ( 0.02%)  							if (lst >= j_i->latest_arrival())
          .           							{
          .           								Time C = no_lst_job->maximal_cost();
    159,556 ( 0.00%)  								if (C > Cmax[0])
          .           								{
    239,334 ( 0.01%)  									linear_inserter(Cmax, C);
  3,309,136 ( 0.09%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (79,778x)
          .           								}
          .           							}
          .           							else
          .           							{
          .           								// otherwise we can still do the augmented one
    592,268 ( 0.02%)  								Time C_aug = lst + no_lst_job->maximal_cost() - (j_i->latest_arrival() - 1);
    592,268 ( 0.02%)  								if (C_aug > Cmax[0])
          .           								{
    626,546 ( 0.02%)  									linear_inserter(Cmax, C_aug);
 13,770,488 ( 0.38%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (313,273x)
          .           								}
          .           							}
          .           						}
          .           					}
          .           					no_lst_it++;
          .           				}
          .           
  2,563,209 ( 0.07%)  				while (it_LPIW != jobs_by_earliest_arrival.end())
          .           				{
  2,554,124 ( 0.07%)  					const Job<Time> *LPIW_job = *it_LPIW;
  2,554,124 ( 0.07%)  					if (LPIW_job == j_i)
          .           					{
          .           						// if we encounter ourselves, then just place it in the no LST list for future
          .           						// std::cout<<"self encounter"<<std::endl;
          .           						no_LST_list.emplace_back(LPIW_job);
          .           						it_LPIW++;
          .           						continue;
          .           					}
  2,553,106 ( 0.07%)  					if (LPIW_job->earliest_arrival() >= j_i->latest_arrival())
          .           					{
          .           						// return where we've ended in this exploration so we can continue further at a later stage
          .           						break;
          .           					}
          .           					// all jobs that we encounter here are not present in the lists above since we will start from where we ended last time, so we'll never encounter the same job twice
          .           
  1,918,863 ( 0.05%)  					if (LPIW_job->get_priority() <= j_i->get_priority())
          .           					{
    345,896 ( 0.01%)  						if (LPIW_job->latest_arrival() >= j_i->latest_arrival())
          .           						{
    518,844 ( 0.01%)  							pre_calc_high += LPIW_job->maximal_cost();
          .           							// since its latest arrival is >= rimax, it wont have an LST so add it to the no LST list
          .           							// std::cout<<"\t pushed hi job" << LPIW_job->get_id() << " to no LST list" << std::endl;
    518,844 ( 0.01%)  							no_LST_list.emplace_back(LPIW_job);
  4,124,468 ( 0.12%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::emplace_back<NP::Job<long long> const*&>(NP::Job<long long> const*&) (172,948x)
          .           						}
          .           						else
          .           						{
          .           							// since its latest arrival is < rimax it will have an LST, so it might be BIW
          .           							Time high_LST = latest_start_times[LPIW_job->get_id()];
          .           							if (high_LST + LPIW_job->maximal_cost() <= j_i->latest_arrival())
          .           							{
          .           								// we can instanly check if its a waste of a job, then we dont have to calculate further and just continue to the next one
-- line 707 ----------------------------------------
-- line 724 ----------------------------------------
          .           							// since we have an LST we add it to the LST list
          .           							// std::cout<<"\t pushed hi job " << LPIW_job->get_id() << " to LST list" << std::endl;
          .           							LST_list.emplace_back(lst_Job<Time>(LPIW_job, high_LST));
          .           						}
          .           					}
          .           					else
          .           					{
          .           						// if its a lower prio job then
    933,346 ( 0.03%)  						if (LPIW_job->latest_arrival() >= j_i->latest_arrival())
          .           						{
          .           							Time C = LPIW_job->maximal_cost();
    933,346 ( 0.03%)  							if (C > Cmax[0])
          .           							{
  1,171,170 ( 0.03%)  								linear_inserter(Cmax, C);
 11,929,854 ( 0.33%)  => /usr/include/c++/10/bits/stl_vector.h:NP::Global::Reduction_set<long long>::linear_inserter(std::vector<long long, std::allocator<long long> >&, long long) [clone .isra.0] (390,390x)
          .           							}
          .           							// std::cout<<"\t pushed lo job" << LPIW_job->get_id() << " to no LST list" << std::endl;
  1,868,728 ( 0.05%)  							no_LST_list.emplace_back(LPIW_job);
 11,266,512 ( 0.31%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::emplace_back<NP::Job<long long> const*&>(NP::Job<long long> const*&) (467,182x)
          .           						}
          .           						else
          .           						{
          .           							// again this means that the latest arrival < rmax so it must ahve an LST
          .           							Time low_LST = latest_start_times[LPIW_job->get_id()];
          .           							if (low_LST + LPIW_job->maximal_cost() <= j_i->latest_arrival())
          .           							{
          .           								// we can instanly check if its a waste of a job, then we dont have to calculate further and just continue to the next one
-- line 748 ----------------------------------------
-- line 774 ----------------------------------------
          .           					}
          .           
          .           					it_LPIW++;
          .           				}
          .           				//  okay now we have the maximum interference that can be caused by jobs that start before j_i
          .           				//  we just need to take the system availabilites into consideration now.
          .           				//  Cmax  is sorted low -> high
          .           				//  A^max is sorted low -> high
 12,416,423 ( 0.35%)  				for (int i = 0; i < m; i++)
          .           				{
          .           					// highest value from Cmax
  5,761,170 ( 0.16%)  					Time Cmax_i = Cmax[m - 1 - i];
          .           					// compared to the lowest value from A^max
          .           					Time A_max_i = cpu_availability[i].max();
          .           					// compared to the current job offset
          .           					// r_i^max
 17,923,640 ( 0.50%)  					Cmax[m - 1 - i] = std::max(j_i->latest_arrival() - 1 + Cmax_i, std::max(A_max_i, j_i->latest_arrival())) - j_i->latest_arrival();
          .           				}
          .           				// make sure its a ascending list
          .           				std::sort(Cmax.begin(), Cmax.end());
          .           				return Cmax;
  5,761,170 ( 0.16%)  			}
          .           
          .           			void linear_inserter(std::vector<Time> &list, Time object)
          .           			{
          .           				Time swap;
  2,239,310 ( 0.06%)  				list[0] = object;
  2,239,310 ( 0.06%)  				int m = cpu_availability.size() - 1;
 25,655,092 ( 0.72%)  				for (int i = 0; i < m; i++)
          .           				{
 23,755,372 ( 0.66%)  					if (list[i] > list[i + 1])
          .           					{
          .           						swap = list[i + 1];
          .           						list[i + 1] = list[i];
 19,278,056 ( 0.54%)  						list[i] = swap;
          .           					}
          .           					else
          .           					{
          .           						return;
          .           					}
          .           				}
  2,236,699 ( 0.06%)  			}
          .           
          .           			Time compute_latest_start_time(const Job<Time> *j_i)
          .           			{
          .           
          .           				//  Blocking interfering workload for job j_i
          .           
          .           				Time Ceq[cpu_availability.size() - 1];
          .           				Time BIW[cpu_availability.size()];
-- line 823 ----------------------------------------
-- line 1084 ----------------------------------------
          .           					{
          .           						// std::cout<<"initial deadline miss for " << j->get_id() << std::endl;
          .           						deadline_miss = true;
          .           						return;
          .           					}
          .           				}
          .           			}
          .           
      4,072 ( 0.00%)  			Time compute_latest_idle_time()
          .           			{
          .           				// remove
          .           				Time latest_idle_time{-1};
          .           				// this is a counter to check how much jobs will always overlap with the release interval of the current job, if its > m then there will never be an interval at that point
          .           				Time overflow = 0;
          .           				// Save the m-1 largest jobs
      2,036 ( 0.00%)  				Time Cmax[cpu_availability.size() - 1];
          .           				// Sum to add the remaining workload
          .           				Time Crest = 0;
          .           				// for convenience sake, remember the earliest time a core might become free
      1,018 ( 0.00%)  				Time Amin = cpu_availability.front().min();
          .           
          .           				// std::cout << "Finding idle intervals..." << std::endl;
          .           				// std::cout << jobs_by_latest_arrival.size() << std::endl;
      1,018 ( 0.00%)  				for (auto it_x = jobs_by_latest_arrival.rbegin(); it_x != jobs_by_latest_arrival.rend(); it_x++)
          .           				{
        509 ( 0.00%)  					const Job<Time> *j_x = *it_x;
          .           					// std::cout << "Looking for idle interval for job " << j_x->get_id() << " possibly ending at " << j_x->latest_arrival() << std::endl;
          .           					//  reset the overflow and Crest values for each job under investigation
        509 ( 0.00%)  					overflow = 0;
      1,018 ( 0.00%)  					Crest = 0;
        509 ( 0.00%)  					int A_i = 0;
          .           					// Compute the interference from possible previous states
      6,617 ( 0.00%)  					for (Interval<Time> A : cpu_availability)
          .           					{
          .           						// since we dont use the first value as it is always 0
          .           						if (A_i != 0)
          .           						{
          .           							// Save the interfering workloads as the initial "highest workloads"
      3,054 ( 0.00%)  							Cmax[A_i - 1] = A.min() - Amin;
          .           							// std::cout << "\t" << Cmax[A_i - 1] << std::endl;
          .           						}
        509 ( 0.00%)  						A_i++;
          .           					}
          .           
          .           					// Now we again need to iterate over this list
          .           					// i think i can set it_y to it_x, to limit the search
  1,280,260 ( 0.04%)  					for (auto it_y = jobs_by_latest_arrival.rbegin(); it_y != jobs_by_latest_arrival.rend(); it_y++)
          .           					{
    640,130 ( 0.02%)  						const Job<Time> *j_y = *it_y;
  1,280,260 ( 0.04%)  						if (j_y->latest_arrival() < j_x->latest_arrival())
          .           						{
          .           							// this statement checks the overflow condition
  1,278,826 ( 0.04%)  							if (j_y->earliest_arrival() + j_y->minimal_cost() >= j_x->latest_arrival())
          .           							{
  2,557,652 ( 0.07%)  								overflow++;
          .           							}
          .           							// if the current minimal cost is larger than the smallest saved value then add that value to Crest and insert the new value linearly
  1,918,239 ( 0.05%)  							if (j_y->minimal_cost() > Cmax[0])
          .           							{
      3,880 ( 0.00%)  								Crest += Cmax[0];
      3,880 ( 0.00%)  								Cmax[0] = j_y->minimal_cost();
          .           								Time swap;
     26,176 ( 0.00%)  								for (int i = 0; i < cpu_availability.size() - 2; i++)
          .           								{
     24,892 ( 0.00%)  									if (Cmax[i] > Cmax[i + 1])
          .           									{
          .           										swap = Cmax[i + 1];
          .           										Cmax[i + 1] = Cmax[i];
     14,208 ( 0.00%)  										Cmax[i] = swap;
          .           									}
          .           									else
          .           									{
          .           										break;
          .           									}
          .           								}
          .           							}
          .           							else
          .           							{
          .           								// if its smaller than all values in Cmax, just add it to Crest
    635,533 ( 0.02%)  								Crest += j_y->minimal_cost();
          .           							}
          .           						}
          .           					}
          .           					/*
          .           					There is an interval if the workload of all jobs except the m-1 largest jobs spread over all cores
          .           					is smaller than the latest release time of the current job under investigation
          .           					*/
     24,432 ( 0.00%)  					if (Amin + ceil((double)Crest / (double)cpu_availability.size()) < j_x->latest_arrival() && overflow < cpu_availability.size())
          .           					{
          .           
          .           						// std::cout << "\t Latest interval might end at:" << j_x->latest_arrival() << std::endl;
          .           						return j_x->latest_arrival();
          .           					}
          .           				}
          .           
          .           				// std::cout << "\t No interval possible in this set" << std::endl;
          .           				//  no jobs can be released at -1 so we can safely return 0 if there is no interval anywhere.
          .           				return 0;
      4,581 ( 0.00%)  			}
          .           
          .           			unsigned long get_num_interfering_jobs_added() const
          .           			{
          .           				return num_interfering_jobs_added;
          .           			}
          .           
          .           			bool can_interfere(const Job<Time> &job) const
          .           			{
          .           				// find_if geeft een pointer naar de eerste value die overeenkomt naar de eerset value in die lijst
          .           				auto pos = std::find_if(jobs.begin(), jobs.end(),
          .           										[&job](const Job<Time> *j)
          .           										{ return j->get_id() == job.get_id(); });
          .           
          .           				// als die pointer naar de laatste plek wijst, dan betekent dat dat deze dus nog niet in de lijst zit en daarom dus.. buiten de lijst zit
      5,014 ( 0.00%)  				if (pos != jobs.end())
          .           				{
          .           					// std::cout << "i already use job " << job.get_id() << std::endl;
          .           					return false;
          .           				}
          .           
          .           				// rx_min < delta_M
          .           				// latest idle time is het einde van de laatste idle interval in de set, dus die moet steeds herberekend
          .           				// worden voor we nieuwe interfering jobs gaan zoekn
      4,998 ( 0.00%)  				if (job.earliest_arrival() <= latest_idle_time)
          .           				{
          .           					// std::cout << "Interference due to idle period " << job.get_id() << std::endl;
          .           					return true;
          .           				}
          .           
          .           				// min_wcet wordt hier niet gebruikt dus waarom doen we dit?
          .           				// Time min_wcet = min_lower_priority_wcet(job);
          .           
-- line 1214 ----------------------------------------
-- line 1233 ----------------------------------------
          .           				}
          .           
          .           				return false;
          .           			}
          .           
          .           			void certainly_available_i(Time *Chigh, Time Clow, Time s, std::vector<Time> *CA_values)
          .           			{
          .           				Time tClow = Clow;
          2 ( 0.00%)  				if (cpu_availability.size() == 1)
          .           				{
          3 ( 0.00%)  					CA_values->emplace_back(s + Clow + Chigh[0]);
          .           					return;
          .           				}
          2 ( 0.00%)  				if (Chigh[0] <= 0)
          .           				{
         15 ( 0.00%)  					for (int i = 0; i < cpu_availability.size(); i++)
          .           					{
         12 ( 0.00%)  						if (Chigh[i] > 0)
          .           						{
         13 ( 0.00%)  							CA_values->emplace_back(s + Chigh[i]);
         23 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<long long, std::allocator<long long> >::emplace_back<long long>(long long&&) (1x)
          .           						}
          .           						else
          .           						{
          .           							CA_values->emplace_back(s);
          .           						}
          .           					}
          .           				}
          .           				else
-- line 1260 ----------------------------------------
-- line 1291 ----------------------------------------
          .           							// we have enough to properly equalize at this point
          .           							CA_values->emplace_back(s + floor((double)eq / (double)cpu_availability.size()) + Chigh[cpu_availability.size() - 1]);
          .           						}
          .           						Clow += Chigh[i - 1];
          .           					}
          .           				}
          .           			}
          .           
         10 ( 0.00%)  			std::vector<Time> compute_certainly_available()
          .           			{
          .           				// std::cout<<"\ncomputing CA"<<std::endl;
          .           				std::vector<Time> CA_values;
          1 ( 0.00%)  				int m = cpu_availability.size();
          9 ( 0.00%)  				Time Chigh[cpu_availability.size()];
          1 ( 0.00%)  				Time Clow = 0;
          .           
          1 ( 0.00%)  				Time last_event = -1;
          4 ( 0.00%)  				Time s = std::max(cpu_availability[0].max(), jobs_by_latest_arrival[0]->latest_arrival());
          .           
          .           				Time event = s;
          .           
          5 ( 0.00%)  				Time prev = 0;
         15 ( 0.00%)  				for (int i = 1; i < m; i++)
          .           				{
         15 ( 0.00%)  					prev += cpu_availability[i].max() - cpu_availability[0].max();
          .           				}
         32 ( 0.00%)  				prev = cpu_availability[0].max() + ceil((double)prev / (double)m);
          .           
          .           				// for some reason i need to have this print otherwise i have a seg fault?!?
          .           				// std::cout<<"\t seg fault prevention?!?"<<std::endl;
          .           				//  initialize the chigh values for the first iterative step
         38 ( 0.00%)  				for (int Chigh_i = 0; Chigh_i < m; Chigh_i++)
          .           				{
          .           					// std::cout<<cpu_availability[Chigh_i].max()<<" - ";
         20 ( 0.00%)  					Chigh[Chigh_i] = cpu_availability[Chigh_i].max() - s;
          .           					// std::cout<<Chigh[Chigh_i]<<std::endl;
          .           				}
          .           				// std::cout<<"\tChihgh Filled"<<std::endl;
     10,031 ( 0.00%)  				for (const Job<Time> *j_x : jobs_by_latest_arrival)
          .           				{
          .           
      6,478 ( 0.00%)  					if (j_x->latest_arrival() <= event && j_x->latest_arrival() > last_event)
          .           					{
          .           						// std::cout << j_x->get_id() << std::endl;
      3,971 ( 0.00%)  						if (j_x->maximal_cost() > Chigh[0])
          .           						{
          .           							if (Chigh[0] > 0)
      2,196 ( 0.00%)  								Clow += Chigh[0];
          .           								
        732 ( 0.00%)  							Chigh[0] = j_x->maximal_cost();
          .           							Time swap;
          .           							// its in this point i think,
     10,155 ( 0.00%)  							for (int i = 0; i < m - 1; i++)
          .           							{
      8,496 ( 0.00%)  								if (Chigh[i] > Chigh[i + 1])
          .           								{
          .           									swap = Chigh[i];
      7,684 ( 0.00%)  									Chigh[i] = Chigh[i + 1];
          .           									Chigh[i + 1] = swap;
          .           								}
          .           								else
          .           								{
          .           									break;
          .           								}
          .           							}
          .           						}
          .           						else
          .           						{
        732 ( 0.00%)  							Clow += j_x->maximal_cost();
          .           						}
          .           
      2,928 ( 0.00%)  						if (Chigh[0] > 0)
          .           						{
          .           							//maybe remove CHigh[0]? why is that there??
        650 ( 0.00%)  							event = s + ceil((double)(Clow + Chigh[0]) / (double)m);
          .           						}
          .           						else
          .           						{
          .           							event = s;
          .           						}
          .           					}
          .           					else
          .           					{
          .           						//its out the bounds
          .           						// std::cout << "\t new iteration" << std::endl;
          .           						last_event = event;
      3,550 ( 0.00%)  						if (Chigh[0] > 0)
          .           						{
          .           							prev = Clow;
        425 ( 0.00%)  							for (int i = 0; i < m; i++)
          .           							{
        400 ( 0.00%)  								prev += (Chigh[i] >= 0) ? Chigh[i] : 0;
          .           							}
        651 ( 0.00%)  							prev = s + ceil((double)prev / (double(m)));
          .           						}
          .           						// std::cout << "prev" << prev << std::endl;
          .           						s = j_x->latest_arrival();
          .           
     33,251 ( 0.00%)  						for (int i = 0; i < m; i++)
          .           						{
          .           
     21,300 ( 0.00%)  							if (Chigh[i] > 0)
          .           							{
      9,229 ( 0.00%)  								Chigh[i] = (0 > ((event + Chigh[i]) - s)) ? 0 : ((event + Chigh[i]) - s);
          .           							}
          .           							else
          .           							{
      7,574 ( 0.00%)  								Chigh[i] = prev - s;
          .           							}
          .           							// std::cout << Chigh[i] << " ";
          .           						}
          .           
          .           						//just in case.. have it sorted, can also have a linear check to see if it needs to be sorted tho
          .           						std::sort(Chigh, Chigh+m);
          .           						// std::cout << std::endl;
          .           						//  reset Clow
          .           						Clow = 0;
     14,200 ( 0.00%)  						if (j_x->maximal_cost() > Chigh[0])
          .           						{
          .           							Clow += Chigh[0];
      1,775 ( 0.00%)  							Chigh[0] = j_x->maximal_cost();
          .           							Time swap = 0;
     24,970 ( 0.00%)  							for (int i = 0; i < m - 1; i++)
          .           							{
     23,075 ( 0.00%)  								if (Chigh[i] > Chigh[i + 1])
          .           								{
          .           									swap = Chigh[i];
     19,088 ( 0.00%)  									Chigh[i] = Chigh[i + 1];
          .           									Chigh[i + 1] = swap;
          .           								}
          .           								else
          .           								{
          .           									break;
          .           								}
          .           							}
          .           						}
-- line 1426 ----------------------------------------
-- line 1427 ----------------------------------------
          .           						else
          .           						{
          .           							Clow += j_x->maximal_cost();
          .           						}
          .           						event = s;
          .           					}
          .           				}
          .           				//i think thus should be sorted
          5 ( 0.00%)  				std::sort(Chigh, Chigh+m);
          .           				//std::cout <<"s: " << s << "CLow" << Clow << " Chigh: ";
          .           				//for(int i = 0; i < m; i ++){
          .           				//	std::cout << Chigh[i] << " ";
          .           				//}
          .           				//std::cout << std::endl;
          .           				certainly_available_i(Chigh, Clow, s, &CA_values);
          .           				return CA_values;
          9 ( 0.00%)  			}
          .           
          .           			void possibly_available_pre_filter(int *LFP, int i)
          .           			{
          .           				// create an empty temporary set
          .           				Job_set J_temp{};
          3 ( 0.00%)  				int m = cpu_availability.size();
          .           
          .           				// Initialize the LFP array
          6 ( 0.00%)  				for (int j = 0; j < m; j++)
          .           				{
         18 ( 0.00%)  					LFP[j] = 0; // Set a default value here, or you can choose any appropriate initial value
         33 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (3x)
          .           				}
          .           				// std::cout << "\t PA_" << i << " for " << m << " cores i have " << jobs_by_rmax_cmin.size() << " jobs to use" << std::endl;
         70 ( 0.00%)  				for (auto it_x = jobs_by_rmax_cmin.rbegin(); it_x != jobs_by_rmax_cmin.rend(); it_x++)
          .           				{
         70 ( 0.00%)  					const Job<Time> *j_x = *it_x;
          .           
         85 ( 0.00%)  					if (J_temp.size() < m - i)
          .           					{
          .           						// remove all elementes from J_temp that exceed the latest finish time of the new job j_x as long as J_temp is not full
         64 ( 0.00%)  						Time newLFP = j_x->latest_arrival() + j_x->minimal_cost();
          .           						// std::cout << "\tnew LFP to add is " << newLFP << std::endl;
          .           						J_temp.erase(std::remove_if(J_temp.begin(),
          .           													J_temp.end(),
          .           													[&newLFP](const Job<Time> *j_y)
          .           													{ return j_y->latest_arrival() > newLFP; }),
          .           									 J_temp.end());
          .           						// afterwards we just add j_x
         99 ( 0.00%)  						J_temp.emplace_back(j_x);
      2,088 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::emplace_back<NP::Job<long long> const*&>(NP::Job<long long> const*&) (32x)
          .           					}
          .           					else
          .           					{
          .           						break;
          .           					}
          .           				}
          .           				// Now that J_temp is filled, we can calculate the m-i latest finish points using only m-i cores
          .           				int LFP_i = 0;
          .           				// std::cout << "\tComputed " << J_temp.size() << " values for the LFP are" << std::endl;
         51 ( 0.00%)  				for (const Job<Time> *j_t : J_temp)
          .           				{
         18 ( 0.00%)  					LFP[LFP_i] = j_t->latest_arrival() + j_t->minimal_cost();
          .           					LFP_i++;
          .           				}
          .           			}
          .           
          .           			Time possibly_available_i(int *LFP, int i)
          .           			{
          .           				int m = cpu_availability.size();
          .           				// std::cout << m << "-" << i << "-" << 2 << "=" << m - i - 2 << std::endl;
         24 ( 0.00%)  				int Cmax[m - 1 - i];
          3 ( 0.00%)  				int Crest = 0;
          .           				// just a really small sort, but can be improved upon. so TODO
         12 ( 0.00%)  				std::sort(LFP, LFP + (m - i));
          .           				// This is the latest end point.
          3 ( 0.00%)  				int LEP = LFP[m - i - 1];
          6 ( 0.00%)  				if (m - i - 1 > 0)
          .           				{
          .           					// create the initial Cmax values;
         23 ( 0.00%)  					for (int Cmax_i = 0; Cmax_i < m - i - 1; Cmax_i++)
          .           					{
         13 ( 0.00%)  						Cmax[Cmax_i] = LEP - LFP[Cmax_i];
          .           					}
          .           				}
          .           				// std::cout<<"init CMax"<<std::endl;
          .           
     15,051 ( 0.00%)  				for (auto it_s = jobs_by_rmin_cmin.rbegin(); it_s != jobs_by_rmin_cmin.rend(); it_s++)
          .           				{
      7,521 ( 0.00%)  					const Job<Time> *j_s = *it_s;
     15,048 ( 0.00%)  					if (j_s->latest_arrival() < LFP[m - i - 1])
          .           					{
          .           						// we should also check if A^max is less than that value i think.
    224,859 ( 0.01%)  						if (j_s->earliest_arrival() + j_s->minimal_cost() > LEP - ceil((double)Crest / (double)(m - i)))
          .           						{
          .           							// here we have to use the i'th core
          .           							// std::cout << "\tWe have to use the i^th core at time " << j_s->earliest_arrival() + j_s->minimal_cost() << std::endl;
          .           							return j_s->earliest_arrival() + j_s->minimal_cost();
          .           						}
          .           						// we cannot use Cmax if we have only one core to fill
     14,990 ( 0.00%)  						if (m - i - 1 > 0)
          .           						{
     14,979 ( 0.00%)  							if (j_s->minimal_cost() < Cmax[m - i - 2])
          .           							{
          .           								Crest += j_s->minimal_cost();
          .           							}
          .           							else
          .           							{
          .           								// insert it into the Cmax array
          .           								// we know for certain that Cmax[m-i-2] has to be added to Crest
        544 ( 0.00%)  								Crest += Cmax[m - i - 2];
        544 ( 0.00%)  								Cmax[m - i - 2] = j_s->minimal_cost();
          .           								Time swap;
      2,720 ( 0.00%)  								for (int Cmax_i = m - i - 2; Cmax_i > 0; Cmax_i--)
          .           								{
      1,355 ( 0.00%)  									if (Cmax[Cmax_i - 1] < Cmax[i])
          .           									{
          .           										swap = Cmax[Cmax_i - 1];
          2 ( 0.00%)  										Cmax[Cmax_i - 1] = Cmax[Cmax_i];
          1 ( 0.00%)  										Cmax[Cmax_i] = swap;
          .           									}
          .           									else
          .           									{
          .           										break;
          .           									}
          .           								}
          .           							}
          .           						}
          .           						else
          .           						{
      6,951 ( 0.00%)  							Crest += j_s->minimal_cost();
          .           						}
          .           					}
          .           				}
         18 ( 0.00%)  				return cpu_availability[i - 1].min();
          .           			}
          .           
          .           			Time possibly_available_m()
          .           			{
          .           				Time Ctot = 0;
          .           				Time maxEnd = 0;
          1 ( 0.00%)  				Time least_start_time = jobs[0]->earliest_arrival();
          .           
     10,030 ( 0.00%)  				for (const Job<Time> *j : jobs)
          .           				{
      7,518 ( 0.00%)  					maxEnd = (maxEnd < j->earliest_arrival() + j->minimal_cost()) ? j->earliest_arrival() + j->minimal_cost() : maxEnd;
      7,518 ( 0.00%)  					Ctot += j->minimal_cost();
          .           					least_start_time = std::min(least_start_time, j->earliest_arrival());
          .           				}
          .           				// this returns 0 + that value so the further we go the worse this bound gets
          .           				// We have two options,  or we do it by setting the value to Amin + bound
          .           				//						or we determine the earliest time that A job can start in this set and then take the maximum between that and the Amin
         33 ( 0.00%)  				return std::max(cpu_availability[0].min(), least_start_time) + (ceil((double)Ctot / (double)cpu_availability.size()) > maxEnd) ? ceil((double)Ctot / (double)cpu_availability.size()) : maxEnd;
          .           			}
          .           
         10 ( 0.00%)  			std::vector<Time> compute_possibly_available()
          .           			{
          3 ( 0.00%)  				int LFP[cpu_availability.size()];
          3 ( 0.00%)  				std::vector<Time> PA_values{};
         22 ( 0.00%)  				for (int i = 1; i < cpu_availability.size(); i++)
          .           				{
          .           					possibly_available_pre_filter(LFP, i);
          3 ( 0.00%)  					PA_values.push_back(possibly_available_i(LFP, i));
          .           				}
          1 ( 0.00%)  				PA_values.push_back(possibly_available_m());
          .           
          .           				return PA_values;
          9 ( 0.00%)  			}
          .           
          .           			void compute_new_system_states()
          .           			{
          .           				std::cout << "\tCores are Possibly available at times: ";
          .           				for (Time PA : compute_possibly_available())
          .           				{
          .           					std::cout << PA << " ";
          .           				}
-- line 1597 ----------------------------------------
-- line 1629 ----------------------------------------
          .           				std::cout << "Where the new system states would be:" << std::endl;
          .           				compute_new_system_states();
          .           			}
          .           		};
          .           
          .           		template <typename T, typename Compare>
          .           		typename std::vector<T>::iterator insert_sorted(std::vector<T> &vec, const T &item, Compare comp)
          .           		{
     37,485 ( 0.00%)  			return vec.insert(std::upper_bound(vec.begin(), vec.end(), item, comp), item);
    381,157 ( 0.01%)  => /usr/include/c++/10/bits/vector.tcc:std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::insert(__gnu_cxx::__normal_iterator<NP::Job<long long> const* const*, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, NP::Job<long long> const* const&) (9,996x)
          .           		}
          .           
          .           		template <class Time>
          6 ( 0.00%)  		class Reduction_set_statistics
          .           		{
          .           
          .           		public:
          .           			bool reduction_success;
          .           
          .           			unsigned long num_jobs, num_interfering_jobs_added;
          .           
          .           			std::vector<Time> priorities;
          .           
          9 ( 0.00%)  			Reduction_set_statistics(bool reduction_success, Reduction_set<Time> &reduction_set)
          .           				: reduction_success{reduction_success},
          .           				  num_jobs{reduction_set.get_jobs().size()},
          .           				  num_interfering_jobs_added{reduction_set.get_num_interfering_jobs_added()},
          4 ( 0.00%)  				  priorities{}
          .           			{
      7,523 ( 0.00%)  				for (const Job<Time> *j : reduction_set.get_jobs())
          .           				{
          .           					priorities.push_back(j->get_priority());
          .           				}
          8 ( 0.00%)  			}
          .           		};
          .           
          .           	}
          .           
          .           };
          .           
          .           #endif
--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_algo.h
--------------------------------------------------------------------------------
Ir                 

-- line 829 ----------------------------------------
        .           
        .             template<typename _ForwardIterator, typename _Predicate>
        .               _GLIBCXX20_CONSTEXPR
        .               _ForwardIterator
        .               __remove_if(_ForwardIterator __first, _ForwardIterator __last,
        .           		_Predicate __pred)
        .               {
        .                 __first = std::__find_if(__first, __last, __pred);
       44 ( 0.00%)        if (__first == __last)
        .           	return __first;
        .                 _ForwardIterator __result = __first;
        .                 ++__first;
       60 ( 0.00%)        for (; __first != __last; ++__first)
        8 ( 0.00%)  	if (!__pred(__first))
        .           	  {
        .           	    *__result = _GLIBCXX_MOVE(*__first);
        .           	    ++__result;
        .           	  }
        .                 return __result;
        .               }
        .           
        .             /**
-- line 850 ----------------------------------------
-- line 1818 ----------------------------------------
        .               void
        .               __unguarded_linear_insert(_RandomAccessIterator __last,
        .           			      _Compare __comp)
        .               {
        .                 typename iterator_traits<_RandomAccessIterator>::value_type
        .           	__val = _GLIBCXX_MOVE(*__last);
        .                 _RandomAccessIterator __next = __last;
        .                 --__next;
7,697,650 ( 0.21%)        while (__comp(__val, __next))
        .           	{
        5 ( 0.00%)  	  *__last = _GLIBCXX_MOVE(*__next);
        .           	  __last = __next;
        .           	  --__next;
        .           	}
3,846,162 ( 0.11%)        *__last = _GLIBCXX_MOVE(__val);
        .               }
        .           
        .             /// This is a helper function for the sort routine.
        .             template<typename _RandomAccessIterator, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               void
       28 ( 0.00%)      __insertion_sort(_RandomAccessIterator __first,
        .           		     _RandomAccessIterator __last, _Compare __comp)
        .               {
1,280,272 ( 0.04%)        if (__first == __last) return;
        .           
5,154,898 ( 0.14%)        for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i)
        .           	{
3,851,508 ( 0.11%)  	  if (__comp(__i, __first))
        .           	    {
        .           	      typename iterator_traits<_RandomAccessIterator>::value_type
        .           		__val = _GLIBCXX_MOVE(*__i);
        .           	      _GLIBCXX_MOVE_BACKWARD3(__first, __i, __i + 1);
        5 ( 0.00%)  	      *__first = _GLIBCXX_MOVE(__val);
        .           	    }
        .           	  else
        .           	    std::__unguarded_linear_insert(__i,
        .           				__gnu_cxx::__ops::__val_comp_iter(__comp));
        .           	}
       24 ( 0.00%)      }
        .           
        .             /// This is a helper function for the sort routine.
        .             template<typename _RandomAccessIterator, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               inline void
        .               __unguarded_insertion_sort(_RandomAccessIterator __first,
        .           			       _RandomAccessIterator __last, _Compare __comp)
        .               {
-- line 1865 ----------------------------------------
-- line 1873 ----------------------------------------
        .              *  This controls some aspect of the sort routines.
        .             */
        .             enum { _S_threshold = 16 };
        .           
        .             /// This is a helper function for the sort routine.
        .             template<typename _RandomAccessIterator, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               void
5,761,188 ( 0.16%)      __final_insertion_sort(_RandomAccessIterator __first,
        .           			   _RandomAccessIterator __last, _Compare __comp)
        .               {
1,290,936 ( 0.04%)        if (__last - __first > int(_S_threshold))
        .           	{
        2 ( 0.00%)  	  std::__insertion_sort(__first, __first + int(_S_threshold), __comp);
        .           	  std::__unguarded_insertion_sort(__first + int(_S_threshold), __last,
        .           					  __comp);
        .           	}
        .                 else
       16 ( 0.00%)  	std::__insertion_sort(__first, __last, __comp);
      194 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#4}>) [clone .isra.0] (1x)
      187 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#3}>) [clone .isra.0] (1x)
      149 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#1}>) [clone .isra.0] (1x)
      125 ( 0.00%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__insertion_sort<__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}> >(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}>, __gnu_cxx::__ops::_Iter_comp_iter<NP::Global::Reduction_set<long long>::Reduction_set(std::vector<Interval<long long>, std::allocator<Interval<long long> > >, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > const&, std::vector<unsigned long, std::allocator<unsigned long> >&, std::vector<unsigned long, std::allocator<unsigned long> >)::{lambda(NP::Job<long long> const*, NP::Job<long long> const*)#2}>) [clone .isra.0] (1x)
5,121,056 ( 0.14%)      }
        .           
        .             /// This is a helper function...
        .             template<typename _RandomAccessIterator, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               _RandomAccessIterator
        .               __unguarded_partition(_RandomAccessIterator __first,
        .           			  _RandomAccessIterator __last,
        .           			  _RandomAccessIterator __pivot, _Compare __comp)
-- line 1900 ----------------------------------------
-- line 1941 ----------------------------------------
        .             /// This is a helper function for the sort routine.
        .             template<typename _RandomAccessIterator, typename _Size, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               void
        .               __introsort_loop(_RandomAccessIterator __first,
        .           		     _RandomAccessIterator __last,
        .           		     _Size __depth_limit, _Compare __comp)
        .               {
1,287,388 ( 0.04%)        while (__last - __first > int(_S_threshold))
        .           	{
        .           	  if (__depth_limit == 0)
        .           	    {
        .           	      std::__partial_sort(__first, __last, __last, __comp);
        .           	      return;
        .           	    }
        .           	  --__depth_limit;
        .           	  _RandomAccessIterator __cut =
        .           	    std::__unguarded_partition_pivot(__first, __last, __comp);
        .           	  std::__introsort_loop(__cut, __last, __depth_limit, __comp);
        .           	  __last = __cut;
        .           	}
        1 ( 0.00%)      }
        .           
        .             // sort
        .           
        .             template<typename _RandomAccessIterator, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               inline void
        .               __sort(_RandomAccessIterator __first, _RandomAccessIterator __last,
        .           	   _Compare __comp)
        .               {
1,283,835 ( 0.04%)        if (__first != __last)
        .           	{
2,572,987 ( 0.07%)  	  std::__introsort_loop(__first, __last,
3,200,650 ( 0.09%)  => /usr/include/c++/10/bits/stl_iterator.h:void std::__introsort_loop<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, long, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (640,130x)
       15 ( 0.00%)  				std::__lg(__last - __first) * 2,
        .           				__comp);
1,920,396 ( 0.05%)  	  std::__final_insertion_sort(__first, __last, __comp);
44,809,100 ( 1.25%)  => /usr/include/c++/10/bits/stl_algo.h:void std::__final_insertion_sort<__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter>(__gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__normal_iterator<long long*, std::vector<long long, std::allocator<long long> > >, __gnu_cxx::__ops::_Iter_less_iter) [clone .isra.0] (640,130x)
        .           	}
        .               }
        .           
        .             template<typename _RandomAccessIterator, typename _Size, typename _Compare>
        .               _GLIBCXX20_CONSTEXPR
        .               void
        .               __introselect(_RandomAccessIterator __first, _RandomAccessIterator __nth,
        .           		  _RandomAccessIterator __last, _Size __depth_limit,
-- line 1985 ----------------------------------------
-- line 2048 ----------------------------------------
        .               __upper_bound(_ForwardIterator __first, _ForwardIterator __last,
        .           		  const _Tp& __val, _Compare __comp)
        .               {
        .                 typedef typename iterator_traits<_ForwardIterator>::difference_type
        .           	_DistanceType;
        .           
        .                 _DistanceType __len = std::distance(__first, __last);
        .           
  208,788 ( 0.01%)        while (__len > 0)
        .           	{
  191,148 ( 0.01%)  	  _DistanceType __half = __len >> 1;
        .           	  _ForwardIterator __middle = __first;
        .           	  std::advance(__middle, __half);
  167,820 ( 0.00%)  	  if (__comp(__val, __middle))
        .           	    __len = __half;
        .           	  else
        .           	    {
        .           	      __first = __middle;
        .           	      ++__first;
  186,444 ( 0.01%)  	      __len = __len - __half - 1;
        .           	    }
        .           	}
        .                 return __first;
        .               }
        .           
        .             /**
        .              *  @brief Finds the last position in which @p __val could be inserted
        .              *         without changing the ordering.
-- line 2075 ----------------------------------------
-- line 4306 ----------------------------------------
        .               {
        .                 // concept requirements
        .                 __glibcxx_function_requires(_InputIteratorConcept<_InputIterator>)
        .                 __glibcxx_function_requires(_OutputIteratorConcept<_OutputIterator,
        .           	    // "the type returned by a _UnaryOperation"
        .           	    __typeof__(__unary_op(*__first))>)
        .                 __glibcxx_requires_valid_range(__first, __last);
        .           
      135 ( 0.00%)        for (; __first != __last; ++__first, (void)++__result)
      180 ( 0.00%)  	*__result = __unary_op(*__first);
      832 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
      473 ( 0.00%)  => ./ctype/ctype.c:tolower (43x)
        .                 return __result;
        .               }
        .           
        .             /**
        .              *  @brief Perform an operation on corresponding elements of two sequences.
        .              *  @ingroup mutating_algorithms
        .              *  @param  __first1     An input iterator.
        .              *  @param  __last1      An input iterator.
-- line 4323 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/vector.tcc
--------------------------------------------------------------------------------
Ir                  

-- line 101 ----------------------------------------
         .           #if __cplusplus >= 201103L
         .             template<typename _Tp, typename _Alloc>
         .               template<typename... _Args>
         .           #if __cplusplus > 201402L
         .                 typename vector<_Tp, _Alloc>::reference
         .           #else
         .                 void
         .           #endif
40,971,172 ( 1.14%)        vector<_Tp, _Alloc>::
         .                 emplace_back(_Args&&... __args)
         .                 {
16,053,487 ( 0.45%)  	if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
         .           	  {
         .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
         .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
         .           				     std::forward<_Args>(__args)...);
 2,985,730 ( 0.08%)  	    ++this->_M_impl._M_finish;
         .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
         .           	  }
         .           	else
    43,450 ( 0.00%)  	  _M_realloc_insert(end(), std::forward<_Args>(__args)...);
 6,936,557 ( 0.19%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<std::reference_wrapper<NP::Job<long long> const>, std::allocator<std::reference_wrapper<NP::Job<long long> const> > >::_M_realloc_insert<std::reference_wrapper<NP::Job<long long> const> >(__gnu_cxx::__normal_iterator<std::reference_wrapper<NP::Job<long long> const>*, std::vector<std::reference_wrapper<NP::Job<long long> const>, std::allocator<std::reference_wrapper<NP::Job<long long> const> > > >, std::reference_wrapper<NP::Job<long long> const>&&) (19,717x)
         .           #if __cplusplus > 201402L
         .           	return back();
         .           #endif
40,971,154 ( 1.14%)        }
         .           #endif
         .           
         .             template<typename _Tp, typename _Alloc>
         .               typename vector<_Tp, _Alloc>::iterator
    69,972 ( 0.00%)      vector<_Tp, _Alloc>::
         .           #if __cplusplus >= 201103L
         .               insert(const_iterator __position, const value_type& __x)
         .           #else
         .               insert(iterator __position, const value_type& __x)
         .           #endif
         .               {
         .                 const size_type __n = __position - begin();
    29,988 ( 0.00%)        if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
    19,920 ( 0.00%)  	if (__position == end())
         .           	  {
         .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
         .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
         .           				     __x);
    18,786 ( 0.00%)  	    ++this->_M_impl._M_finish;
         .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
         .           	  }
         .           	else
         .           	  {
         .           #if __cplusplus >= 201103L
         .           	    const auto __pos = begin() + (__position - cbegin());
         .           	    // __x could be an existing element of this vector, so make a
         .           	    // copy of it before _M_insert_aux moves elements around.
-- line 152 ----------------------------------------
-- line 153 ----------------------------------------
         .           	    _Temporary_value __x_copy(this, __x);
         .           	    _M_insert_aux(__pos, std::move(__x_copy._M_val()));
         .           #else
         .           	    _M_insert_aux(__position, __x);
         .           #endif
         .           	  }
         .                 else
         .           #if __cplusplus >= 201103L
        72 ( 0.00%)  	_M_realloc_insert(begin() + (__position - cbegin()), __x);
   124,815 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::_M_realloc_insert<NP::Job<long long> const* const&>(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, NP::Job<long long> const* const&) (36x)
         .           #else
         .           	_M_realloc_insert(__position, __x);
         .           #endif
         .           
       284 ( 0.00%)        return iterator(this->_M_impl._M_start + __n);
    68,802 ( 0.00%)      }
         .           
         .             template<typename _Tp, typename _Alloc>
         .               typename vector<_Tp, _Alloc>::iterator
         .               vector<_Tp, _Alloc>::
         .               _M_erase(iterator __position)
         .               {
 1,353,916 ( 0.04%)        if (__position + 1 != end())
         .           	_GLIBCXX_MOVE3(__position + 1, end(), __position);
 2,635,221 ( 0.07%)        --this->_M_impl._M_finish;
         .                 _Alloc_traits::destroy(this->_M_impl, this->_M_impl._M_finish);
         .                 _GLIBCXX_ASAN_ANNOTATE_SHRINK(1);
         .                 return __position;
         .               }
         .           
         .             template<typename _Tp, typename _Alloc>
         .               typename vector<_Tp, _Alloc>::iterator
         .               vector<_Tp, _Alloc>::
         .               _M_erase(iterator __first, iterator __last)
         .               {
         8 ( 0.00%)        if (__first != __last)
         .           	{
         .           	  if (__last != end())
         .           	    _GLIBCXX_MOVE3(__last, end(), __first);
         .           	  _M_erase_at_end(__first.base() + (end() - __last));
         .           	}
         .                 return __first;
         .               }
         .           
-- line 195 ----------------------------------------
-- line 399 ----------------------------------------
         .               void
         .               vector<_Tp, _Alloc>::
         .               _M_insert_aux(iterator __position, const _Tp& __x)
         .           #endif
         .               {
         .                 _GLIBCXX_ASAN_ANNOTATE_GROW(1);
         .                 _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
         .           			       _GLIBCXX_MOVE(*(this->_M_impl._M_finish - 1)));
     1,134 ( 0.00%)        ++this->_M_impl._M_finish;
         .                 _GLIBCXX_ASAN_ANNOTATE_GREW(1);
         .           #if __cplusplus < 201103L
         .                 _Tp __x_copy = __x;
         .           #endif
     1,134 ( 0.00%)        _GLIBCXX_MOVE_BACKWARD3(__position.base(),
         .           			      this->_M_impl._M_finish - 2,
         .           			      this->_M_impl._M_finish - 1);
         .           #if __cplusplus < 201103L
         .                 *__position = __x_copy;
         .           #else
     1,134 ( 0.00%)        *__position = std::forward<_Arg>(__arg);
         .           #endif
         .               }
         .           
         .           #if __cplusplus >= 201103L
         .             template<typename _Tp, typename _Alloc>
         .               template<typename... _Args>
         .                 void
 4,026,805 ( 0.11%)        vector<_Tp, _Alloc>::
         .                 _M_realloc_insert(iterator __position, _Args&&... __args)
         .           #else
         .             template<typename _Tp, typename _Alloc>
         .               void
         .               vector<_Tp, _Alloc>::
         .               _M_realloc_insert(iterator __position, const _Tp& __x)
         .           #endif
         .               {
-- line 434 ----------------------------------------
-- line 441 ----------------------------------------
         .                 pointer __new_finish(__new_start);
         .                 __try
         .           	{
         .           	  // The order of the three operations is dictated by the C++11
         .           	  // case, where the moves could alter a new element belonging
         .           	  // to the existing vector.  This is an issue only for callers
         .           	  // taking the element by lvalue ref (see last bullet of C++11
         .           	  // [res.on.arguments]).
        30 ( 0.00%)  	  _Alloc_traits::construct(this->_M_impl,
         .           				   __new_start + __elems_before,
         .           #if __cplusplus >= 201103L
         .           				   std::forward<_Args>(__args)...);
         .           #else
         .           				   __x);
         .           #endif
         .           	  __new_finish = pointer();
         .           
         .           #if __cplusplus >= 201103L
         .           	  if _GLIBCXX17_CONSTEXPR (_S_use_relocate())
         .           	    {
         .           	      __new_finish = _S_relocate(__old_start, __position.base(),
         .           					 __new_start, _M_get_Tp_allocator());
         .           
 3,862,720 ( 0.11%)  	      ++__new_finish;
         .           
         .           	      __new_finish = _S_relocate(__position.base(), __old_finish,
         .           					 __new_finish, _M_get_Tp_allocator());
         .           	    }
         .           	  else
         .           #endif
         .           	    {
         .           	      __new_finish
         .           		= std::__uninitialized_move_if_noexcept_a
         .           		(__old_start, __position.base(),
         .           		 __new_start, _M_get_Tp_allocator());
         .           
     2,658 ( 0.00%)  	      ++__new_finish;
         .           
         .           	      __new_finish
         .           		= std::__uninitialized_move_if_noexcept_a
         .           		(__position.base(), __old_finish,
         .           		 __new_finish, _M_get_Tp_allocator());
         .           	    }
         .           	}
         .                 __catch(...)
-- line 485 ----------------------------------------
-- line 493 ----------------------------------------
         .           	  __throw_exception_again;
         .           	}
         .           #if __cplusplus >= 201103L
         .                 if _GLIBCXX17_CONSTEXPR (!_S_use_relocate())
         .           #endif
         .           	std::_Destroy(__old_start, __old_finish, _M_get_Tp_allocator());
         .                 _GLIBCXX_ASAN_ANNOTATE_REINIT;
         .                 _M_deallocate(__old_start,
 5,164,891 ( 0.14%)  		    this->_M_impl._M_end_of_storage - __old_start);
15,464,208 ( 0.43%)        this->_M_impl._M_start = __new_start;
         .                 this->_M_impl._M_finish = __new_finish;
11,618,855 ( 0.32%)        this->_M_impl._M_end_of_storage = __new_start + __len;
   181,488 ( 0.01%)      }
         .           
         .             template<typename _Tp, typename _Alloc>
         .               void
         .               vector<_Tp, _Alloc>::
         .               _M_fill_insert(iterator __position, size_type __n, const value_type& __x)
         .               {
         .                 if (__n != 0)
         .           	{
-- line 513 ----------------------------------------
-- line 816 ----------------------------------------
         .                 this->_M_deallocate();
         .                 this->_M_impl._M_start = __start;
         .                 this->_M_impl._M_finish = __finish;
         .                 this->_M_impl._M_end_of_storage = __q + _S_nword(__n);
         .               }
         .           
         .             template<typename _Alloc>
         .               void
    25,310 ( 0.00%)      vector<bool, _Alloc>::
         .               _M_fill_insert(iterator __position, size_type __n, bool __x)
         .               {
     5,062 ( 0.00%)        if (__n == 0)
         .           	return;
     7,593 ( 0.00%)        if (capacity() - size() >= __n)
         .           	{
         .           	  std::copy_backward(__position, end(),
         .           			     this->_M_impl._M_finish + difference_type(__n));
         .           	  std::fill(__position, __position + difference_type(__n), __x);
         .           	  this->_M_impl._M_finish += difference_type(__n);
         .           	}
         .                 else
         .           	{
-- line 837 ----------------------------------------
-- line 838 ----------------------------------------
         .           	  const size_type __len = 
         .           	    _M_check_len(__n, "vector<bool>::_M_fill_insert");
         .           	  _Bit_pointer __q = this->_M_allocate(__len);
         .           	  iterator __start(std::__addressof(*__q), 0);
         .           	  iterator __i = _M_copy_aligned(begin(), __position, __start);
         .           	  std::fill(__i, __i + difference_type(__n), __x);
         .           	  iterator __finish = std::copy(__position, end(),
         .           					__i + difference_type(__n));
        33 ( 0.00%)  	  this->_M_deallocate();
     1,226 ( 0.00%)  => /usr/include/c++/10/bits/stl_bvector.h:std::_Bvector_base<std::allocator<bool> >::_M_deallocate() (11x)
        33 ( 0.00%)  	  this->_M_impl._M_end_of_storage = __q + _S_nword(__len);
        22 ( 0.00%)  	  this->_M_impl._M_start = __start;
        44 ( 0.00%)  	  this->_M_impl._M_finish = __finish;
         .           	}
    20,248 ( 0.00%)      }
         .           
         .             template<typename _Alloc>
         .               template<typename _ForwardIterator>
         .                 void
         .                 vector<bool, _Alloc>::
         .                 _M_insert_range(iterator __position, _ForwardIterator __first, 
         .           		      _ForwardIterator __last, std::forward_iterator_tag)
         .                 {
-- line 859 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: include/global/por_space.hpp
--------------------------------------------------------------------------------
Ir              

-- line 28 ----------------------------------------
     .           
     .           namespace NP
     .           {
     .           
     .           	namespace Global
     .           	{
     .           
     .           		template <class Time, class IIP = Null_IIP<Time>, class POR_criterion = POR_criterion<Time>>
     6 ( 0.00%)  		class Por_state_space : public State_space<Time, IIP>
1,708,693 ( 0.05%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::~State_space() (1x)
15,201 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::~vector() (1x)
     .           		{
     .           
     .           		public:
     .           			typedef typename State_space<Time, IIP>::Problem Problem;
     .           			typedef typename State_space<Time, IIP>::Workload Workload;
     .           			// typedef typename State_space<Time, IIP>::Abort_actions Abort_actions;
     .           			typedef typename State_space<Time, IIP>::State State;
     .           			typedef typename State_space<Time, IIP>::Job_precedence_set Job_precedence_set;
-- line 44 ----------------------------------------
-- line 47 ----------------------------------------
     .           				const Problem &prob,
     .           				const Analysis_options &opts)
     .           			{
     .           				DM("!! MULTI PROCESSOR POR SAG !!" << std::endl);
     .           				// this is a multiprocessor analysis
     .           				assert(prob.num_processors > 1);
     .           
     .           				// Preprocess the job such that they release at or after their predecessors release
     5 ( 0.00%)  				auto jobs = preprocess_jobs<Time>(prob.dag, prob.jobs);
3,125,192 ( 0.09%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/precedence.hpp:NP::Job<long long>::Job_set NP::preprocess_jobs<long long>(std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > > const&, NP::Job<long long>::Job_set const&) (1x)
     .           
     .           				Por_state_space s = Por_state_space(jobs, prob.dag, prob.num_processors,
     .           													opts.timeout, opts.max_depth,
     .           													opts.num_buckets);
     2 ( 0.00%)  				s.group_add = opts.group_add;
     2 ( 0.00%)  				s.limit_fail = opts.limit_fail;
     .           				s.cpu_time.start();
     2 ( 0.00%)  				if (opts.be_naive)
     .           				{
     .           					DM("\texploring naively" << std::endl);
     .           					s.explore_naively();
     .           				}
     .           				else
     3 ( 0.00%)  					s.explore();
3,549,977,181 (99.08%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::explore() (1x)
     .           				s.cpu_time.stop();
     .           				return s;
     .           			}
     .           
     .           			// convenience interface for tests
     .           			static Por_state_space explore_naively(const Workload &jobs)
     .           			{
     .           				Problem p{jobs};
-- line 77 ----------------------------------------
-- line 95 ----------------------------------------
     .           
     .           			unsigned long number_of_por_failures() const
     .           			{
     .           				return reduction_failures;
     .           			}
     .           
     .           			unsigned long number_of_jobs_in_por() const
     .           			{
     1 ( 0.00%)  				unsigned long jobs_in_por = 0;
     5 ( 0.00%)  				for (Reduction_set_statistics<Time> rss : reduction_set_statistics)
     .           				{
     2 ( 0.00%)  					if (rss.reduction_success)
     .           					{
     1 ( 0.00%)  						jobs_in_por += rss.num_jobs;
     .           					}
     .           				}
     .           				return jobs_in_por;
     .           			}
     .           
     .           			std::vector<Reduction_set_statistics<Time>> get_reduction_set_statistics() const
     .           			{
     .           				return reduction_set_statistics;
-- line 116 ----------------------------------------
-- line 129 ----------------------------------------
     .           			// Normal state space: jobs, dag_edges, num_cpus, max_cpu_time, max_depth, num_buckets
     .           			Por_state_space(const Workload &jobs,
     .           							const Precedence_constraints &dag_edges,
     .           							unsigned int num_cpus,
     .           							double max_cpu_time = 0,
     .           							unsigned int max_depth = 0,
     .           							std::size_t num_buckets = 1000,
     .           							bool early_exit = true)
    16 ( 0.00%)  				: State_space<Time, IIP>(jobs, dag_edges, num_cpus, max_cpu_time, max_depth, num_buckets), por_criterion(), reduction_successes(0), reduction_failures(0), reduction_set_statistics(), job_precedence_sets(jobs.size())
11,709,177 ( 0.33%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::State_space(std::vector<NP::Job<long long>, std::allocator<NP::Job<long long> > > const&, std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > > const&, unsigned int, double, unsigned int, unsigned long) (1x)
 8,462 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::vector<unsigned long, std::allocator<unsigned long> >, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > >::vector(unsigned long, std::allocator<std::vector<unsigned long, std::allocator<unsigned long> > > const&) [clone .constprop.0] (1x)
     .           			{
     2 ( 0.00%)  				for (auto e : dag_edges)
     .           				{
     .           					const Job<Time> &from = lookup<Time>(jobs, e.first);
     .           					const Job<Time> &to = lookup<Time>(jobs, e.second);
     .           					job_precedence_sets[index_of(to, jobs)].push_back(index_of(from, jobs));
     .           				}
     .           			}
     .           
     .           			// entrance: this creates the reduction set
    12 ( 0.00%)  			Reduction_set<Time> create_reduction_set(const State &s, typename Reduction_set<Time>::Job_set &eligible_successors)
     .           			{
     .           				std::vector<std::size_t> indices{};
    28 ( 0.00%)  				for (const Job<Time> *j : eligible_successors)
     .           				{
     8 ( 0.00%)  					indices.push_back(this->index_of(*j));
     .           				}
     .           				// okie nu heb ik een basis reductie set
     .           				//========================Creating reduction set===========================//
     5 ( 0.00%)  				Reduction_set<Time> reduction_set = Reduction_set<Time>(s.get_all_core_availabilities(), eligible_successors, indices);
   165 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (1x)
     .           
     .           				const Job<Time> *j;
     .           				Time lb = s.core_availability().min();
     .           				while (true)
     .           				{
     .           					if (reduction_set.has_potential_deadline_misses())
     .           					{
     .           						reduction_failures++;
-- line 165 ----------------------------------------
-- line 182 ----------------------------------------
     .           						if (reduction_set.can_interfere(*j))
     .           						{
     .           							interfering_jobs.push_back(j);
     .           						}
     .           					}*/
     .           
     .           					// i aint using the macros as they require a lot of additions to the states etc
     .           					std::vector<const Job<Time> *> interfering_jobs{};
   509 ( 0.00%)  					bool updated = false;
     .           					// adding the lower bound here does work, but it was not really that much of a time save
15,776 ( 0.00%)  					for (auto it = this->jobs_by_earliest_arrival.lower_bound(lb); it != this->jobs_by_earliest_arrival.upper_bound(reduction_set.get_latest_LST()); it++)
     .           					{
10,012 ( 0.00%)  						j = it->second;
     .           						const Job<Time> &j_i = *j;
     .           						const Job_precedence_set &preds = this->job_precedence_sets[this->index_of(j_i)];
     .           						if (reduction_set.can_interfere(*j) && s.job_incomplete(this->index_of(j_i)))
     .           						{
     .           							if (!updated)
     .           							{
     .           								lb = j->earliest_arrival();
     .           								updated = true;
     .           							}
     .           							interfering_jobs.push_back(j);
     .           						}
     .           					}
     .           
     .           					// Now we have a (possible) set of interfering jobs
 1,527 ( 0.00%)  					if (!interfering_jobs.empty())
     .           					{
     .           						// if we have at least one element in it, we must select it to add it to the redution set.
     .           						// This must be done under a criteria, these criteria now are the same as for uniproc, but may change in the future
     .           						// We also must push this criterion to its own hpp file
     .           
 1,525 ( 0.00%)  						if (!this->group_add)
     .           						{
     .           							const Job<Time> *jx = *std::min_element(interfering_jobs.begin(), interfering_jobs.end(),
     .           																	[](const Job<Time> *i, const Job<Time> *j) -> bool
     .           																	{
     .           																		return i->higher_priority_than(*j);
     .           																	});
     .           							reduction_set.add_job(jx, this->index_of(*jx));
     .           							reduction_set.update_set();
     .           						}
     .           						else
     .           						{
10,504 ( 0.00%)  							for (const Job<Time> *j_i : interfering_jobs)
     .           							{
 4,998 ( 0.00%)  								reduction_set.add_job(j_i, this->index_of(*j_i));
3,586,179 ( 0.10%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::add_job(NP::Job<long long> const*, unsigned long) (2,499x)
     .           							}
     .           							reduction_set.update_set();
     .           						}
     .           					}
     .           					else
     .           					{
     6 ( 0.00%)  						reduction_set_statistics.push_back(Reduction_set_statistics<Time>{true, reduction_set});
96,363 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set_statistics<long long>::Reduction_set_statistics(bool, NP::Global::Reduction_set<long long>&) (1x)
     2 ( 0.00%)  						reduction_successes++;
     2 ( 0.00%)  						jobs_in_por += reduction_set.get_number_of_jobs();
     .           						// no more interfering jobs so we can return the reduction set.
     .           						// std::cout << " interference took in total " << interfering_total << " ns " << std::endl;
     .           						// reduction_set.get_timings();
     .           						return reduction_set;
     .           					}
     .           				}
     9 ( 0.00%)  			}
     .           
     9 ( 0.00%)  			void process_new_edge(
     .           				const State &from,
     .           				const State &to,
     .           				const Reduction_set<Time> &reduction_set,
     .           				const Interval<Time> &finish_range)
     .           			{
     .           				// update response times
15,045 ( 0.00%)  				for (const Job<Time> *j : reduction_set.get_jobs())
     .           				{
     .           					this->update_finish_times(*j, Interval<Time>{reduction_set.earliest_finish_time(*j), reduction_set.latest_finish_time(*j)});
     .           				}
     .           				// update statistics
     1 ( 0.00%)  				this->num_edges++;
     .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
     .           				this->edges.push_back(std::make_unique<Reduced_edge>(reduction_set, &from, &to, finish_range));
     .           #endif
     8 ( 0.00%)  			}
     .           
    10 ( 0.00%)  			void dispatch_reduction_set_merge(const State &current_state, Reduction_set<Time> &reduction_set)
     .           			{
     .           				std::vector<std::size_t> indices{};
 7,526 ( 0.00%)  				for (const Job<Time> *j : reduction_set.get_jobs())
     .           				{
 2,507 ( 0.00%)  					indices.push_back(this->index_of(*j));
     .           				}
     .           
     4 ( 0.00%)  				std::vector<Time> PA = reduction_set.compute_possibly_available();
354,679 ( 0.01%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_possibly_available() (1x)
     4 ( 0.00%)  				std::vector<Time> CA = reduction_set.compute_certainly_available();
370,564 ( 0.01%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::compute_certainly_available() (1x)
     .           
     2 ( 0.00%)  				const State &next = this->new_or_merged_state(current_state, indices, PA, CA, reduction_set.get_key());
     .           
     3 ( 0.00%)  				process_new_edge(current_state, next, reduction_set, {0, 0});
871,091 ( 0.02%)  => include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::process_new_edge(NP::Global::Schedule_state<long long> const&, NP::Global::Schedule_state<long long> const&, NP::Global::Reduction_set<long long> const&, Interval<long long> const&) [clone .isra.0] (1x)
     8 ( 0.00%)  			}
     .           
     .           			void dispatch_reduction_set_naive(const State &current_state, Reduction_set<Time> &reduction_set)
     .           			{
     .           
     .           				// Interval<Time> finish_range = next_finish_times(reduction_set);
     .           
     .           				std::vector<std::size_t> indices{};
     .           				for (const Job<Time> *j : reduction_set.get_jobs())
-- line 287 ----------------------------------------
-- line 332 ----------------------------------------
     .           #endif
     .           				this->count_edge();
     .           				this->current_job_count++;
     .           
     .           				return true;
     .           			}
     .           
     .           			// Here we explore our current state where we also try to make our reduction set, this is why this is placed in this file
    22 ( 0.00%)  			void explore(const State &s) override
     .           			{
     .           				bool found_one = false;
     .           
     .           				DM("----" << std::endl);
     .           
     .           				// (0) define the window of interest
     .           
     .           				// earliest time a core is possibly available
     .           				auto t_min = s.core_availability().min();
     .           				// latest time some unfinished job is certainly ready
     4 ( 0.00%)  				auto t_job = this->next_job_ready(s, t_min);
109,468 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/space.hpp:NP::Global::State_space<long long, NP::Global::Null_IIP<long long> >::next_job_ready(NP::Global::Schedule_state<long long> const&, long long) const (2x)
     .           				// latest time some core is certainly available
     .           				auto t_core = s.core_availability().max();
     .           				// latest time by which a work-conserving scheduler
     .           				// certainly schedules some job
     .           				auto t_wc = std::max(t_core, t_job);
     .           
     .           				DM(s << std::endl
     .           					 << "t_min: " << t_min << std::endl
-- line 359 ----------------------------------------
-- line 373 ----------------------------------------
     .           				eligible successors are:
     .           				1. all jobs that can start before t_min, aka. A_min
     .           				2. all jobs that can start before the first job in the first set is CERTAINLY released
     .           				*/
     .           				// so first all jobs that might be pending before A_min
     .           
     .           				// (1) first check jobs that may be already pending
     .           				DM("==== [1] ====" << std::endl);
     2 ( 0.00%)  				bool skip_set = true;
    37 ( 0.00%)  				for (const Job<Time> &j : this->jobs_by_win.lookup(t_min))
     .           				{
    20 ( 0.00%)  					if (j.earliest_arrival() <= t_min && this->ready(s, j))
     .           					{
     5 ( 0.00%)  						eligible_successors.push_back(&j);
     .           						if (!s.job_in_failed_set(this->index_of(j)))
     .           						{
     5 ( 0.00%)  							skip_set = false;
     .           						}
     .           					}
     .           				}
     .           
     .           				DM("==== [2] ====" << std::endl);
     .           				// (2) check jobs that are released only later in the interval
     4 ( 0.00%)  				for (auto it = this->jobs_by_earliest_arrival.upper_bound(t_min);
 5,014 ( 0.00%)  					 it != this->jobs_by_earliest_arrival.end();
     .           					 it++)
     .           				{
 2,506 ( 0.00%)  					const Job<Time> &j = *it->second;
     .           					DM(j << " (" << index_of(j) << ")" << std::endl);
     .           					// stop looking once we've left the window of interest
 7,518 ( 0.00%)  					if (j.earliest_arrival() > t_wc)
     .           						break;
     .           
     .           					// Job could be not ready due to precedence constraints
     .           					if (!this->ready(s, j))
     .           						continue;
     .           
     .           					// Since this job is released in the future, it better
     .           					// be incomplete...
-- line 411 ----------------------------------------
-- line 414 ----------------------------------------
     .           					// Hier gaan we dispatchen dus moet er erna gemerged worden
     .           					// Dispatch vanaf de huidige state s, job j, met een worst-case computation time t_wc
     .           					// found_one |= dispatch(s, j, t_wc);
     .           					// now we dont dispatch, but we create the eligible successors so just add it to that set
     .           					eligible_successors.push_back(it->second);
     .           
     .           					if (!s.job_in_failed_set(this->index_of(j)))
     .           					{
     6 ( 0.00%)  						skip_set = false;
     .           					}
     .           				}
     .           				// now we can finially create a proper reduction set
     .           
     .           				// look if all eligible successors are present in the previously failed set
     .           				// if so, lets not make a reduction
     .           
     .           				// if the size is 1 then just do a normal schedule
     .           
     .           				std::vector<std::size_t> failed_reduction;
     .           				// if we skip making this set, then keep looking at the previous set in the next state
     4 ( 0.00%)  				bool limit_failures = this->limit_fail;
     4 ( 0.00%)  				if (skip_set && limit_failures)
     .           				{
     .           					failed_reduction = s.get_previous_failed_set();
     .           				}
     .           				if(!limit_failures){
     .           					skip_set = false;
     .           				}
     4 ( 0.00%)  				if (eligible_successors.size() > 1 && !skip_set)
     .           				{
     6 ( 0.00%)  					Reduction_set<Time> reduction_set = create_reduction_set(s, eligible_successors);
3,546,986,621 (99.00%)  => include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::create_reduction_set(NP::Global::Schedule_state<long long> const&, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >&) (1x)
     .           
     .           					if (!reduction_set.has_potential_deadline_misses())
     .           					{
     .           						DM("\n---\nPartial-order reduction is safe" << std::endl);
     .           						// uncomment to print the CA and PA values
     .           						//reduction_set.created_set();
     .           						//  now we must create something to properly schedule the set
     .           						// reduction_set.show_time_waste();
     3 ( 0.00%)  						if (this->be_naive)
     .           						{
     2 ( 0.00%)  							dispatch_reduction_set_naive(s, reduction_set);
     .           						}
     .           						else
     .           						{
     2 ( 0.00%)  							dispatch_reduction_set_merge(s, reduction_set);
1,807,910 ( 0.05%)  => include/global/por_space.hpp:NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> >::dispatch_reduction_set_merge(NP::Global::Schedule_state<long long> const&, NP::Global::Reduction_set<long long>&) (1x)
     .           						}
     .           						// this->current_job_count += reduction_set.get_jobs().size();
     .           						found_one = true;
     .           						// if there were no deadline misses and we were able to dispatch it normally, then we can return here.
     2 ( 0.00%)  						return;
957,231 ( 0.03%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/global/reduction_set.hpp:NP::Global::Reduction_set<long long>::~Reduction_set() (1x)
     .           					}
     .           					else
     .           					{
     .           						DM("\tPartial order reduction is not safe" << std::endl);
     .           						if (limit_failures)
     .           						{
     .           							for (const Job<Time> *j : reduction_set.get_jobs())
     .           							{
     .           								failed_reduction.push_back(this->index_of(*j));
     .           							}
     .           						}
     .           					}
     .           				}
     .           
     .           				DM("\n---\nPartial-order reduction is not safe, or just one job" << std::endl);
     2 ( 0.00%)  				for (const Job<Time> *j : eligible_successors)
     .           				{
     .           					// we can use the normal dispatch now so no worries
     .           					// maybe we can give some information to the next state if we have a failed reduction, in such a way that it wont try a next reduction
     .           					//  if that next job was in the failed set.
     .           					found_one |= dispatch(s, *j, t_wc, failed_reduction);
     .           				}
     .           
     .           				// check for a dead end
     2 ( 0.00%)  				if (!found_one && !this->all_jobs_scheduled(s))
     .           				{
     .           					// out of options and we didn't schedule all jobs
     .           					DM("POR dead end abortion" << std::endl);
     .           					this->aborted = true;
     .           				}
    16 ( 0.00%)  			}
     .           
     .           			Time earliest_possible_job_release(const State &s, const Reduction_set<Time> &ignored_set)
     .           			{
     .           				DM("      - looking for earliest possible job release starting from: "
     .           				   << s.earliest_job_release() << std::endl);
     .           				const Job<Time> *jp;
     .           				foreach_possibly_pending_job(s, jp)
     .           				{
-- line 503 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: src/nptest.cpp
--------------------------------------------------------------------------------
Ir          

-- line 34 ----------------------------------------
 .           static bool want_group;
 .           static bool want_limit_fail;
 .           static bool want_prm_iip;
 .           static bool want_cw_iip;
 .           static bool want_priority_por;
 .           static bool want_release_por;
 .           
 .           static bool want_precedence = false;
 6 ( 0.00%)  static std::string precedence_file;
76 ( 0.00%)  => ./stdlib/cxa_atexit.c:__cxa_atexit (1x)
 .           
 .           static bool want_aborts = false;
 5 ( 0.00%)  static std::string aborts_file;
76 ( 0.00%)  => ./stdlib/cxa_atexit.c:__cxa_atexit (1x)
 .           
 .           static bool want_multiprocessor = false;
 .           static unsigned int num_processors = 1;
 .           
 .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
 .           static bool want_dot_graph;
 .           #endif
 .           static double timeout;
-- line 53 ----------------------------------------
-- line 56 ----------------------------------------
 .           static bool want_rta_file;
 .           
 .           static bool continue_after_dl_miss = false;
 .           
 .           #ifdef CONFIG_PARALLEL
 .           static unsigned int num_worker_threads = 0;
 .           #endif
 .           
12 ( 0.00%)  struct Analysis_result {
 .           	bool schedulable;
 .           	bool timeout;
 .           	unsigned long number_of_states, number_of_edges, max_width, number_of_jobs;
 .           	double cpu_time;
 .           	std::string graph;
 .           	std::string response_times_csv;
 .           	unsigned long por_successes, por_failures, jobs_per_por;
 .           };
 .           
 .           template<class Time, class Space>
10 ( 0.00%)  static Analysis_result analyze(
 .           	std::istream &in,
 .           	std::istream &dag_in,
 .           	std::istream &aborts_in)
 .           {
 .           #ifdef CONFIG_PARALLEL
 .           	tbb::task_scheduler_init init(
 .           		num_worker_threads ? num_worker_threads : tbb::task_scheduler_init::automatic);
 .           #endif
 .           
 .           	// Parse input files and create NP scheduling problem description
10 ( 0.00%)  	NP::Scheduling_problem<Time> problem{
107,674 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/problem.hpp:NP::Scheduling_problem<long long>::Scheduling_problem(std::vector<NP::Job<long long>, std::allocator<NP::Job<long long> > >, std::vector<std::pair<NP::JobID, NP::JobID>, std::allocator<std::pair<NP::JobID, NP::JobID> > >, std::vector<NP::Abort_action<long long>, std::allocator<NP::Abort_action<long long> > >, unsigned int) (1x)
154 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/problem.hpp:NP::Scheduling_problem<long long>::~Scheduling_problem() (1x)
 3 ( 0.00%)  		NP::parse_file<Time>(in),
13,183,670 ( 0.37%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:NP::Job<long long>::Job_set NP::parse_file<long long>(std::istream&) (1x)
 4 ( 0.00%)  		NP::parse_dag_file(dag_in),
177 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:NP::parse_dag_file(std::istream&) (1x)
 5 ( 0.00%)  		NP::parse_abort_file<Time>(aborts_in),
169 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/include/io.hpp:std::vector<NP::Abort_action<long long>, std::allocator<NP::Abort_action<long long> > > NP::parse_abort_file<long long>(std::istream&) (1x)
 .           		num_processors};
 .           
 .           	// Set common analysis options
 .           	NP::Analysis_options opts;
 1 ( 0.00%)  	opts.timeout = timeout;
 1 ( 0.00%)  	opts.max_depth = max_depth;
 .           	opts.early_exit = !continue_after_dl_miss;
 .           	opts.num_buckets = problem.jobs.size();
 2 ( 0.00%)  	opts.be_naive = want_naive;
 2 ( 0.00%)  	opts.group_add = want_group;
 2 ( 0.00%)  	opts.limit_fail = want_limit_fail;
 .           
 .           	// Actually call the analysis engine
 .           	auto space = Space::explore(problem, opts);
 .           
 .           	// Extract the analysis results
 6 ( 0.00%)  	auto graph = std::ostringstream();
1,938 ( 0.00%)  => ???:std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream() (1x)
84 ( 0.00%)  => ???:std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream() (1x)
 .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
 .           	if (want_dot_graph)
 .           		graph << space;
 .           #endif
 .           
10 ( 0.00%)  	auto rta = std::ostringstream();
1,938 ( 0.00%)  => ???:std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream() (1x)
1,303 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
 .           
 2 ( 0.00%)  	if (want_rta_file) {
 .           		rta << "Task ID, Job ID, BCCT, WCCT, BCRT, WCRT" << std::endl;
 .           		for (const auto& j : problem.jobs) {
 .           			Interval<Time> finish = space.get_finish_times(j);
 .           			rta << j.get_task_id() << ", "
 .           			    << j.get_job_id() << ", "
 .           			    << finish.from() << ", "
 .           			    << finish.until() << ", "
 .           			    << std::max<long long>(0,
-- line 122 ----------------------------------------
-- line 134 ----------------------------------------
 .           		space.max_exploration_front_width(),
 .           		problem.jobs.size(),
 .           		space.get_cpu_time(),
 .           		graph.str(),
 .           		rta.str(),
 .           		space.number_of_por_successes(),
 .           		space.number_of_por_failures(),
 .           		space.number_of_jobs_in_por()
13 ( 0.00%)  	};
 9 ( 0.00%)  }
 .           
 .           static Analysis_result process_stream(
 .           	std::istream &in,
 .           	std::istream &dag_in,
 .           	std::istream &aborts_in)
 2 ( 0.00%)  {
 8 ( 0.00%)  	if (!want_dense && want_priority_por && want_multiprocessor)
 3 ( 0.00%)  		return analyze<dtime_t, NP::Global::Por_state_space<dtime_t, NP::Global::Null_IIP<dtime_t>, NP::Global::POR_priority_order<dtime_t>>>(in, dag_in, aborts_in);
3,579,863,189 (99.92%)  => src/nptest.cpp:Analysis_result analyze<long long, NP::Global::Por_state_space<long long, NP::Global::Null_IIP<long long>, NP::Global::POR_priority_order<long long> > >(std::istream&, std::istream&, std::istream&) (1x)
 1 ( 0.00%)  	else if (!want_dense && want_release_por && want_multiprocessor)
 .           		return analyze<dtime_t, NP::Global::Por_state_space<dtime_t, NP::Global::Null_IIP<dtime_t>, NP::Global::POR_release_order<dtime_t>>>(in, dag_in, aborts_in);
 .           	else if (want_dense && want_priority_por && want_multiprocessor)
 .           		return analyze<dense_t, NP::Global::Por_state_space<dense_t, NP::Global::Null_IIP<dense_t>, NP::Global::POR_priority_order<dense_t>>>(in, dag_in, aborts_in);
 .           	else if (want_dense && want_release_por && want_multiprocessor)
 .           		return analyze<dense_t, NP::Global::Por_state_space<dense_t, NP::Global::Null_IIP<dense_t>, NP::Global::POR_release_order<dense_t>>>(in, dag_in, aborts_in);
 .           	else if (want_multiprocessor && want_dense)
 .           		return analyze<dense_t, NP::Global::State_space<dense_t>>(in, dag_in, aborts_in);
 .           	else if (want_multiprocessor && !want_dense)
-- line 160 ----------------------------------------
-- line 174 ----------------------------------------
 .           	else if (!want_dense && want_cw_iip)
 .           		return analyze<dtime_t, NP::Uniproc::State_space<dtime_t, NP::Uniproc::Critical_window_IIP<dtime_t>>>(in, dag_in, aborts_in);
 .           	else if (!want_dense && want_priority_por)
 .           		return analyze<dtime_t, NP::Uniproc::Por_state_space<dtime_t, NP::Uniproc::Null_IIP<dtime_t>, NP::Uniproc::POR_priority_order<dtime_t>>>(in, dag_in, aborts_in);
 .           	else if (!want_dense && want_release_por)
 .           		return analyze<dtime_t, NP::Uniproc::Por_state_space<dtime_t, NP::Uniproc::Null_IIP<dtime_t>, NP::Uniproc::POR_release_order<dtime_t>>>(in, dag_in, aborts_in);
 .           	else
 .           		return analyze<dtime_t, NP::Uniproc::State_space<dtime_t>>(in, dag_in, aborts_in);
 3 ( 0.00%)  }
 .           
 .           static void process_file(const std::string& fname)
 8 ( 0.00%)  {
 .           	try {
 .           		Analysis_result result;
 .           
11 ( 0.00%)  		auto empty_dag_stream = std::istringstream("\n");
2,006 ( 0.00%)  => ???:std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::basic_istringstream(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::_Ios_Openmode) (1x)
43 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
10 ( 0.00%)  		auto empty_aborts_stream = std::istringstream("\n");
2,006 ( 0.00%)  => ???:std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::basic_istringstream(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::_Ios_Openmode) (1x)
43 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 9 ( 0.00%)  		auto dag_stream = std::ifstream();
7,247 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
 5 ( 0.00%)  		auto aborts_stream = std::ifstream();
2,204 ( 0.00%)  => ???:std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream() (1x)
 .           
 2 ( 0.00%)  		if (want_precedence)
 .           			dag_stream.open(precedence_file);
 .           
 2 ( 0.00%)  		if (want_aborts)
 .           			aborts_stream.open(aborts_file);
 .           
 3 ( 0.00%)  		std::istream &dag_in = want_precedence ?
 .           			static_cast<std::istream&>(dag_stream) :
 .           			static_cast<std::istream&>(empty_dag_stream);
 .           
 1 ( 0.00%)  		std::istream &aborts_in = want_aborts ?
 .           			static_cast<std::istream&>(aborts_stream) :
 .           			static_cast<std::istream&>(empty_aborts_stream);
 .           
 2 ( 0.00%)  		if (fname == "-")
 .           			result = process_stream(std::cin, dag_in, aborts_in);
 .           		else {
 .           			auto in = std::ifstream(fname, std::ios::in);
 7 ( 0.00%)  			result = process_stream(in, dag_in, aborts_in);
3,579,863,206 (99.92%)  => src/nptest.cpp:process_stream(std::istream&, std::istream&, std::istream&) (1x)
 .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
 .           			if (want_dot_graph) {
 .           				std::string dot_name = fname;
 .           				auto p = dot_name.find(".csv");
 .           				if (p != std::string::npos) {
 .           					dot_name.replace(p, std::string::npos, ".dot");
 .           					auto out  = std::ofstream(dot_name,  std::ios::out);
 .           					out << result.graph;
 .           					out.close();
 .           				}
 .           			}
 .           #endif
 2 ( 0.00%)  			if (want_rta_file) {
 .           				std::string rta_name = fname;
 .           				auto p = rta_name.find(".csv");
 .           				if (p != std::string::npos) {
 .           					rta_name.replace(p, std::string::npos, ".rta.csv");
 .           					auto out  = std::ofstream(rta_name,  std::ios::out);
 .           					out << result.response_times_csv;
 .           					out.close();
 .           				}
-- line 233 ----------------------------------------
-- line 234 ----------------------------------------
 .           			}
 .           		}
 .           
 .           #ifdef _WIN32 // rusage does not work under Windows
 .           		long mem_used = 0;
 .           #else
 .           		struct rusage u;
 .           		long mem_used = 0;
 8 ( 0.00%)  		if (getrusage(RUSAGE_SELF, &u) == 0)
868 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
 3 ( 0.00%)  			mem_used = u.ru_maxrss;
 .           #endif
 .           
 .           		std::cout << fname;
 .           
 3 ( 0.00%)  		if (max_depth && max_depth < result.number_of_jobs)
 .           			// mark result as invalid due to debug abort
 .           			std::cout << ",  X";
 .           		else
 8 ( 0.00%)  			std::cout << ",  " << (int) result.schedulable;
5,358 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave (1x)
 .           
 .           		std::cout << ",  " << result.number_of_jobs
 .           		          << ",  " << result.number_of_states
 .           		          << ",  " << result.number_of_edges
 .           		          << ",  " << result.max_width
 .           		          << ",  " << std::fixed << result.cpu_time
 3 ( 0.00%)  		          << ",  " << ((double) mem_used) / (1024.0)
 .           		          << ",  " << (int) result.timeout
 5 ( 0.00%)  		          << ",  " << num_processors
376 ( 0.00%)  => ???:std::ostream::operator<<(int) (1x)
 .           		          << ",  " << result.por_successes
 .           		          << ",  " << result.por_failures
 .           				  << ",  " << result.jobs_per_por
 .           		          << std::endl;
 .           	} catch (std::ios_base::failure& ex) {
 .           		std::cerr << fname;
 .           		if (want_precedence)
 .           			std::cerr << " + " << precedence_file;
-- line 269 ----------------------------------------
-- line 281 ----------------------------------------
 .           		          << ex.ref.job << " of task " << ex.ref.task
 .           			      << " has an impossible abort time (abort before release)"
 .           			      << std::endl;
 .           		exit(4);
 .           	} catch (std::exception& ex) {
 .           		std::cerr << fname << ": '" << ex.what() << "'" << std::endl;
 .           		exit(1);
 .           	}
 8 ( 0.00%)  }
 .           
 .           static void print_header(){
 .           	std::cout << "# file name"
 .           	          << ", schedulable?"
 .           	          << ", #jobs"
 .           	          << ", #states"
 .           	          << ", #edges"
 .           	          << ", max width"
-- line 297 ----------------------------------------
-- line 301 ----------------------------------------
 .           	          << ", #CPUs"
 .           	          << ", #POR successes"
 .           	          << ", #POR failures"
 .           	          << ", #total jobs in POR"
 .           	          << std::endl;
 .           }
 .           
 .           int main(int argc, char** argv)
 9 ( 0.00%)  {
 6 ( 0.00%)  	auto parser = optparse::OptionParser();
18,317 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::OptionParser::~OptionParser() (1x)
135 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionParser::OptionParser() (1x)
 .           
 4 ( 0.00%)  	parser.description("Exact NP Schedulability Tester");
3,753 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	parser.usage("usage: %prog [OPTIONS]... [JOB SET FILES]...");
997 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
20 ( 0.00%)  	parser.add_option("-t", "--time").dest("time_model")
11,518 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
198 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 4 ( 0.00%)  	      .metavar("TIME-MODEL")
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
16 ( 0.00%)  	      .choices({"dense", "discrete"}).set_default("discrete")
13,535 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
130 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
636 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (1x)
 4 ( 0.00%)  	      .help("choose 'discrete' or 'dense' time (default: discrete)");
205 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
14 ( 0.00%)  	parser.add_option("-l", "--time-limit").dest("timeout")
5,439 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
198 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 3 ( 0.00%)  	      .help("maximum CPU time allowed (in seconds, zero means no limit)")
205 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("0");
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
14 ( 0.00%)  	parser.add_option("-d", "--depth-limit").dest("depth")
3,329 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
198 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 3 ( 0.00%)  	      .help("abort graph exploration after reaching given depth (>= 2)")
205 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("0");
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
20 ( 0.00%)  	parser.add_option("-n", "--naive").dest("naive").set_default("0")
4,509 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
200 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
10 ( 0.00%)  	      .action("store_const").set_const("1")
107 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
145 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .help("use the naive exploration method (default: merging)");
201 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
15 ( 0.00%)  	parser.add_option("-i", "--iip").dest("iip")
3,406 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
202 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
32 ( 0.00%)  	      .choices({"none", "P-RM", "CW"}).set_default("none")
2,401 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
200 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
859 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (1x)
 3 ( 0.00%)  	      .help("the IIP to use (default: none)");
193 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
14 ( 0.00%)  	parser.add_option("-p", "--precedence").dest("precedence_file")
3,503 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
196 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 3 ( 0.00%)  	      .help("name of the file that contains the job set's precedence DAG")
201 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("");
3,342 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
14 ( 0.00%)  	parser.add_option("-a", "--abort-actions").dest("abort_file")
3,375 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
196 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 3 ( 0.00%)  	      .help("name of the file that contains the job set's abort actions")
201 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("");
2,219 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
14 ( 0.00%)  	parser.add_option("-m", "--multiprocessor").dest("num_processors")
3,871 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
325 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
 3 ( 0.00%)  	      .help("set the number of processors of the platform")
211 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("1");
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
10 ( 0.00%)  	parser.add_option("--threads").dest("num_threads")
128 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
2,524 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .help("set the number of worker threads (parallel analysis)")
215 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 3 ( 0.00%)  	      .set_default("0");
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
11 ( 0.00%)  	parser.add_option("--header").dest("print_header")
128 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
2,395 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .help("print a column header")
193 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
10 ( 0.00%)  	      .action("store_const").set_const("1")
107 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
145 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .set_default("0");
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
 .           
20 ( 0.00%)  	parser.add_option("-g", "--save-graph").dest("dot").set_default("0")
3,636 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
200 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
10 ( 0.00%)  	      .action("store_const").set_const("1")
107 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
145 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .help("store the state graph in Graphviz dot format (default: off)");
205 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
20 ( 0.00%)  	parser.add_option("-r", "--save-response-times").dest("rta").set_default("0")
5,174 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
329 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
10 ( 0.00%)  	      .action("store_const").set_const("1")
107 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
145 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 3 ( 0.00%)  	      .help("store the best- and worst-case response times (default: off)");
205 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
13 ( 0.00%)  	parser.add_option("-c", "--continue-after-deadline-miss")
5,416 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
261 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
 7 ( 0.00%)  	      .dest("go_on_after_dl").set_default("0")
2,340 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
10 ( 0.00%)  	      .action("store_const").set_const("1")
107 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
145 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::Option::action(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 .           	      .help("do not abort the analysis on the first deadline miss "
 3 ( 0.00%)  	            "(default: off)");
226 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
10 ( 0.00%)  	parser.add_option("--por").dest("por")
134 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
2,373 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
28 ( 0.00%)  		  .choices({"none", "priority", "release"}).set_default("none")
2,401 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
196 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (3x)
845 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (1x)
 3 ( 0.00%)  		  .help("the type of partial-order reduction to use (default: none)");
215 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
10 ( 0.00%)  	parser.add_option("--interfering").dest("interfering")
128 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
2,338 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
14 ( 0.00%)  		  .choices({"one", "all"}).set_default("one")
2,405 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
136 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
662 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (1x)
 3 ( 0.00%)  		  .help("only works with --por, add one or all possible interfering jobs (default: one)");
226 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
10 ( 0.00%)  	parser.add_option("--limit_fail").dest("limit_fail")
128 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
2,488 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionContainer::add_option(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
14 ( 0.00%)  		  .choices({"yes", "no"}).set_default("no")
2,404 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option& optparse::Option::set_default<char const*>(char const*) (1x)
136 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (2x)
662 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Option::choices(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >) (1x)
 3 ( 0.00%)  		  .help("only works with --por, limits failures by not forming new sets with jobs in the latest rejected set (default: one)");
1,023 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           
 .           
 9 ( 0.00%)  	auto options = parser.parse_args(argc, argv);
59,689 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/src/OptionParser.cpp:optparse::OptionParser::parse_args(int, char const* const*) (1x)
4,182 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::Values(optparse::Values const&) (1x)
2,392 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::~Values() (1x)
 .           
 8 ( 0.00%)  	const std::string& time_model = options.get("time_model");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
2,519 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 1 ( 0.00%)  	want_dense = time_model == "dense";
 .           
 8 ( 0.00%)  	const std::string& iip = options.get("iip");
68 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
573 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 1 ( 0.00%)  	want_prm_iip = iip == "P-RM";
 1 ( 0.00%)  	want_cw_iip = iip == "CW";
 .           
 7 ( 0.00%)  	const std::string& por = options.get("por");
68 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
501 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 1 ( 0.00%)  	want_priority_por = por == "priority";
 1 ( 0.00%)  	want_release_por = por == "release";
 .           
 7 ( 0.00%)  	const std::string& interfering = options.get("interfering");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
553 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 1 ( 0.00%)  	want_group = interfering == "all";
 .           
 7 ( 0.00%)  	const std::string& limit_fail = options.get("limit_fail");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
577 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 1 ( 0.00%)  	want_limit_fail = limit_fail == "yes";
 .           
 .           
10 ( 0.00%)  	want_naive = options.get("naive");
9,562 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator bool() (1x)
66 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
525 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
 9 ( 0.00%)  	timeout = options.get("timeout");
66 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
445 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
10 ( 0.00%)  	max_depth = options.get("depth");
66 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
519 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
4,970 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator unsigned int() (1x)
 7 ( 0.00%)  	if (options.is_set_by_user("depth")) {
66 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           		if (max_depth <= 1) {
 .           			std::cerr << "Error: invalid depth argument\n" << std::endl;
 .           			return 1;
 .           		}
 .           		max_depth -= 1;
 .           	}
 .           
 6 ( 0.00%)  	want_precedence = options.is_set_by_user("precedence_file");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 2 ( 0.00%)  	if (want_precedence && parser.args().size() > 1) {
 .           		std::cerr << "[!!] Warning: multiple job sets "
 .           		          << "with a single precedence DAG specified."
 .           		          << std::endl;
 .           	}
 7 ( 0.00%)  	precedence_file = (const std::string&) options.get("precedence_file");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
227 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
 6 ( 0.00%)  	want_aborts = options.is_set_by_user("abort_file");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 2 ( 0.00%)  	if (want_aborts && parser.args().size() > 1) {
 .           		std::cerr << "[!!] Warning: multiple job sets "
 .           		          << "with a single abort action list specified."
 .           		          << std::endl;
 .           	}
 7 ( 0.00%)  	aborts_file = (const std::string&) options.get("abort_file");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
227 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
 4 ( 0.00%)  	want_multiprocessor = options.is_set_by_user("num_processors");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
10 ( 0.00%)  	num_processors = options.get("num_processors");
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
471 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
2,477 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator unsigned int() (1x)
 4 ( 0.00%)  	if (!num_processors || num_processors > MAX_PROCESSORS) {
 .           		std::cerr << "Error: invalid number of processors\n" << std::endl;
 .           		return 1;
 .           	}
 .           
 .           	/*
 .           	Andre: Not when i'm done with it
 .           	if ((want_priority_por || want_release_por) && num_processors > 1) {
 .           		std::cerr << "Error: partial-order reduction is currently only available for uniprocessor\n" << std::endl;
 .           		return 1;
 .           	}*/
 .           
 6 ( 0.00%)  	if ((want_priority_por || want_release_por) && (want_prm_iip || want_cw_iip)) {
 .           		std::cerr << "Error: partial-order reduction currently does not support idle-time insertion policies\n" << std::endl;
 .           		return 1;
 .           	}
 .           
10 ( 0.00%)  	want_rta_file = options.get("rta");
2,514 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator bool() (1x)
68 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
551 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
10 ( 0.00%)  	continue_after_dl_miss = options.get("go_on_after_dl");
2,514 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator bool() (1x)
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
615 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           
 .           #ifdef CONFIG_COLLECT_SCHEDULE_GRAPH
 .           	want_dot_graph = options.get("dot");
 .           #else
 5 ( 0.00%)  	if (options.is_set_by_user("dot")) {
68 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           		std::cerr << "Error: graph collection support must be enabled "
 .           		          << "during compilation (CONFIG_COLLECT_SCHEDULE_GRAPH "
 .           		          << "is not set)." << std::endl;
 .           		return 2;
 .           	}
 .           #endif
 .           
 .           #ifdef CONFIG_PARALLEL
 .           	num_worker_threads = options.get("num_threads");
 .           #else
 5 ( 0.00%)  	if (options.is_set_by_user("num_threads")) {
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
 .           		std::cerr << "Error: parallel analysis must be enabled "
 .           		          << "during compilation (CONFIG_PARALLEL "
 .           		          << "is not set)." << std::endl;
 .           		return 3;
 .           	}
 .           #endif
 .           
12 ( 0.00%)  	if (options.get("print_header"))
2,514 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Value::operator bool() (1x)
64 ( 0.00%)  => /usr/include/c++/10/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&) [clone .constprop.0] (1x)
543 ( 0.00%)  => /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/lib/include/OptionParser.h:optparse::Values::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (1x)
 .           		print_header();
 .           
 7 ( 0.00%)  	for (auto f : parser.args())
276 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~vector() (1x)
 2 ( 0.00%)  		process_file(f);
3,579,930,677 (99.92%)  => src/nptest.cpp:process_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (1x)
 .           
 4 ( 0.00%)  	if (parser.args().empty())
276 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~vector() (1x)
 .           		process_file("-");
 .           
 1 ( 0.00%)  	return 0;
11 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/ext/new_allocator.h
--------------------------------------------------------------------------------
Ir                  

-- line 74 ----------------------------------------
         .                 // 2103. propagate_on_container_move_assignment
         .                 typedef std::true_type propagate_on_container_move_assignment;
         .           #endif
         .           
         .                 _GLIBCXX20_CONSTEXPR
         .                 new_allocator() _GLIBCXX_USE_NOEXCEPT { }
         .           
         .                 _GLIBCXX20_CONSTEXPR
         2 ( 0.00%)        new_allocator(const new_allocator&) _GLIBCXX_USE_NOEXCEPT { }
         .           
         .                 template<typename _Tp1>
         .           	_GLIBCXX20_CONSTEXPR
         .           	new_allocator(const new_allocator<_Tp1>&) _GLIBCXX_USE_NOEXCEPT { }
         .           
         .           #if __cplusplus <= 201703L
     5,587 ( 0.00%)        ~new_allocator() _GLIBCXX_USE_NOEXCEPT { }
         .           
         .                 pointer
         .                 address(reference __x) const _GLIBCXX_NOEXCEPT
         .                 { return std::__addressof(__x); }
         .           
         .                 const_pointer
         .                 address(const_reference __x) const _GLIBCXX_NOEXCEPT
         .                 { return std::__addressof(__x); }
         .           #endif
         .           
         .                 // NB: __n is permitted to be 0.  The C++ standard says nothing
         .                 // about what the return value is when __n == 0.
         .                 _GLIBCXX_NODISCARD _Tp*
         .                 allocate(size_type __n, const void* = static_cast<const void*>(0))
         .                 {
 2,560,988 ( 0.07%)  	if (__n > this->_M_max_size())
         .           	  std::__throw_bad_alloc();
         .           
         .           #if __cpp_aligned_new
         .           	if (alignof(_Tp) > __STDCPP_DEFAULT_NEW_ALIGNMENT__)
         .           	  {
         .           	    std::align_val_t __al = std::align_val_t(alignof(_Tp));
         .           	    return static_cast<_Tp*>(::operator new(__n * sizeof(_Tp), __al));
         .           	  }
         .           #endif
39,405,902 ( 1.10%)  	return static_cast<_Tp*>(::operator new(__n * sizeof(_Tp)));
       228 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc_proxy/proxy.cpp:operator new(unsigned long) (2x)
         .                 }
         .           
         .                 // __p is not permitted to be a null pointer.
         .                 void
         .                 deallocate(_Tp* __p, size_type __t)
         .                 {
         .           #if __cpp_aligned_new
         .           	if (alignof(_Tp) > __STDCPP_DEFAULT_NEW_ALIGNMENT__)
-- line 123 ----------------------------------------
-- line 125 ----------------------------------------
         .           	    ::operator delete(__p,
         .           # if __cpp_sized_deallocation
         .           			      __t * sizeof(_Tp),
         .           # endif
         .           			      std::align_val_t(alignof(_Tp)));
         .           	    return;
         .           	  }
         .           #endif
15,499,347 ( 0.43%)  	::operator delete(__p
    59,068 ( 0.00%)  => ???:operator delete(void*, unsigned long) (512x)
         .           #if __cpp_sized_deallocation
         .           			  , __t * sizeof(_Tp)
         .           #endif
         .           			 );
         .                 }
         .           
         .           #if __cplusplus <= 201703L
         .                 size_type
         .                 max_size() const _GLIBCXX_USE_NOEXCEPT
         .                 { return _M_max_size(); }
         .           
         .           #if __cplusplus >= 201103L
         .                 template<typename _Up, typename... _Args>
         .           	void
        13 ( 0.00%)  	construct(_Up* __p, _Args&&... __args)
         .           	noexcept(std::is_nothrow_constructible<_Up, _Args...>::value)
10,287,741 ( 0.29%)  	{ ::new((void *)__p) _Up(std::forward<_Args>(__args)...); }
    20,299 ( 0.00%)  => /usr/include/c++/10/bits/stl_vector.h:std::vector<unsigned long, std::allocator<unsigned long> >::vector(std::vector<unsigned long, std::allocator<unsigned long> > const&) (1x)
         .           
         .                 template<typename _Up>
         .           	void
         .           	destroy(_Up* __p)
         .           	noexcept(std::is_nothrow_destructible<_Up>::value)
        71 ( 0.00%)  	{ __p->~_Up(); }
     1,297 ( 0.00%)  => /usr/include/c++/10/bits/stl_deque.h:std::deque<NP::Global::Schedule_state<long long>, std::allocator<NP::Global::Schedule_state<long long> > >::~deque() (3x)
         .           #else
         .                 // _GLIBCXX_RESOLVE_LIB_DEFECTS
         .                 // 402. wrong new expression in [some_] allocator::construct
         .                 void
         .                 construct(pointer __p, const _Tp& __val)
         .                 { ::new((void *)__p) _Tp(__val); }
         .           
         .                 void
-- line 164 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_uninitialized.h
--------------------------------------------------------------------------------
Ir                  

-- line 79 ----------------------------------------
         .             template<bool _TrivialValueTypes>
         .               struct __uninitialized_copy
         .               {
         .                 template<typename _InputIterator, typename _ForwardIterator>
         .                   static _ForwardIterator
         .                   __uninit_copy(_InputIterator __first, _InputIterator __last,
         .           		      _ForwardIterator __result)
         .                   {
        32 ( 0.00%)  	  _ForwardIterator __cur = __result;
         .           	  __try
         .           	    {
    73,260 ( 0.00%)  	      for (; __first != __last; ++__first, (void)++__cur)
         .           		std::_Construct(std::__addressof(*__cur), *__first);
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__result, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 98 ----------------------------------------
-- line 227 ----------------------------------------
         .                 template<typename _ForwardIterator, typename _Size, typename _Tp>
         .                   static _ForwardIterator
         .                   __uninit_fill_n(_ForwardIterator __first, _Size __n,
         .           			const _Tp& __x)
         .                   {
         .           	  _ForwardIterator __cur = __first;
         .           	  __try
         .           	    {
        12 ( 0.00%)  	      for (; __n > 0; --__n, (void) ++__cur)
         .           		std::_Construct(std::__addressof(*__cur), __x);
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__first, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 243 ----------------------------------------
-- line 558 ----------------------------------------
         .               {
         .                 template<typename _ForwardIterator, typename _Size>
         .                   static _ForwardIterator
         .                   __uninit_default_n(_ForwardIterator __first, _Size __n)
         .                   {
         .           	  _ForwardIterator __cur = __first;
         .           	  __try
         .           	    {
     7,560 ( 0.00%)  	      for (; __n > 0; --__n, (void) ++__cur)
         .           		std::_Construct(std::__addressof(*__cur));
         .           	      return __cur;
         .           	    }
         .           	  __catch(...)
         .           	    {
         .           	      std::_Destroy(__first, __cur);
         .           	      __throw_exception_again;
         .           	    }
-- line 574 ----------------------------------------
-- line 983 ----------------------------------------
         .               struct __is_bitwise_relocatable
         .               : is_trivial<_Tp> { };
         .           
         .             template <typename _Tp, typename _Up>
         .               inline __enable_if_t<std::__is_bitwise_relocatable<_Tp>::value, _Tp*>
         .               __relocate_a_1(_Tp* __first, _Tp* __last,
         .           		   _Tp* __result, allocator<_Up>&) noexcept
         .               {
     3,908 ( 0.00%)        ptrdiff_t __count = __last - __first;
 7,697,396 ( 0.21%)        if (__count > 0)
12,823,010 ( 0.36%)  	__builtin_memmove(__result, __first, __count * sizeof(_Tp));
    25,586 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms (12x)
     1,954 ( 0.00%)        return __result + __count;
         .               }
         .           
         .             template <typename _InputIterator, typename _ForwardIterator,
         .           	    typename _Allocator>
         .               inline _ForwardIterator
         .               __relocate_a_1(_InputIterator __first, _InputIterator __last,
         .           		   _ForwardIterator __result, _Allocator& __alloc)
         .               noexcept(noexcept(std::__relocate_object_a(std::addressof(*__result),
-- line 1002 ----------------------------------------
-- line 1004 ----------------------------------------
         .           					       __alloc)))
         .               {
         .                 typedef typename iterator_traits<_InputIterator>::value_type
         .           	_ValueType;
         .                 typedef typename iterator_traits<_ForwardIterator>::value_type
         .           	_ValueType2;
         .                 static_assert(std::is_same<_ValueType, _ValueType2>::value,
         .           	  "relocation is only possible for values of the same type");
    17,414 ( 0.00%)        _ForwardIterator __cur = __result;
 1,153,113 ( 0.03%)        for (; __first != __last; ++__first, (void)++__cur)
         .           	std::__relocate_object_a(std::__addressof(*__cur),
         .           				 std::__addressof(*__first), __alloc);
         .                 return __cur;
         .               }
         .           
         .             template <typename _InputIterator, typename _ForwardIterator,
         .           	    typename _Allocator>
         .               inline _ForwardIterator
-- line 1021 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/src/nptest.cpp
--------------------------------------------------------------------------------
  No information has been collected for /home/sag/Downloads/np-schedulability-analysis-partial_order_reduction/src/nptest.cpp

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/stl_vector.h
--------------------------------------------------------------------------------
Ir                  

-- line 90 ----------------------------------------
         .           
         .                 struct _Vector_impl_data
         .                 {
         .           	pointer _M_start;
         .           	pointer _M_finish;
         .           	pointer _M_end_of_storage;
         .           
         .           	_Vector_impl_data() _GLIBCXX_NOEXCEPT
 4,505,846 ( 0.13%)  	: _M_start(), _M_finish(), _M_end_of_storage()
         .           	{ }
         .           
         .           #if __cplusplus >= 201103L
         .           	_Vector_impl_data(_Vector_impl_data&& __x) noexcept
         3 ( 0.00%)  	: _M_start(__x._M_start), _M_finish(__x._M_finish),
        11 ( 0.00%)  	  _M_end_of_storage(__x._M_end_of_storage)
         6 ( 0.00%)  	{ __x._M_start = __x._M_finish = __x._M_end_of_storage = pointer(); }
         .           #endif
         .           
         .           	void
         .           	_M_copy_data(_Vector_impl_data const& __x) _GLIBCXX_NOEXCEPT
         .           	{
         .           	  _M_start = __x._M_start;
         .           	  _M_finish = __x._M_finish;
         .           	  _M_end_of_storage = __x._M_end_of_storage;
-- line 113 ----------------------------------------
-- line 328 ----------------------------------------
         .                 _Vector_base(const allocator_type& __a, _Vector_base&& __x)
         .                 : _M_impl(_Tp_alloc_type(__a), std::move(__x._M_impl))
         .                 { }
         .           #endif
         .           
         .                 ~_Vector_base() _GLIBCXX_NOEXCEPT
         .                 {
         .           	_M_deallocate(_M_impl._M_start,
 3,209,424 ( 0.09%)  		      _M_impl._M_end_of_storage - _M_impl._M_start);
         .                 }
         .           
         .               public:
         .                 _Vector_impl _M_impl;
         .           
         .                 pointer
 4,483,482 ( 0.13%)        _M_allocate(size_t __n)
         .                 {
         .           	typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;
12,215,632 ( 0.34%)  	return __n != 0 ? _Tr::allocate(_M_impl, __n) : pointer();
         .                 }
         .           
         .                 void
         .                 _M_deallocate(pointer __p, size_t __n)
         .                 {
         .           	typedef __gnu_cxx::__alloc_traits<_Tp_alloc_type> _Tr;
 5,188,425 ( 0.14%)  	if (__p)
         .           	  _Tr::deallocate(_M_impl, __p, __n);
         .                 }
         .           
         .               protected:
         .                 void
         .                 _M_create_storage(size_t __n)
         .                 {
        67 ( 0.00%)  	this->_M_impl._M_start = this->_M_allocate(__n);
         .           	this->_M_impl._M_finish = this->_M_impl._M_start;
        50 ( 0.00%)  	this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;
         .                 }
         .               };
         .           
         .             /**
         .              *  @brief A standard container which offers fixed time access to
         .              *  individual elements in any order.
         .              *
         .              *  @ingroup sequences
-- line 371 ----------------------------------------
-- line 502 ----------------------------------------
         .                  *  @brief  Creates a %vector with default constructed elements.
         .                  *  @param  __n  The number of elements to initially create.
         .                  *  @param  __a  An allocator.
         .                  *
         .                  *  This constructor fills the %vector with @a __n default
         .                  *  constructed elements.
         .                  */
         .                 explicit
         9 ( 0.00%)        vector(size_type __n, const allocator_type& __a = allocator_type())
         .                 : _Base(_S_check_init_len(__n, __a), __a)
        12 ( 0.00%)        { _M_default_initialize(__n); }
         .           
         .                 /**
         .                  *  @brief  Creates a %vector with copies of an exemplar element.
         .                  *  @param  __n  The number of elements to initially create.
         .                  *  @param  __value  An element to copy.
         .                  *  @param  __a  An allocator.
         .                  *
         .                  *  This constructor fills the %vector with @a __n copies of @a __value.
-- line 520 ----------------------------------------
-- line 545 ----------------------------------------
         .                  *
         .                  *  All the elements of @a __x are copied, but any unused capacity in
         .                  *  @a __x  will not be copied
         .                  *  (i.e. capacity() == size() in the new %vector).
         .                  *
         .                  *  The newly-created %vector uses a copy of the allocator object used
         .                  *  by @a __x (unless the allocator traits dictate a different object).
         .                  */
        65 ( 0.00%)        vector(const vector& __x)
         .                 : _Base(__x.size(),
         .           	_Alloc_traits::_S_select_on_copy(__x._M_get_Tp_allocator()))
         .                 {
         8 ( 0.00%)  	this->_M_impl._M_finish =
         .           	  std::__uninitialized_copy_a(__x.begin(), __x.end(),
         .           				      this->_M_impl._M_start,
         .           				      _M_get_Tp_allocator());
        52 ( 0.00%)        }
         .           
         .           #if __cplusplus >= 201103L
         .                 /**
         .                  *  @brief  %Vector move constructor.
         .                  *
         .                  *  The newly-created %vector contains the exact contents of the
         .                  *  moved instance.
         .                  *  The contents of the moved instance are a valid, but unspecified
-- line 569 ----------------------------------------
-- line 645 ----------------------------------------
         .                  *  constructor N times (where N is distance(first,last)) and do
         .                  *  no memory reallocation.  But if only input iterators are
         .                  *  used, then this will do at most 2N calls to the copy
         .                  *  constructor, and logN memory reallocations.
         .                  */
         .           #if __cplusplus >= 201103L
         .                 template<typename _InputIterator,
         .           	       typename = std::_RequireInputIter<_InputIterator>>
        16 ( 0.00%)  	vector(_InputIterator __first, _InputIterator __last,
         .           	       const allocator_type& __a = allocator_type())
         .           	: _Base(__a)
         .           	{
         .           	  _M_range_initialize(__first, __last,
         .           			      std::__iterator_category(__first));
        16 ( 0.00%)  	}
         .           #else
         .                 template<typename _InputIterator>
         .           	vector(_InputIterator __first, _InputIterator __last,
         .           	       const allocator_type& __a = allocator_type())
         .           	: _Base(__a)
         .           	{
         .           	  // Check whether it's an integral type.  If so, it's not an iterator.
         .           	  typedef typename std::__is_integer<_InputIterator>::__type _Integral;
-- line 667 ----------------------------------------
-- line 670 ----------------------------------------
         .           #endif
         .           
         .                 /**
         .                  *  The dtor only erases the elements, and note that if the
         .                  *  elements themselves are pointers, the pointed-to memory is
         .                  *  not touched in any way.  Managing the pointer is the user's
         .                  *  responsibility.
         .                  */
        24 ( 0.00%)        ~vector() _GLIBCXX_NOEXCEPT
         .                 {
   652,103 ( 0.02%)  	std::_Destroy(this->_M_impl._M_start, this->_M_impl._M_finish,
         .           		      _M_get_Tp_allocator());
         .           	_GLIBCXX_ASAN_ANNOTATE_BEFORE_DEALLOC;
        19 ( 0.00%)        }
         .           
         .                 /**
         .                  *  @brief  %Vector assignment operator.
         .                  *  @param  __x  A %vector of identical element and allocator types.
         .                  *
         .                  *  All the elements of @a __x are copied, but any unused capacity in
         .                  *  @a __x will not be copied.
         .                  *
-- line 691 ----------------------------------------
-- line 910 ----------------------------------------
         .                 const_reverse_iterator
         .                 crend() const noexcept
         .                 { return const_reverse_iterator(begin()); }
         .           #endif
         .           
         .                 // [23.2.4.2] capacity
         .                 /**  Returns the number of elements in the %vector.  */
         .                 size_type
    20,732 ( 0.00%)        size() const _GLIBCXX_NOEXCEPT
38,807,836 ( 1.08%)        { return size_type(this->_M_impl._M_finish - this->_M_impl._M_start); }
         .           
         .                 /**  Returns the size() of the largest possible %vector.  */
         .                 size_type
         .                 max_size() const _GLIBCXX_NOEXCEPT
         .                 { return _S_max_size(_M_get_Tp_allocator()); }
         .           
         .           #if __cplusplus >= 201103L
         .                 /**
-- line 927 ----------------------------------------
-- line 1038 ----------------------------------------
         .                  *  Note that data access with this operator is unchecked and
         .                  *  out_of_range lookups are not defined. (For checked lookups
         .                  *  see at().)
         .                  */
         .                 reference
         .                 operator[](size_type __n) _GLIBCXX_NOEXCEPT
         .                 {
         .           	__glibcxx_requires_subscript(__n);
 3,635,568 ( 0.10%)  	return *(this->_M_impl._M_start + __n);
         .                 }
         .           
         .                 /**
         .                  *  @brief  Subscript access to the data contained in the %vector.
         .                  *  @param __n The index of the element for which data should be
         .                  *  accessed.
         .                  *  @return  Read-only (constant) reference to data.
         .                  *
         .                  *  This operator allows for easy, array-style, data access.
         .                  *  Note that data access with this operator is unchecked and
         .                  *  out_of_range lookups are not defined. (For checked lookups
         .                  *  see at().)
         .                  */
         .                 const_reference
         4 ( 0.00%)        operator[](size_type __n) const _GLIBCXX_NOEXCEPT
         .                 {
         .           	__glibcxx_requires_subscript(__n);
    30,241 ( 0.00%)  	return *(this->_M_impl._M_start + __n);
         .                 }
         .           
         .               protected:
         .                 /// Safety check used only from at().
         .                 void
         .                 _M_range_check(size_type __n) const
         .                 {
         .           	if (__n >= this->size())
-- line 1072 ----------------------------------------
-- line 1181 ----------------------------------------
         .                  *  element at the end of the %vector and assigns the given data
         .                  *  to it.  Due to the nature of a %vector this operation can be
         .                  *  done in constant time if the %vector has preallocated space
         .                  *  available.
         .                  */
         .                 void
         .                 push_back(const value_type& __x)
         .                 {
    47,529 ( 0.00%)  	if (this->_M_impl._M_finish != this->_M_impl._M_end_of_storage)
         .           	  {
         .           	    _GLIBCXX_ASAN_ANNOTATE_GROW(1);
         .           	    _Alloc_traits::construct(this->_M_impl, this->_M_impl._M_finish,
         .           				     __x);
    25,611 ( 0.00%)  	    ++this->_M_impl._M_finish;
         .           	    _GLIBCXX_ASAN_ANNOTATE_GREW(1);
         .           	  }
         .           	else
     7,652 ( 0.00%)  	  _M_realloc_insert(end(), __x);
   537,188 ( 0.01%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> >::_M_realloc_insert<NP::Job<long long> const* const&>(__gnu_cxx::__normal_iterator<NP::Job<long long> const**, std::vector<NP::Job<long long> const*, std::allocator<NP::Job<long long> const*> > >, NP::Job<long long> const* const&) (1,864x)
         .                 }
         .           
         .           #if __cplusplus >= 201103L
         .                 void
         .                 push_back(value_type&& __x)
     7,581 ( 0.00%)        { emplace_back(std::move(__x)); }
     1,213 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<unsigned long, std::allocator<unsigned long> >::emplace_back<unsigned long>(unsigned long&&) (8x)
       196 ( 0.00%)  => /usr/include/c++/10/bits/vector.tcc:void std::vector<NP::Global::Reduction_set_statistics<long long>, std::allocator<NP::Global::Reduction_set_statistics<long long> > >::emplace_back<NP::Global::Reduction_set_statistics<long long> >(NP::Global::Reduction_set_statistics<long long>&&) (1x)
         .           
         .                 template<typename... _Args>
         .           #if __cplusplus > 201402L
         .           	reference
         .           #else
         .           	void
         .           #endif
         .           	emplace_back(_Args&&... __args);
-- line 1212 ----------------------------------------
-- line 1574 ----------------------------------------
         .                 // Called by the second initialize_dispatch above
         .                 template<typename _ForwardIterator>
         .           	void
         .           	_M_range_initialize(_ForwardIterator __first, _ForwardIterator __last,
         .           			    std::forward_iterator_tag)
         .           	{
         .           	  const size_type __n = std::distance(__first, __last);
         .           	  this->_M_impl._M_start
        21 ( 0.00%)  	    = this->_M_allocate(_S_check_init_len(__n, _M_get_Tp_allocator()));
        42 ( 0.00%)  	  this->_M_impl._M_end_of_storage = this->_M_impl._M_start + __n;
        20 ( 0.00%)  	  this->_M_impl._M_finish =
         .           	    std::__uninitialized_copy_a(__first, __last,
         .           					this->_M_impl._M_start,
         .           					_M_get_Tp_allocator());
         .           	}
         .           
         .                 // Called by the first initialize_dispatch above and by the
         .                 // vector(n,value,a) constructor.
         .                 void
         .                 _M_fill_initialize(size_type __n, const value_type& __value)
         .                 {
         1 ( 0.00%)  	this->_M_impl._M_finish =
         .           	  std::__uninitialized_fill_n_a(this->_M_impl._M_start, __n, __value,
         .           					_M_get_Tp_allocator());
         .                 }
         .           
         .           #if __cplusplus >= 201103L
         .                 // Called by the vector(n) constructor.
         .                 void
         .                 _M_default_initialize(size_type __n)
         .                 {
         3 ( 0.00%)  	this->_M_impl._M_finish =
         .           	  std::__uninitialized_default_n_a(this->_M_impl._M_start, __n,
         .           					   _M_get_Tp_allocator());
         .                 }
         .           #endif
         .           
         .                 // Internal assign functions follow.  The *_aux functions do the actual
         .                 // assignment work for the range versions.
         .           
-- line 1613 ----------------------------------------
-- line 1750 ----------------------------------------
         .                 _M_emplace_aux(const_iterator __position, value_type&& __v)
         .                 { return _M_insert_rval(__position, std::move(__v)); }
         .           #endif
         .           
         .                 // Called by _M_fill_insert, _M_insert_aux etc.
         .                 size_type
         .                 _M_check_len(size_type __n, const char* __s) const
         .                 {
34,819,042 ( 0.97%)  	if (max_size() - size() < __n)
         .           	  __throw_length_error(__N(__s));
         .           
         .           	const size_type __len = size() + (std::max)(size(), __n);
    45,372 ( 0.00%)  	return (__len < size() || __len > max_size()) ? max_size() : __len;
         .                 }
         .           
         .                 // Called by constructors to check initial size.
         .                 static size_type
         .                 _S_check_init_len(size_type __n, const allocator_type& __a)
         .                 {
        21 ( 0.00%)  	if (__n > _S_max_size(_Tp_alloc_type(__a)))
         .           	  __throw_length_error(
         .           	      __N("cannot create std::vector larger than max_size()"));
         .           	return __n;
         .                 }
         .           
         .                 static size_type
         .                 _S_max_size(const _Tp_alloc_type& __a) _GLIBCXX_NOEXCEPT
         .                 {
-- line 1777 ----------------------------------------
-- line 1790 ----------------------------------------
         .                 // _M_assign_aux.
         .                 void
         .                 _M_erase_at_end(pointer __pos) _GLIBCXX_NOEXCEPT
         .                 {
         .           	if (size_type __n = this->_M_impl._M_finish - __pos)
         .           	  {
         .           	    std::_Destroy(__pos, this->_M_impl._M_finish,
         .           			  _M_get_Tp_allocator());
        22 ( 0.00%)  	    this->_M_impl._M_finish = __pos;
         .           	    _GLIBCXX_ASAN_ANNOTATE_SHRINK(__n);
         .           	  }
         .                 }
         .           
         .                 iterator
         .                 _M_erase(iterator __position);
         .           
         .                 iterator
-- line 1806 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/c++/10/bits/hashtable_policy.h
--------------------------------------------------------------------------------
Ir                 

-- line 210 ----------------------------------------
        .              *  template parameter of class template _Hashtable controls whether
        .              *  nodes also store a hash code. In some cases (e.g. strings) this
        .              *  may be a performance win.
        .              */
        .             struct _Hash_node_base
        .             {
        .               _Hash_node_base* _M_nxt;
        .           
  650,164 ( 0.02%)      _Hash_node_base() noexcept : _M_nxt() { }
        .           
        .               _Hash_node_base(_Hash_node_base* __next) noexcept : _M_nxt(__next) { }
        .             };
        .           
        .             /**
        .              *  struct _Hash_node_value_base
        .              *
        .              *  Node type with the value to store.
-- line 226 ----------------------------------------
-- line 261 ----------------------------------------
        .              *  Base class is __detail::_Hash_node_value_base.
        .              */
        .             template<typename _Value>
        .               struct _Hash_node<_Value, true> : _Hash_node_value_base<_Value>
        .               {
        .                 std::size_t  _M_hash_code;
        .           
        .                 _Hash_node*
  668,126 ( 0.02%)        _M_next() const noexcept
  668,126 ( 0.02%)        { return static_cast<_Hash_node*>(this->_M_nxt); }
        .               };
        .           
        .             /**
        .              *  Specialization for nodes without caches, struct _Hash_node.
        .              *
        .              *  Base class is __detail::_Hash_node_value_base.
        .              */
        .             template<typename _Value>
        .               struct _Hash_node<_Value, false> : _Hash_node_value_base<_Value>
        .               {
        .                 _Hash_node*
        1 ( 0.00%)        _M_next() const noexcept
        1 ( 0.00%)        { return static_cast<_Hash_node*>(this->_M_nxt); }
        .               };
        .           
        .             /// Base class for node iterators.
        .             template<typename _Value, bool _Cache_hash_code>
        .               struct _Node_iterator_base
        .               {
        .                 using __node_type = _Hash_node<_Value, _Cache_hash_code>;
        .           
        .                 __node_type*  _M_cur;
        .           
        .                 _Node_iterator_base(__node_type* __p) noexcept
    5,014 ( 0.00%)        : _M_cur(__p) { }
        .           
        .                 void
        .                 _M_incr() noexcept
        .                 { _M_cur = _M_cur->_M_next(); }
        .               };
        .           
        .             template<typename _Value, bool _Cache_hash_code>
        .               inline bool
-- line 303 ----------------------------------------
-- line 333 ----------------------------------------
        .                 using reference = typename std::conditional<__constant_iterators,
        .           						  const _Value&, _Value&>::type;
        .           
        .                 _Node_iterator() noexcept
        .                 : __base_type(0) { }
        .           
        .                 explicit
        .                 _Node_iterator(__node_type* __p) noexcept
    5,014 ( 0.00%)        : __base_type(__p) { }
        .           
        .                 reference
        .                 operator*() const noexcept
        .                 { return this->_M_cur->_M_v(); }
        .           
        .                 pointer
        .                 operator->() const noexcept
        .                 { return this->_M_cur->_M_valptr(); }
-- line 349 ----------------------------------------
-- line 425 ----------------------------------------
        .             {
        .               typedef std::size_t first_argument_type;
        .               typedef std::size_t second_argument_type;
        .               typedef std::size_t result_type;
        .           
        .               result_type
        .               operator()(first_argument_type __num,
        .           	       second_argument_type __den) const noexcept
8,799,762 ( 0.25%)      { return __num % __den; }
        .             };
        .           
        .             /// Default ranged hash function H.  In principle it should be a
        .             /// function object composed from objects of type H1 and H2 such that
        .             /// h(k, N) = h2(h1(k), N), but that would mean making extra copies of
        .             /// h1 and h2.  So instead we'll just use a tag to tell class template
        .             /// hashtable to do that composition.
        .             struct _Default_ranged_hash { };
-- line 441 ----------------------------------------
-- line 442 ----------------------------------------
        .           
        .             /// Default value for rehash policy.  Bucket size is (usually) the
        .             /// smallest prime that keeps the load factor small enough.
        .             struct _Prime_rehash_policy
        .             {
        .               using __has_load_factor = true_type;
        .           
        .               _Prime_rehash_policy(float __z = 1.0) noexcept
       13 ( 0.00%)      : _M_max_load_factor(__z), _M_next_resize(0) { }
        .           
        .               float
        .               max_load_factor() const noexcept
        .               { return _M_max_load_factor; }
        .           
        .               // Return a bucket size no smaller than n.
        .               std::size_t
        .               _M_next_bkt(std::size_t __n) const;
        .           
        .               // Return a bucket count appropriate for n elements
        .               std::size_t
        .               _M_bkt_for_elements(std::size_t __n) const
   22,393 ( 0.00%)      { return __builtin_ceill(__n / (long double)_M_max_load_factor); }
        .           
        .               // __n_bkt is current bucket count, __n_elt is current element count,
        .               // and __n_ins is number of elements to be inserted.  Do we need to
        .               // increase bucket count?  If so, return make_pair(true, n), where n
        .               // is the new bucket count.  If not, return make_pair(false, 0).
        .               std::pair<bool, std::size_t>
        .               _M_need_rehash(std::size_t __n_bkt, std::size_t __n_elt,
        .           		   std::size_t __n_ins) const;
-- line 471 ----------------------------------------
-- line 477 ----------------------------------------
        .               { return _M_next_resize; }
        .           
        .               void
        .               _M_reset() noexcept
        .               { _M_next_resize = 0; }
        .           
        .               void
        .               _M_reset(_State __state)
    1,338 ( 0.00%)      { _M_next_resize = __state; }
        .           
        .               static const std::size_t _S_growth_factor = 2;
        .           
        .               float		_M_max_load_factor;
        .               mutable std::size_t	_M_next_resize;
        .             };
        .           
        .             /// Range hashing function assuming that second arg is a power of 2.
-- line 493 ----------------------------------------
-- line 720 ----------------------------------------
        .                 __node._M_node = nullptr;
        .                 return __pos->second;
        .               }
        .           
        .             template<typename _Key, typename _Pair, typename _Alloc, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash,
        .           	   typename _RehashPolicy, typename _Traits>
        .               auto
7,225,421 ( 0.20%)      _Map_base<_Key, _Pair, _Alloc, _Select1st, _Equal,
        .           	      _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::
        .               operator[](key_type&& __k)
        .               -> mapped_type&
        .               {
        .                 __hashtable* __h = static_cast<__hashtable*>(this);
        .                 __hash_code __code = __h->_M_hash_code(__k);
        .                 std::size_t __bkt = __h->_M_bucket_index(__k, __code);
        .                 if (__node_type* __node = __h->_M_find_node(__bkt, __k, __code))
1,032,203 ( 0.03%)  	return __node->_M_v().second;
        .           
        .                 typename __hashtable::_Scoped_node __node {
        .           	__h,
        .           	std::piecewise_construct,
        .           	std::forward_as_tuple(std::move(__k)),
        .           	std::tuple<>()
        .                 };
        .                 auto __pos
        .           	= __h->_M_insert_unique_node(__k, __bkt, __code, __node._M_node);
        .                 __node._M_node = nullptr;
        .                 return __pos->second;
6,193,218 ( 0.17%)      }
        .           
        .             template<typename _Key, typename _Pair, typename _Alloc, typename _Equal,
        .           	   typename _H1, typename _H2, typename _Hash,
        .           	   typename _RehashPolicy, typename _Traits>
        .               auto
        .               _Map_base<_Key, _Pair, _Alloc, _Select1st, _Equal,
        .           	      _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::
        .               at(const key_type& __k)
-- line 757 ----------------------------------------
-- line 1383 ----------------------------------------
        .                 _M_bucket_index(const _Key&, __hash_code __c,
        .           		      std::size_t __bkt_count) const
        .                 { return _M_h2()(__c, __bkt_count); }
        .           
        .                 std::size_t
        .                 _M_bucket_index(const __node_type* __p, std::size_t __bkt_count) const
        .           	noexcept( noexcept(declval<const _H2&>()((__hash_code)0,
        .           						 (std::size_t)0)) )
  147,565 ( 0.00%)        { return _M_h2()(__p->_M_hash_code, __bkt_count); }
        .           
        .                 void
        .                 _M_store_code(__node_type* __n, __hash_code __c) const
  650,158 ( 0.02%)        { __n->_M_hash_code = __c; }
        .           
        .                 void
        .                 _M_copy_code(__node_type* __to, const __node_type* __from) const
        .                 { __to->_M_hash_code = __from->_M_hash_code; }
        .           
        .                 void
        .                 _M_swap(_Hash_code_base& __x)
        .                 {
-- line 1403 ----------------------------------------
-- line 1779 ----------------------------------------
        .                  { return true; }
        .                 };
        .           
        .               template<typename _Ptr2>
        .                 struct _Equal_hash_code<_Hash_node<_Ptr2, true>>
        .                 {
        .                  static bool
        .                  _S_equals(__hash_code __c, const _Hash_node<_Ptr2, true>& __n)
2,358,560 ( 0.07%)         { return __c == __n._M_hash_code; }
        .                 };
        .           
        .             protected:
        .               _Hashtable_base() = default;
        .               _Hashtable_base(const _ExtractKey& __ex, const _H1& __h1, const _H2& __h2,
        .           		    const _Hash& __hash, const _Equal& __eq)
        .               : __hash_code_base(__ex, __h1, __h2, __hash), _EqualEBO(__eq)
        .               { }
-- line 1795 ----------------------------------------
-- line 1796 ----------------------------------------
        .           
        .               bool
        .               _M_equals(const _Key& __k, __hash_code __c, __node_type* __n) const
        .               {
        .                 static_assert(__is_invocable<const _Equal&, const _Key&, const _Key&>{},
        .           	  "key equality predicate must be invocable with two arguments of "
        .           	  "key type");
        .                 return _Equal_hash_code<__node_type>::_S_equals(__c, *__n)
2,374,974 ( 0.07%)  	&& _M_eq()(__k, this->_M_extract()(__n->_M_v()));
        .               }
        .           
        .               void
        .               _M_swap(_Hashtable_base& __x)
        .               {
        .                 __hash_code_base::_M_swap(__x);
        .                 std::swap(_EqualEBO::_M_get(), __x._EqualEBO::_M_get());
        .               }
-- line 1812 ----------------------------------------
-- line 2063 ----------------------------------------
        .                 __n->~__node_type();
        .                 __node_alloc_traits::deallocate(_M_node_allocator(), __ptr, 1);
        .               }
        .           
        .             template<typename _NodeAlloc>
        .               void
        .               _Hashtable_alloc<_NodeAlloc>::_M_deallocate_nodes(__node_type* __n)
        .               {
1,302,379 ( 0.04%)        while (__n)
        .           	{
        .           	  __node_type* __tmp = __n;
        .           	  __n = __n->_M_next();
        .           	  _M_deallocate_node(__tmp);
        .           	}
        .               }
        .           
        .             template<typename _NodeAlloc>
        .               typename _Hashtable_alloc<_NodeAlloc>::__bucket_type*
        .               _Hashtable_alloc<_NodeAlloc>::_M_allocate_buckets(std::size_t __bkt_count)
        .               {
        .                 __bucket_alloc_type __alloc(_M_node_allocator());
        .           
        .                 auto __ptr = __bucket_alloc_traits::allocate(__alloc, __bkt_count);
        .                 __bucket_type* __p = std::__to_address(__ptr);
      504 ( 0.00%)        __builtin_memset(__p, 0, __bkt_count * sizeof(__bucket_type));
  261,750 ( 0.01%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (63x)
        .                 return __p;
        .               }
        .           
        .             template<typename _NodeAlloc>
        .               void
        .               _Hashtable_alloc<_NodeAlloc>::_M_deallocate_buckets(__bucket_type* __bkts,
        .           							std::size_t __bkt_count)
        .               {
-- line 2095 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp
--------------------------------------------------------------------------------
Ir                  

-- line 34 ----------------------------------------
         .               std::atomic<int> allocatedCount; // the number of objects allocated
         .               BackRefIdx::main_t myNum;   // the index in the main
         .               MallocMutex   blockMutex;
         .               // true if this block has been added to the listForUse chain,
         .               // modifications protected by mainMutex
         .               std::atomic<bool> addedToForUse;
         .           
         .               BackRefBlock(const BackRefBlock *blockToUse, intptr_t num) :
         8 ( 0.00%)          nextForUse(nullptr), bumpPtr((FreeObject*)((uintptr_t)blockToUse + slabSize - sizeof(void*))),
         .                   freeList(nullptr), nextRawMemBlock(nullptr), allocatedCount(0), myNum(num),
        16 ( 0.00%)          addedToForUse(false) {
         4 ( 0.00%)          memset(&blockMutex, 0, sizeof(MallocMutex));
         .           
         .                   MALLOC_ASSERT(!(num >> CHAR_BIT*sizeof(BackRefIdx::main_t)),
         .                                 "index in BackRefMain must fit to BackRefIdx::main");
         .               }
         .               // clean all but header
        24 ( 0.00%)      void zeroSet() { memset(this+1, 0, BackRefBlock::bytes-sizeof(BackRefBlock)); }
    49,008 ( 0.00%)  => ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms (3x)
    17,195 ( 0.00%)  => ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave'2 (1x)
         .               static const int bytes = slabSize;
         .           };
         .           
         .           // max number of backreference pointers in slab block
         .           static const int BR_MAX_CNT = (BackRefBlock::bytes-sizeof(BackRefBlock))/sizeof(void*);
         .           
         .           struct BackRefMain {
         .           /* On 64-bit systems a slab block can hold up to ~2K back pointers to slab blocks
-- line 59 ----------------------------------------
-- line 92 ----------------------------------------
         .           
         .           static MallocMutex mainMutex;
         .           static std::atomic<BackRefMain*> backRefMain;
         .           
         .           bool initBackRefMain(Backend *backend)
         .           {
         .               bool rawMemUsed;
         .               BackRefMain *main =
         6 ( 0.00%)          (BackRefMain*)backend->getBackRefSpace(BackRefMain::mainSize,
     1,817 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backend.cpp:rml::internal::Backend::getBackRefSpace(unsigned long, bool*) (1x)
         .                                                            &rawMemUsed);
         2 ( 0.00%)      if (! main)
         .                   return false;
         1 ( 0.00%)      main->backend = backend;
         .               main->listForUse.store(nullptr, std::memory_order_relaxed);
         1 ( 0.00%)      main->allRawMemBlocks = nullptr;
         2 ( 0.00%)      main->rawMemUsed = rawMemUsed;
         .               main->lastUsed = -1;
         1 ( 0.00%)      memset(&main->requestNewSpaceMutex, 0, sizeof(MallocMutex));
        19 ( 0.00%)      for (int i=0; i<BackRefMain::leaves; i++) {
         .                   BackRefBlock *bl = (BackRefBlock*)((uintptr_t)main + BackRefMain::bytes + i*BackRefBlock::bytes);
         .                   bl->zeroSet();
        12 ( 0.00%)          main->initEmptyBackRefBlock(bl);
        56 ( 0.00%)  => /home/sag/Downloads/oneTBB-master/src/tbbmalloc/backref.cpp:rml::internal::BackRefMain::initEmptyBackRefBlock(rml::internal::BackRefBlock*) (4x)
         8 ( 0.00%)          if (i)
         .                       main->addToForUseList(bl);
         .                   else // active leaf is not needed in listForUse
         .                       main->active.store(bl, std::memory_order_relaxed);
         .               }
         .               // backRefMain is read in getBackRef, so publish it in consistent state
         .               backRefMain.store(main, std::memory_order_release);
         .               return true;
         .           }
-- line 122 ----------------------------------------
-- line 135 ----------------------------------------
         .                   backend->putBackRefSpace(backRefMain.load(std::memory_order_relaxed), BackRefMain::mainSize,
         .                                            backRefMain.load(std::memory_order_relaxed)->rawMemUsed);
         .               }
         .           }
         .           #endif
         .           
         .           void BackRefMain::addToForUseList(BackRefBlock *bl)
         .           {
         3 ( 0.00%)      bl->nextForUse = listForUse.load(std::memory_order_relaxed);
         .               listForUse.store(bl, std::memory_order_relaxed);
         .               bl->addedToForUse.store(true, std::memory_order_relaxed);
         .           }
         .           
         .           void BackRefMain::initEmptyBackRefBlock(BackRefBlock *newBl)
         .           {
         4 ( 0.00%)      intptr_t nextLU = lastUsed+1;
         .               new (newBl) BackRefBlock(newBl, nextLU);
         .               MALLOC_ASSERT(nextLU < dataSz, nullptr);
         4 ( 0.00%)      backRefBl[nextLU] = newBl;
         .               // lastUsed is read in getBackRef, and access to backRefBl[lastUsed]
         .               // is possible only after checking backref against current lastUsed
         .               lastUsed.store(nextLU, std::memory_order_release);
         4 ( 0.00%)  }
         .           
         .           bool BackRefMain::requestNewSpace()
         .           {
         .               bool isRawMemUsed;
         .               static_assert(!(blockSpaceSize % BackRefBlock::bytes),
         .                                    "Must request space for whole number of blocks.");
         .           
         .               if (BackRefMain::dataSz <= lastUsed + 1) // no space in main
-- line 165 ----------------------------------------
-- line 207 ----------------------------------------
         .               return true;
         .           }
         .           
         .           BackRefBlock *BackRefMain::findFreeBlock()
         .           {
         .               BackRefBlock* active_block = active.load(std::memory_order_acquire);
         .               MALLOC_ASSERT(active_block, ASSERT_TEXT);
         .           
       450 ( 0.00%)      if (active_block->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT)
         .                   return active_block;
         .           
         .               if (listForUse.load(std::memory_order_relaxed)) { // use released list
         .                   MallocMutex::scoped_lock lock(mainMutex);
         .           
         .                   if (active_block->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT) {
         .                       active_block = listForUse.load(std::memory_order_relaxed);
         .                       if (active_block) {
-- line 223 ----------------------------------------
-- line 233 ----------------------------------------
         .               return active.load(std::memory_order_acquire); // reread because of requestNewSpace
         .           }
         .           
         .           void *getBackRef(BackRefIdx backRefIdx)
         .           {
         .               // !backRefMain means no initialization done, so it can't be valid memory
         .               // see addEmptyBackRefBlock for fences around lastUsed
         .               if (!(backRefMain.load(std::memory_order_acquire))
 5,166,808 ( 0.14%)          || backRefIdx.getMain() > (backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_acquire))
31,000,848 ( 0.87%)          || backRefIdx.getOffset() >= BR_MAX_CNT)
         .               {
       136 ( 0.00%)          return nullptr;
         .               }
         .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)(
         .                       (uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
 5,166,740 ( 0.14%)              + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(std::atomic<void*>)
         .                   );
         .               return backRefEntry.load(std::memory_order_relaxed);
 5,166,740 ( 0.14%)  }
         .           
         .           void setBackRef(BackRefIdx backRefIdx, void *newPtr)
         .           {
         .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
         .                                            && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
       375 ( 0.00%)      ((std::atomic<void*>*)((uintptr_t)backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()]
       268 ( 0.00%)          + sizeof(BackRefBlock) + backRefIdx.getOffset() * sizeof(void*)))->store(newPtr, std::memory_order_relaxed);
         .           }
         .           
         .           BackRefIdx BackRefIdx::newBackRef(bool largeObj)
     2,025 ( 0.00%)  {
         .               BackRefBlock *blockToUse;
         .               void **toUse;
         .               BackRefIdx res;
         .               bool lastBlockFirstUsed = false;
         .           
         .               do {
         .                   MALLOC_ASSERT(backRefMain.load(std::memory_order_relaxed), ASSERT_TEXT);
         .                   blockToUse = backRefMain.load(std::memory_order_relaxed)->findFreeBlock();
       450 ( 0.00%)          if (!blockToUse)
         .                       return BackRefIdx();
         .                   toUse = nullptr;
         .                   { // the block is locked to find a reference
       225 ( 0.00%)              MallocMutex::scoped_lock lock(blockToUse->blockMutex);
         .           
       675 ( 0.00%)              if (blockToUse->freeList) {
         .                           toUse = (void**)blockToUse->freeList;
         .                           blockToUse->freeList = blockToUse->freeList->next;
         .                           MALLOC_ASSERT(!blockToUse->freeList ||
         .                                         ((uintptr_t)blockToUse->freeList>=(uintptr_t)blockToUse
         .                                          && (uintptr_t)blockToUse->freeList <
         .                                          (uintptr_t)blockToUse + slabSize), ASSERT_TEXT);
       450 ( 0.00%)              } else if (blockToUse->allocatedCount.load(std::memory_order_relaxed) < BR_MAX_CNT) {
       225 ( 0.00%)                  toUse = (void**)blockToUse->bumpPtr;
         .                           blockToUse->bumpPtr =
       450 ( 0.00%)                      (FreeObject*)((uintptr_t)blockToUse->bumpPtr - sizeof(void*));
         .                           if (blockToUse->allocatedCount.load(std::memory_order_relaxed) == BR_MAX_CNT-1) {
         .                               MALLOC_ASSERT((uintptr_t)blockToUse->bumpPtr
         .                                             < (uintptr_t)blockToUse+sizeof(BackRefBlock),
         .                                             ASSERT_TEXT);
       900 ( 0.00%)                      blockToUse->bumpPtr = nullptr;
         .                           }
         .                       }
       675 ( 0.00%)              if (toUse) {
       450 ( 0.00%)                  if (!blockToUse->allocatedCount.load(std::memory_order_relaxed) &&
         .                               !backRefMain.load(std::memory_order_relaxed)->listForUse.load(std::memory_order_relaxed)) {
         .                               lastBlockFirstUsed = true;
         .                           }
       225 ( 0.00%)                  blockToUse->allocatedCount.store(blockToUse->allocatedCount.load(std::memory_order_relaxed) + 1, std::memory_order_relaxed);
         .                       }
         .                   } // end of lock scope
         .               } while (!toUse);
         .               // The first thread that uses the last block requests new space in advance;
         .               // possible failures are ignored.
         2 ( 0.00%)      if (lastBlockFirstUsed)
         .                   backRefMain.load(std::memory_order_relaxed)->requestNewSpace();
         .           
         .               res.main = blockToUse->myNum;
       225 ( 0.00%)      uintptr_t offset =
       450 ( 0.00%)          ((uintptr_t)toUse - ((uintptr_t)blockToUse + sizeof(BackRefBlock)))/sizeof(void*);
         .               // Is offset too big?
         .               MALLOC_ASSERT(!(offset >> 15), ASSERT_TEXT);
         .               res.offset = offset;
         .               if (largeObj) res.largeObj = largeObj;
         .           
     1,575 ( 0.00%)      return res;
     1,800 ( 0.00%)  }
         .           
         .           void removeBackRef(BackRefIdx backRefIdx)
       960 ( 0.00%)  {
         .               MALLOC_ASSERT(!backRefIdx.isInvalid(), ASSERT_TEXT);
         .               MALLOC_ASSERT(backRefIdx.getMain()<=backRefMain.load(std::memory_order_relaxed)->lastUsed.load(std::memory_order_relaxed)
         .                             && backRefIdx.getOffset()<BR_MAX_CNT, ASSERT_TEXT);
       384 ( 0.00%)      BackRefBlock *currBlock = backRefMain.load(std::memory_order_relaxed)->backRefBl[backRefIdx.getMain()];
         .               std::atomic<void*>& backRefEntry = *(std::atomic<void*>*)((uintptr_t)currBlock + sizeof(BackRefBlock)
       576 ( 0.00%)                                          + backRefIdx.getOffset()*sizeof(std::atomic<void*>));
         .               MALLOC_ASSERT(((uintptr_t)&backRefEntry >(uintptr_t)currBlock &&
         .                              (uintptr_t)&backRefEntry <(uintptr_t)currBlock + slabSize), ASSERT_TEXT);
         .               {
       192 ( 0.00%)          MallocMutex::scoped_lock lock(currBlock->blockMutex);
         .           
         .                   backRefEntry.store(currBlock->freeList, std::memory_order_relaxed);
         .           #if MALLOC_DEBUG
         .                   uintptr_t backRefEntryValue = (uintptr_t)backRefEntry.load(std::memory_order_relaxed);
         .                   MALLOC_ASSERT(!backRefEntryValue ||
         .                                 (backRefEntryValue > (uintptr_t)currBlock
         .                                  && backRefEntryValue < (uintptr_t)currBlock + slabSize), ASSERT_TEXT);
         .           #endif
       192 ( 0.00%)          currBlock->freeList = (FreeObject*)&backRefEntry;
       192 ( 0.00%)          currBlock->allocatedCount.store(currBlock->allocatedCount.load(std::memory_order_relaxed)-1, std::memory_order_relaxed);
         .               }
         .               // TODO: do we need double-check here?
       768 ( 0.00%)      if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
         .                   currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed)) {
         .                   MallocMutex::scoped_lock lock(mainMutex);
         .           
         .                   if (!currBlock->addedToForUse.load(std::memory_order_relaxed) &&
         .                       currBlock!=backRefMain.load(std::memory_order_relaxed)->active.load(std::memory_order_relaxed))
         .                       backRefMain.load(std::memory_order_relaxed)->addToForUseList(currBlock);
         .               }
     1,152 ( 0.00%)  }
         .           
         .           /********* End of backreferences ***********************/
         .           
         .           } // namespace internal
         .           } // namespace rml
         .           

--------------------------------------------------------------------------------
The following files chosen for auto-annotation could not be found:
--------------------------------------------------------------------------------
  ./csu/../csu/libc-start.c
  ./nptl/pthread_getspecific.c
  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S

--------------------------------------------------------------------------------
Ir                     
--------------------------------------------------------------------------------
3,230,799,930 (90.17%)  events annotated

